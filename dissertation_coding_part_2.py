# -*- coding: utf-8 -*-
"""Dissertation Coding Part 2

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1jsA6h3xFf-SyKoAzfGL3qyrz088utm37

# **Appendix E: Full Implementation Code**

## **Project Title: A Socio-Technical Framework for Uncertainty-Aware, Forecast-Informed Optimisation of the Public EV Charging Network.**

Project Description:
This appendix contains the complete Python script used to implement the Socio-Technical Uncertainty-Aware Optimisation Framework (ST-UOF) detailed in this dissertation. The code is structured to follow the methodological pipeline, beginning with data loading and pre-processing, followed by the training and evaluation of machine learning forecasting models, forecast-informed linear programming optimisation and concluding with the social technical analysis of the current infrastructures.

The primary goal of this codebase is to translate historical EV charging data from Perth and Kinross Council into a cost-optimal, 24-hour charging schedule that respects real-world operational constraints. Key libraries used include Pandas for data manipulation, Scikit-learn and XGBoost for machine learning, Optuna for hyperparameter tuning, and PuLP for the optimisation.

# **This Python notebook implements the end-to-end data science pipeline for the dissertation, covering data cleaning, feature engineering, the training and evaluation of multiple forecasting models, and the final forecast-informed optimisation simulation.**

## **Mount Google Drive**
This initial step connects my Google Colab notebook to the user's personal Google Drive, allowing the script to access datasets or other necessary files stored there.
"""

from google.colab import drive
drive.mount('/content/drive')

"""Description:

This is a recommended procedure for working with files in a Colab environment. The first line imports the necessary drive module from the google.colab library. The drive.mount() function then executes the mounting process. When run, it prompts the user with a URL to authorize access. Once the authorization code is provided, the user's Google Drive becomes accessible within the Colab environment at the path /content/drive.

## **Setting Up the Tools**

This block prepares my coding environment.

Importing Libraries: I am loading the essential toolkits for the project.

- pandas: For handling my data in tables.

- numpy: For numerical calculations.

- matplotlib.pyplot: For creating graphs and charts.

Hiding Warnings: The warnings.filterwarnings('ignore') line keeps my output clean by hiding non-critical warning messages.

Improving Display: The pd.set_option lines make sure that when I view my data tables, they are displayed clearly without being cut off.
"""

# Import required libraries
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import warnings

# Suppress warnings for cleaner output
warnings.filterwarnings('ignore')

# Set pandas display options for easier inspection
pd.set_option('display.max_columns', 50)
pd.set_option('display.width', 120)

"""## **Loading My Data**
In this step, I'm loading my dataset into the programming environment.

- df = pd.read_csv(...): I *italicized text*'m using the pandas library which I imported earlier, to read my charging data from a CSV file. A CSV is just a simple text file that stores data in a table format, like a spreadsheet.

- 'EV_Charge_Sessions_2016-2019_perth.csv': This is the name of my data file. I'm telling the program to find this specific file.

- encoding='latin1': Sometimes, data files have special characters that the computer doesn't recognize by default. I've added this part to make sure all the characters in my file are read correctly, preventing any errors during the import.

- df: After loading the data, I'm storing it in a variable named df. Think of df as a container, which now holds my entire dataset as a neatly organized table called a DataFrame. From now on, whenever I want to work with my data, I'll use this df variable.
"""

# Load Data and Initial Inspection
csv_path = "/content/drive/MyDrive/CIM/Dissertation/Data/PKC_EVChargeStationUse_Sept2016toAug2019.csv"

# Read CSV into a DataFrame
df = pd.read_csv(csv_path)

# Initial inspection
print("Shape of dataset:", df.shape)
print("\nFirst five rows:")
df.head()

"""## **Initial Data Overview**

I am performing a quick health check on the dataset. I display the column names to see the variables I'm working with, check data types and for missing values, and generate summary statistics to understand the basic distribution of my numerical data.


"""

# Initial inspection
print("\nColumn names:", df.columns.tolist())
print("\nData types and missing values:")
df.info()
print("\nSummary statistics:")
df.describe()

"""## **Data Cleaning and Preprocessing**

Here, I am refining the dataset to ensure its quality and consistency for analysis. I first standardize all column names by removing spaces and special characters, which makes them easier to work with. I then remove any rows that are missing critical information, specifically dates, times, or the total energy delivered. I also filter out any records showing non-positive energy consumption, as these represent invalid charging sessions. Finally, I reset the dataframe's index to ensure clean, sequential numbering after the row deletions and confirm the new size of the dataset.
"""

# Data Cleaning and Preprocessing

# Standardise column names for easier access
df.columns = [col.strip().replace(' ', '_').replace('/', '_') for col in df.columns]

# Check missing values again, now with new names
print("Missing values per column:\n", df.isnull().sum())

# Remove rows where critical columns are missing (dates, times, kWh)
critical_cols = ['Start_Date', 'Start_Time', 'End_Date', 'End_Time', 'Total_kWh']
df = df.dropna(subset=critical_cols)

# Remove rows with non-positive energy delivered (Total_kWh <= 0)
df = df[df['Total_kWh'] > 0]

# Reset index after dropping rows
df = df.reset_index(drop=True)

print(f"After cleaning, dataset shape: {df.shape}")

"""Quick check to confirm the new shape of my data after dropping missing values."""

# Confirm nomissing values
print("Missing values per column:\n", df.isnull().sum())
df.describe()

"""## **Calculating Charging Duration**

I am converting the separate date and time columns into a unified datetime format for both the start and end of each charging session. This allows me to calculate the precise duration of every session, which I then convert into hours for straightforward analysis. I also clean the data by removing any records where the start or end times could not be properly parsed or where the calculated duration is invalid (i.e., zero or negative). Finally, I display the new shape of my dataset and the first few rows of the new time-related columns to verify the changes.
"""

# Parse date and time columns and calculate charging session duration

# Combine and convert to datetime
df['StartDateTime'] = pd.to_datetime(df['Start_Date'] + ' ' + df['Start_Time'], dayfirst=True, errors='coerce')
df['EndDateTime']   = pd.to_datetime(df['End_Date'] + ' ' + df['End_Time'], dayfirst=True, errors='coerce')

# Remove rows where parsing failed (if any)
df = df.dropna(subset=['StartDateTime', 'EndDateTime'])

# Compute duration in hours
df['ChargingDurationHrs'] = (df['EndDateTime'] - df['StartDateTime']).dt.total_seconds() / 3600

# Remove rows with zero or negative durations (invalid sessions)
df = df[df['ChargingDurationHrs'] > 0]

print(f"After parsing datetimes and computing duration, shape: {df.shape}")
print(df[['StartDateTime', 'EndDateTime', 'ChargingDurationHrs']].head())

"""## **Aggregating Data to an Hourly Level**

I am now transforming the data from individual charging sessions into a format suitable for time-series analysis. First, I calculate the average power (in kW) for each session by dividing the total energy by the duration. After cleaning up any potential calculation errors, I set the session's start time as the main index for my dataset. This allows me to resample the data into fixed hourly intervals. For each hour, I calculate the average power demand and the sum of energy delivered, which gives me a clear, aggregated view of the network's usage over time. Finally, I display the first few rows of this new hourly data to verify the results.
"""

# Compute average charging power (kW) per session
df['Pavg_kW'] = df['Total_kWh'] / df['ChargingDurationHrs']

# Remove any infinite or NaN average power values (from very short or odd sessions, if any remain)
df = df.replace([np.inf, -np.inf], np.nan).dropna(subset=['Pavg_kW'])

# Set start time as the index for time series analysis
df = df.set_index('StartDateTime').sort_index()

# Resample to hourly intervals (you can change 'H' to '30T' for half-hourly etc.)
df_hourly = df.resample('H').agg({
    'Pavg_kW': 'mean',             # average power demand that hour
    'Total_kWh': 'sum',            # total energy delivered that hour
    'ChargingDurationHrs': 'sum',  # total charging time that hour
})

# Preview results
print(df_hourly.head())

"""## **Identifying and Removing Outliers**

I am now identifying and removing anomalous data points to improve the quality of my dataset. I first define a valid range for energy consumption per session using the Interquartile Range (IQR) method, a standard statistical technique for outlier detection. To visualize these potential outliers, I generate a boxplot and a scatter plot, which help me confirm the calculated thresholds. After this visual inspection, I filter out any charging sessions that fall outside these established upper and lower bounds. Finally, I print a confirmation of how many outliers were removed and the final size of the cleaned dataset.
"""

# Remove non-positive charging before outlier checks

df = df[df['Total_kWh'] > 0]

# Outlier Detection (IQR method) for Total_kWh at session level
Q1 = df['Total_kWh'].quantile(0.25)
Q3 = df['Total_kWh'].quantile(0.75)
IQR = Q3 - Q1
lower_bound = Q1 - 1.5 * IQR
upper_bound = Q3 + 1.5 * IQR

# Ensure StartDateTime is available and parsed correctly
if 'StartDateTime' not in df.columns:
    df['StartDateTime'] = pd.to_datetime(
        df['Start_Date'].astype(str).str.strip() + ' ' + df['Start_Time'].astype(str).str.strip(),
        dayfirst=True, errors='coerce'
    )
df = df.dropna(subset=['StartDateTime'])

# Visualise session-level outliers before removal
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 5))
plt.boxplot(df['Total_kWh'].dropna(), vert=False)
plt.title('Boxplot of Session Total kWh (Potential Outliers)')
plt.xlabel('Session Total kWh')
plt.tight_layout()
plt.savefig("Boxplot before outlier.png", dpi=1200, bbox_inches='tight')
plt.show()

plt.figure(figsize=(12, 4))
plt.scatter(df['StartDateTime'], df['Total_kWh'], s=6, label='Session Total kWh', alpha=0.7)
plt.axhline(upper_bound, color='r', linestyle='--', label='Upper bound')
plt.axhline(lower_bound, color='g', linestyle='--', label='Lower bound')
plt.legend()
plt.title('Session Total kWh Over Time (Outlier Thresholds Shown)')
plt.ylabel('Total kWh')
plt.xlabel('Start DateTime')
plt.tight_layout()
plt.savefig("Outliertreshold before outlier.png", dpi=1200, bbox_inches='tight')
plt.show()

# Flag and remove session outliers
n_before = len(df)
df['Outlier'] = ~df['Total_kWh'].between(lower_bound, upper_bound)
n_outliers = df['Outlier'].sum()
print(f"Identified {n_outliers} outlier sessions out of {n_before} ({n_outliers/n_before:.2%})")
df = df[df['Total_kWh'].between(lower_bound, upper_bound)].copy()

# Drop the flag column after filtering
if 'Outlier' in df.columns:
    df = df.drop(columns='Outlier')
df = df.reset_index(drop=True)

print("After cleaning and outlier removal, session-level shape:", df.shape)

# Visualise session-level outliers after removal
import matplotlib.pyplot as plt

plt.figure(figsize=(8, 5))
plt.boxplot(df['Total_kWh'].dropna(), vert=False)
plt.title('Boxplot of Session Total kWh (Potential Outliers)')
plt.xlabel('Session Total kWh')
plt.tight_layout()
plt.savefig("Boxplot after outlier.png", dpi=1200, bbox_inches='tight')
plt.show()

plt.figure(figsize=(12, 4))
plt.scatter(df['StartDateTime'], df['Total_kWh'], s=6, label='Session Total kWh', alpha=0.7)
plt.axhline(upper_bound, color='r', linestyle='--', label='Upper bound')
plt.axhline(lower_bound, color='g', linestyle='--', label='Lower bound')
plt.legend()
plt.title('Session Total kWh Over Time (Outlier Thresholds Shown)')
plt.ylabel('Total kWh')
plt.xlabel('Start DateTime')
plt.tight_layout()
plt.savefig("Outliertreshold after outlier.png", dpi=1200, bbox_inches='tight')
plt.show()

"""## **Feature Engineering for Forecasting**

To prepare the data for my forecasting model, I am creating several new time-based features from the hourly data. I extract standard calendar variables like the hour, day of the week, month, and year, which will help the model identify cyclical patterns. I also add custom features, such as a "WorkingStatus" indicator to distinguish weekdays from weekends, and a "Season" variable tailored to UK meteorological definitions. Furthermore, I engineer two more complex features: a cumulative hourly average demand and a 24-hour rolling average demand. These are designed to provide the model with a sense of both long-term and recent historical trends. Finally, I remove the initial rows that have incomplete data due to the rolling calculations, ensuring the dataset is clean for the next stage.
"""

# Time-Based and Rolling Feature Engineering for EV Load Forecasting

# Hour of day (0–23)
df_hourly['ConnectionHour'] = df_hourly.index.hour

# Day of week (0=Monday, 6=Sunday)
df_hourly['DayofWeek'] = df_hourly.index.dayofweek

# Week number (ISO standard)
df_hourly['Week'] = df_hourly.index.isocalendar().week.astype(int)

# Month (1–12)
df_hourly['Month'] = df_hourly.index.month

# Year
df_hourly['Year'] = df_hourly.index.year

# Season (UK Meteorological seasons)
def uk_season(month):
    if month in [12, 1, 2]: return 'Winter'
    if month in [3, 4, 5]: return 'Spring'
    if month in [6, 7, 8]: return 'Summer'
    return 'Autumn'
df_hourly['Season'] = df_hourly['Month'].apply(uk_season)

# Working day indicator (1 = Mon–Fri, 0 = Sat/Sun)
df_hourly['WorkingStatus'] = df_hourly['DayofWeek'].apply(lambda x: 1 if x < 5 else 0)

# HourlyAverageDemand (cumulative mean for each hour of day, excluding current hour)
df_hourly['HourlyAverageDemand'] = (
    df_hourly.groupby('ConnectionHour')['Total_kWh']
    .transform(lambda x: x.expanding().mean().shift(1))
)

# Previous24HrAverageDemand (rolling mean of previous 24 hours, shifted by 1 to prevent leakage)
df_hourly['Previous24HrAverageDemand'] = (
    df_hourly['Total_kWh'].shift(1).rolling(window=24, min_periods=1).mean()
)

# Remove rows with NaNs introduced by rolling/expanding calculations
df_hourly = df_hourly.dropna(subset=['HourlyAverageDemand', 'Previous24HrAverageDemand'])

# Preview the features
print(df_hourly[['ConnectionHour', 'DayofWeek', 'Week', 'Month', 'Year', 'Season',
                 'WorkingStatus', 'HourlyAverageDemand', 'Previous24HrAverageDemand']].head(10))

"""## **Data Preparation**

I am applying one-hot encoding to the "Season" column. This technique converts the categorical season labels (like 'Winter', 'Spring') into a numerical format that machine learning models can understand, creating a separate binary (0 or 1) column for each season. This is the final preparation step before feeding the data into the forecasting model. I then display the head of the dataframe to confirm the new season columns have been added correctly.
"""

# One-hot encode the 'Season' column (creates new columns for each unique season)
df_hourly = pd.get_dummies(df_hourly, columns=['Season'], drop_first=True)

df_hourly.head()

season_cols = [col for col in df_hourly.columns if col.startswith('Season_')]
df_hourly[season_cols] = df_hourly[season_cols].astype(int)

df_hourly.head()

"""# **Exploratory Data Analysis (EDA)**

## **Final Data Verification**

Before proceeding to the modeling stage, I am performing a final check on the prepared hourly data. I print the shape to confirm the number of rows and columns, use .describe() to review the statistical summary of my features, and double-check for any remaining missing values to ensure the dataset is complete and clean.
"""

# Quick Data Check

print("Hourly DataFrame shape:", df_hourly.shape)
print(df_hourly.describe())
print("\nMissing values per column:\n", df_hourly.isnull().sum())

"""## **Visualising Daily Energy Consumption**

I am generating a plot to visualize the total energy delivered across the network each day. By resampling the hourly data to a daily frequency and summing the Total_kWh, I can create a high-level overview of the demand over time. This helps in identifying long-term trends, seasonal patterns, and any significant anomalies in the historical data.
"""

import matplotlib.pyplot as plt

plt.figure(figsize=(12,4))
df_hourly['Total_kWh'].resample('D').sum().plot()
plt.title('Total Energy Delivered per Day')
plt.xlabel('Date')
plt.ylabel('Energy (kWh)')
plt.tight_layout()
plt.show()

"""## **Visualising Average Hourly Demand**

I am now creating a bar chart to visualize the average energy demand for each hour of the day. I group the data by the "ConnectionHour" and then calculate the mean Total_kWh for each hour. This visualisation is crucial for understanding the daily demand cycle, clearly showing at which times of day the charging network is most and least busy. I then save the chart as a high-resolution image for use in my dissertation.
"""

plt.figure(figsize=(10,4))
df_hourly.groupby('ConnectionHour')['Total_kWh'].mean().plot(kind='bar')
plt.title('Average Hourly EV Demand by Hour of Day')
plt.xlabel('Hour of Day')
plt.ylabel('Average Hourly Demand (kWh)')
plt.tight_layout()
plt.savefig("Average hourly demand.png", dpi=1200, bbox_inches='tight')
plt.show()

"""## **Visualising Demand by Day of the Week**

I am creating a bar chart to show how the average hourly energy demand varies across different days of the week. To make the chart more readable, I first map the numerical day-of-the-week codes to their corresponding names (e.g., 0 to 'Mon'). Then, I group the data by these day names and calculate the mean Total_kWh. This visualisation helps me identify weekly patterns, such as potential differences in charging behavior between weekdays and weekends.
"""

dow_map = {0: 'Mon', 1: 'Tue', 2: 'Wed', 3: 'Thu', 4: 'Fri', 5: 'Sat', 6: 'Sun'}
mean_by_dow = df_hourly.groupby('DayofWeek')['Total_kWh'].mean().rename(index=dow_map)

plt.figure(figsize=(8,4))
mean_by_dow.plot(kind='bar')
plt.title('Mean Hourly Demand by Day of Week')
plt.ylabel('Average Hourly Demand (kWh)')
plt.tight_layout()
plt.show()

"""## **Visualising Average Power by Hour**

I am creating a bar chart to examine the average charging power (in kW) for each hour of the day. By grouping the data by the hour and calculating the mean of the average power, this plot helps me identify the times when charging sessions are typically most power-intensive. This provides a different perspective from total energy consumption, focusing instead on the rate of energy transfer.
"""

# Average Pavg by Hour of Day
plt.figure(figsize=(12,4))
df_hourly.groupby('ConnectionHour')['Pavg_kW'].mean().plot(kind='bar', color='skyblue')
plt.title('Average Pavg by Hour of Day')
plt.xlabel('Hour')
plt.ylabel('Average Pavg (kW)')
plt.tight_layout()
plt.show()

"""## **Visualising Average Power by Day of the Week**

I am generating a bar chart to see how the average charging power changes depending on the day of the week. I first map the numerical day codes to their full names to make the chart labels clear. Then, I group the data by these day names and calculate the mean average power for each. The resulting bar chart allows me to easily compare the typical charging intensity across the week, identifying any weekly usage patterns.
"""

# Average Pavg by Day of Week
plt.figure(figsize=(10,4))
dow_map = {0: 'Monday', 1: 'Tuesday', 2: 'Wednesday', 3: 'Thursday', 4: 'Friday', 5: 'Saturday', 6: 'Sunday'}
avg_by_dow = df_hourly.groupby('DayofWeek')['Pavg_kW'].mean().rename(index=dow_map)
avg_by_dow.plot(kind='bar', color='salmon')
plt.title('Average Pavg by Day of Week')
plt.xlabel('Day')
plt.ylabel('Average Pavg (kW)')
plt.tight_layout()
plt.show()

"""## **Visualising Seasonal Power Trends**

I am creating a bar chart to analyze the average charging power across all the weeks of the year. By grouping my data by the ISO week number and plotting the mean of the average power, this visualisation allows me to identify any long-term seasonal trends or variations in charging intensity over the course of a year.
"""

# Average Pavg by ISO Week Number
plt.figure(figsize=(18,4))
df_hourly.groupby('Week')['Pavg_kW'].mean().plot(kind='bar', color='mediumseagreen')
plt.title('Average Pavg by ISO Week Number')
plt.xlabel('Week Number')
plt.ylabel('Average Pavg (kW)')
plt.tight_layout()
plt.show()

"""## **Visualising Monthly Power Trends**

I am creating a bar chart to analyze how the average charging power varies throughout the year. To make the visualization clear, I first map the numerical months to their full names. Then, by grouping the data by month and plotting the mean of the average power, I can effectively identify any seasonal patterns or trends in charging intensity.
"""

# Average Pavg by Month
plt.figure(figsize=(12,4))
month_map = {1:'January', 2:'February', 3:'March', 4:'April', 5:'May', 6:'June', 7:'July', 8:'August', 9:'September', 10:'October', 11:'November', 12:'December'}
avg_by_month = df_hourly.groupby('Month')['Pavg_kW'].mean().rename(index=month_map)
avg_by_month.plot(kind='bar', color='orchid')
plt.title('Average Pavg by Month')
plt.xlabel('Month')
plt.ylabel('Average Pavg (kW)')
plt.tight_layout()
plt.show()

"""## **Visualising Average Power by Season**

To create this visualization, I first need to reverse the one-hot encoding I performed earlier. I create a new "Season_Label" column by interpreting the binary season columns, which allows me to group the data by season again. Once that's done, I plot the mean average power for each season, ordering them logically. This bar chart provides a clear, direct comparison of charging intensity across Spring, Summer, Autumn, and Winter.
"""

# Create a new 'Season_Label' column based on the dummies (assuming you used drop_first=True and Autumn is base)
def infer_season(row):
    if row['Season_Spring'] == 1:
        return 'Spring'
    elif row['Season_Summer'] == 1:
        return 'Summer'
    elif row['Season_Winter'] == 1:
        return 'Winter'
    else:
        return 'Autumn'  # all zeros means Autumn

df_hourly['Season_Label'] = df_hourly.apply(infer_season, axis=1)

plt.figure(figsize=(7,4))
df_hourly.groupby('Season_Label')['Pavg_kW'].mean().loc[['Spring','Summer','Autumn','Winter']].plot(kind='bar', color='goldenrod')
plt.title('Average Pavg by Season')
plt.xlabel('Season')
plt.ylabel('Average Pavg (kW)')
plt.tight_layout()
plt.show()

"""## **Visualising Year-Over-Year Trends**

I am generating two bar charts to analyze trends across the different years in my dataset. The first chart shows the average charging power for each year, which helps me understand if the typical intensity of charging sessions has been changing over time. The second chart visualises the sum of the average power values for each year, providing a clear indication of the overall growth in network usage from one year to the next.
"""

# Average Pavg by Year (if multiple years in dataset)
if 'Year' in df_hourly.columns:
    plt.figure(figsize=(10,4))
    df_hourly.groupby('Year')['Pavg_kW'].mean().plot(kind='bar', color='slateblue')
    plt.title('Average Pavg by Year')
    plt.xlabel('Year')
    plt.ylabel('Average Pavg (kW)')
    plt.tight_layout()
    plt.show()

    # Sum of Pavg by Year
    plt.figure(figsize=(10,4))
    df_hourly.groupby('Year')['Pavg_kW'].sum().plot(kind='bar', color='slateblue')
    plt.title('Sum of Pavg by Year')
    plt.xlabel('Year')
    plt.ylabel('Sum of Pavg (kW)')
    plt.tight_layout()
    plt.show()

!pip install statsmodels

"""Analysing Temporal Dependencies

I am generating an autocorrelation plot for the average power (Pavg_kW) to understand its temporal structure. This plot measures the correlation of the time series with itself at different time lags. By examining these correlations for the past 50 hours, I can identify significant repeating patterns, such as a strong 24-hour cycle, which is a crucial step in diagnosing the data's characteristics before building a forecasting model.
"""

# Autocorrelation Plot of Pavg
from statsmodels.graphics.tsaplots import plot_acf

# Plot autocorrelation for the first 48 lags (2 days for hourly data)
plt.figure(figsize=(7,5))
plot_acf(df_hourly['Pavg_kW'].dropna(), lags=50)
plt.title('Autocorrelation of Pavg (First 48 Lags)')
plt.tight_layout()
plt.show()

"""How to Interpret This Plot
1. Spike at Lag 0

The value is always 1 at lag 0—this is just the correlation of the series with itself.

2. Lag 1 to Lag 48

You see the autocorrelation coefficient for lags 1 to 50.

Each point is the correlation between the series and itself shifted by that many hours.

3. What the shape means:

The pattern is a gentle oscillation: small positive and negative autocorrelations that repeat with a moderate period (about 24 hours, as you can see from a slight cycle).

No extremely strong daily (24h) or weekly (168h) repeating structure, but there is weak seasonality—visible as the small “bumps” near every 24 lags.

The autocorrelation values are mostly near zero except for the first few lags, which means:

There is some short-term correlation: The value at hour t is weakly related to recent hours (this is typical in real energy or demand series, where consumption or demand is “sticky”).

But the series is not dominated by strong regular cycles (e.g. if this were household electricity, you'd see a huge peak at 24, 48, etc.; your workplace/public EV demand is more diffused).

4. Stationarity

The lack of strong upward or downward trend in the autocorrelation coefficients (after lag 0) suggests your series is fairly stationary in the mean.

What Does This Mean for Forecasting?
Some benefit to including lagged demand features: e.g., using demand at t-1, t-2 as predictors.

Possible value in capturing daily effects: Rolling features or periodic encodings (hour of day, day of week) could help.

No dominant seasonality: Your EV load is not simply repeating the same pattern every day/hour, likely due to workplace and public station context.

## **Visualising Weekly Demand Patterns**

I am creating a pivot table to structure the average power demand with the days of the week as rows and hours of the day as columns. I then use this table to generate a heatmap, which provides a clear, colour-coded visualisation of when charging demand is at its highest and lowest. This allows me to easily spot patterns, such as peak times during weekdays versus different usage on weekends, all within a single, intuitive chart. Finally, I customise the labels to ensure the chart is easy to interpret.
"""

# Heatmap of Average Power Demand by Day of Week and Hour of Day
import seaborn as sns

pivot = df_hourly.pivot_table(index='DayofWeek', columns='ConnectionHour', values='Pavg_kW', aggfunc='mean')
plt.figure(figsize=(14,7))
sns.heatmap(pivot, cmap='YlOrRd', linewidths=0.5)
plt.title('Heatmap of Average Power Demand\nby Day of Week and Hour of Day')
plt.xlabel('Hour of Day')
plt.ylabel('Day of Week')
plt.yticks(ticks=np.arange(0.5, 7.5), labels=['Monday','Tuesday','Wednesday','Thursday','Friday','Saturday','Sunday'], rotation=0)
plt.tight_layout()
plt.show()

"""## **Analysing Feature Relationships**

I am generating a correlation heatmap to understand the relationships between all the numerical features in my dataset. First, I select only the columns containing numerical data and calculate their correlation matrix. I then use this matrix to create a heatmap, which provides a colour-coded grid where each cell shows the correlation between two features. I have also annotated the map with the specific correlation values to make it easier to interpret. This visualisation is a powerful tool for identifying which features are strongly related to each other, which is an important consideration for my forecasting model. Finally, I save a high-resolution version of this chart for my dissertation.
"""

# Correlation Heatmap (of all numeric features)
plt.figure(figsize=(12,10))
num_cols = df_hourly.select_dtypes(include=np.number).columns
corr = df_hourly[num_cols].corr()
sns.heatmap(corr, annot=True, cmap='coolwarm', fmt='.2f')
plt.title('Correlation Heatmap of Hourly Dataset')
plt.tight_layout()
plt.savefig("Heatmap.png", dpi=1200, bbox_inches='tight')
plt.show()

"""Features to Drop (with Reasoning)
1. Target column
'Total_kWh' – This is your target, so must not be included as a feature.

2. Highly correlated features (choose only one for each highly-correlated group)
'Month' and 'Week' – Both are highly correlated with each other and with 'Year'. If your aim is short-term load forecasting, you typically keep only one or drop all if you have more granular time/season features (like 'Season').

'Year' – Only needed if you expect a strong trend; otherwise, can drop to prevent collinearity.

3. Duplicates or categorical features already captured elsewhere
'DayOfWeek' – If you keep 'WorkingStatus' (which is a binarised/aggregated version), drop the original.

Any "Code" columns created for encoding if you use one-hot or ordinal encoding elsewhere.

'ChargingDurationHrs' – Unless justified by your research question, as it's a function of the total kWh and session times. If you keep only demand forecasting, you can drop this.

4. Pavg_kW
'Pavg_kW' – Average power per hour; since you are forecasting energy, not power, this may be dropped unless used as a useful lagged feature.

## **Encoding Day of the Week**

I am now converting the numerical DayofWeek column into a format that is more suitable for machine learning models. First, I map the numbers (0-6) to their corresponding day names for clarity. Then, I apply one-hot encoding to these names, which creates new binary columns for each day of the week. This allows my model to treat each day as a distinct category without assuming any numerical relationship between them. Finally, I remove the original DayofWeek column as it is now redundant.
"""

# First, let's map the day of the week numbers to names for clarity in the new columns
dow_map = {0: 'Mon', 1: 'Tue', 2: 'Wed', 3: 'Thu', 4: 'Fri', 5: 'Sat', 6: 'Sun'}
df_hourly['DayofWeek_Name'] = df_hourly['DayofWeek'].map(dow_map)

# Now, one-hot encode the new 'DayofWeek_Name' column.
# drop_first=True is good practice for linear models to avoid multicollinearity.
df_hourly = pd.get_dummies(df_hourly, columns=['DayofWeek_Name'], prefix='Day', drop_first=True)

# You can now drop the original numeric 'DayofWeek' column as it's redundant.
if 'DayofWeek' in df_hourly.columns:
    df_hourly = df_hourly.drop(columns=['DayofWeek'])

print("DataFrame columns after one-hot encoding Day of Week:")
print(df_hourly.columns)

"""## **Defining Features and Target**

I am now finalising the dataset for the modelling stage. I am formally separating my data into two distinct components: the feature matrix (X), which contains all the input variables the model will learn from, and the target vector (y), which is the Total_kWh that I want to predict. To create the feature matrix, I explicitly drop the target variable and any other columns that are not required as inputs, such as redundant date parts or the original categorical labels. Finally, I print the list of remaining columns in my feature matrix to confirm that the separation has been performed correctly.
"""

# Define the list of columns you want to drop to create your features.
features_to_drop = [
    'Total_kWh',              # The target variable
    'Pavg_kw',
    'Month',
    'Week',
    'Year',
    'ChargingDurationHrs',
]

# Add the categorical 'Season_Label' if it exists
if 'Season_Label' in df_hourly.columns:
    features_to_drop.append('Season_Label')


# Define your target vector 'y' from the original DataFrame
y = df_hourly['Total_kWh']

# Define your feature matrix 'X' by dropping the columns from the original DataFrame
X = df_hourly.drop(columns=[col for col in features_to_drop if col in df_hourly.columns])

# Verify the final feature set
print("Final feature columns in X:")
print(X.columns.tolist())

print("Feature columns in X:")
print(X.columns.tolist())

if 'Season_Label' in X.columns:
    X = X.drop(columns=['Season_Label'])

print(X.tail())

"""## **Converting Data Types**

I am identifying any columns in my feature set that have a boolean (True/False) data type and converting them into an integer format, where True becomes 1 and False becomes 0. This is a final standardisation step to ensure all my input features are numerical, which is a requirement for the machine learning algorithms I will be using. I then display the last few rows of the data to confirm that the changes have been applied correctly.
"""

# Identify all columns with the boolean data type
bool_columns = X.select_dtypes(include='bool').columns

# Convert those columns to integer type (True becomes 1, False becomes 0)
X[bool_columns] = X[bool_columns].astype(int)

# Display the end of the DataFrame to confirm the change
print(X.tail())

"""## **Splitting the Data for Model Evaluation**

To properly evaluate my forecasting model, I am splitting the data chronologically into three distinct sets. I first ensure the data is sorted by time. Then, I allocate the first 70% of the data for training the model, the next 15% as a validation set for tuning the model, and the final 15% as a test set. This final test set remains entirely unseen by the model during training and tuning, allowing for an unbiased assessment of its real-world performance. Finally, I print the dimensions of each set to confirm the split was successful.
"""

# Ensure data is sorted by time
X = X.sort_index()
y = y.sort_index()

n = len(X)
train_size = int(n * 0.7)
val_size = int(n * 0.15)
test_size = n - train_size - val_size

X_train = X.iloc[:train_size]
y_train = y.iloc[:train_size]
X_val = X.iloc[train_size:train_size + val_size]
y_val = y.iloc[train_size:train_size + val_size]
X_test = X.iloc[train_size + val_size:]
y_test = y.iloc[train_size + val_size:]

print("Train:", X_train.shape)
print("Val:", X_val.shape)
print("Test:", X_test.shape)

"""## **Visualising the Data Split**

I am creating a plot to visualise how my time series data has been divided into the training, validation, and test sets. By plotting each of the three sets in a different colour on the same timeline, I can clearly see the chronological split. This graph serves as a crucial visual confirmation that the data has been partitioned correctly before I proceed with model training. Finally, I customise the plot with a title and labels and save it as a high-resolution image for my dissertation.
"""

# Plotting
plt.figure(figsize=(14, 4))
plt.plot(y_train.index, y_train, label='Training (70%)', color='steelblue', linewidth=1)
plt.plot(y_val.index, y_val, label='Validation (15%)', color='darkorange', linewidth=1)
plt.plot(y_test.index, y_test, label='Testing (15%)', color='forestgreen', linewidth=1)

# Customisation
plt.title("70/15/15 Chronological Split of Total KWh")
plt.xlabel("Date")
plt.ylabel(" Power Demand Power (kWh)")
plt.legend()
plt.tight_layout()

# Save high-quality figure
plt.savefig("pavg_split_visualisation.png", dpi=1200, bbox_inches='tight')
plt.show()

"""## **Handling Missing Data**

To ensure my model runs without errors, I am now addressing any gaps in the data that may have resulted from the feature engineering steps. I am using an imputer to fill any missing values with the mean of the respective column. Crucially, I calculate this mean using only the training data to prevent any information from the validation or test sets leaking into the training process. I then apply this same learned imputation to all three data splits—training, validation, and test—to ensure consistent processing. Finally, I run a check to confirm that no missing values remain in any of the datasets.
"""

from sklearn.impute import SimpleImputer
import pandas as pd

# Create the imputer
imputer = SimpleImputer(strategy='mean')

# Fit the imputer on the training data and transform X_train
print("Imputing NaNs in X_train...")
# We save the columns/index because transform() returns a NumPy array
X_train_cols = X_train.columns
X_train_idx = X_train.index
X_train = pd.DataFrame(imputer.fit_transform(X_train), columns=X_train_cols, index=X_train_idx)

# Transform X_val and X_test using the same fitted imputer
print("Imputing NaNs in X_val and X_test...")
X_val_cols = X_val.columns
X_val_idx = X_val.index
X_val = pd.DataFrame(imputer.transform(X_val), columns=X_val_cols, index=X_val_idx)

X_test_cols = X_test.columns
X_test_idx = X_test.index
X_test = pd.DataFrame(imputer.transform(X_test), columns=X_test_cols, index=X_test_idx)

print("\nImputation complete. Checking for remaining NaNs:")
print(f"NaNs in X_train: {X_train.isnull().sum().sum()}")
print(f"NaNs in X_val: {X_val.isnull().sum().sum()}")
print(f"NaNs in X_test: {X_test.isnull().sum().sum()}")

"""## **Normalising the Data**

I am now scaling all my data to ensure that every feature has a consistent range, which is a crucial step for many machine learning algorithms. Using a MinMaxScaler, I am normalising both my feature sets (X) and my target variable (y) to a value between 0 and 1. To prevent any data leakage, I first fit the scaler exclusively on my training data. I then use this same fitted scaler to transform the training, validation, and test sets, ensuring that the same scaling parameters are applied consistently across all my data splits.
"""

from sklearn.preprocessing import MinMaxScaler
import pandas as pd

# --- 1. Scale the Feature Sets (X) ---
scaler_X = MinMaxScaler()

# Fit the scaler on the training data ONLY, then transform all three sets
print("Scaling feature sets (X)...")
X_train_scaled = scaler_X.fit_transform(X_train)
X_val_scaled = scaler_X.transform(X_val)
X_test_scaled = scaler_X.transform(X_test) # <-- This line creates X_test_scaled


# --- 2. Scale the Target Sets (y) ---
scaler_y = MinMaxScaler()

# Fit the scaler on the training target ONLY, then transform all three sets
print("Scaling target sets (y)...")
y_train_scaled = scaler_y.fit_transform(y_train.to_numpy().reshape(-1, 1))
y_val_scaled = scaler_y.transform(y_val.to_numpy().reshape(-1, 1))
y_test_scaled = scaler_y.transform(y_test.to_numpy().reshape(-1, 1)) # <-- This line creates y_test_scaled


print("\nAll data splits have been successfully scaled.")
print(f"Shape of X_train_scaled: {X_train_scaled.shape}")
print(f"Shape of X_test_scaled: {X_test_scaled.shape}")

"""## **Training a Baseline Model**

As a baseline for comparison, I am now training a standard Linear Regression model. I first train, or 'fit', the model using my prepared training data. Once trained, I use it to make predictions on both the validation and the unseen test datasets. Finally, to judge its performance, I calculate three key metrics for both sets: the Mean Absolute Error (MAE), the Root Mean Squared Error (RMSE), and the R-squared value. This gives me a clear, quantitative measure of how accurately this basic model can predict energy demand.
"""

from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# 1. Instantiate and fit the model
lr = LinearRegression()
lr.fit(X_train, y_train)

# 2. Predict on validation and test sets
y_val_pred = lr.predict(X_val)
y_test_pred = lr.predict(X_test)

# 3. Evaluate performance
def print_metrics(y_true, y_pred, set_name='Set'):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = mean_squared_error(y_true, y_pred) ** 0.5
    r2 = r2_score(y_true, y_pred)
    print(f"{set_name} MAE: {mae:.2f}")
    print(f"{set_name} RMSE: {rmse:.2f}")
    print(f"{set_name} R^2: {r2:.3f}")

print_metrics(y_val, y_val_pred, 'Validation')
print_metrics(y_test, y_test_pred, 'Test')

"""## **Visualising Model Performance**

To visually assess the performance of my Linear Regression model, I am plotting its predictions against the actual values from the test set. I create a line graph that shows the true energy demand alongside the demand predicted by the model for the same time period. This provides an intuitive way to see how well the model's forecasts track the real-world data. Finally, I add a title and labels for clarity and save the visualisation as a high-resolution image for my dissertation.
"""

import matplotlib.pyplot as plt

plt.figure(figsize=(12,5))
plt.plot(y_test.index, y_test, label='Actual', alpha=0.8)
plt.plot(y_test.index, y_test_pred, label='Predicted', alpha=0.8)
plt.legend()
plt.title('OLS: Actual vs Predicted Total kWh (Test Set)')
plt.xlabel('Datetime')
plt.ylabel('Total kWh')
plt.tight_layout()
plt.savefig("OLS_visualisation.png", dpi=1200, bbox_inches='tight')
plt.show()

"""## **Training a Statistical Time-Series Model**

I am now implementing a SARIMA model, which is a powerful statistical method specifically designed for time-series data that has seasonal patterns. I have configured the model to recognise a daily seasonality by setting the seasonal period to 24 hours. After fitting the model to the training data, I generate forecasts for both the validation and test periods. Finally, I evaluate its performance using the same set of metrics as the baseline model to provide a direct comparison.


"""

from statsmodels.tsa.statespace.sarimax import SARIMAX

# Try with seasonal order (e.g., daily seasonality, s=24)
model = SARIMAX(y_train, order=(2,0,2), seasonal_order=(1,1,1,24))
sarima_fit = model.fit()
y_val_pred_sarima = sarima_fit.forecast(steps=len(y_val))
y_test_pred_sarima = sarima_fit.forecast(steps=len(y_val)+len(y_test))[-len(y_test):]
print_metrics(y_val, y_val_pred_sarima, 'SARIMA Validation')
print_metrics(y_test, y_test_pred_sarima, 'SARIMA Test')

"""## **Visualising SARIMA Model Performance**

To visually evaluate the SARIMA model, I am plotting its forecasts directly against the actual Total_kWh values from the test set. This line graph provides a clear, side-by-side comparison, allowing me to intuitively judge how well the model's predictions align with the real-world data and capture its patterns. I then add a title and labels for clarity before saving the final visualisation as a high-resolution image for my dissertation.
"""

plt.figure(figsize=(12,5))
plt.plot(y_test.index, y_test, label='Actual', alpha=0.8)
plt.plot(y_test.index, y_test_pred_sarima, label='SARIMA Predicted', alpha=0.8)
plt.legend()
plt.title('SARIMA: Actual vs Predicted Total kWh (Test Set)')
plt.xlabel('Datetime')
plt.ylabel('Total kWh')
plt.tight_layout()
plt.savefig("SARIMA_visualisation.png", dpi=1200, bbox_inches='tight')
plt.show()

""""The SARIMA model predicts a smoothed, periodic pattern that fails to capture the pronounced peaks and high variability in actual EV charging demand. This underfitting is typical for linear time series models when applied to highly non-stationary and event-driven datasets. The model’s inability to adapt to rapid changes or spikes highlights the need for advanced machine learning approaches that can leverage engineered features and capture more complex, non-linear relationships."

## **Implementing a Support Vector Regression (SVR) Model**

I am now training a SVR model, which is a powerful algorithm capable of capturing non-linear relationships in the data. I have configured the model with a radial basis function (RBF) kernel and specific hyperparameters that I have tuned for this problem. After fitting the model on the training data, I use it to generate predictions for the validation and test sets. I then evaluate its performance using my standard set of metrics and create a final visualisation that plots the model's predictions against the actual test data, saving the chart for my dissertation.
"""

from sklearn.svm import SVR
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# Instantiate SVR with your chosen parameters
svr = SVR(kernel='rbf', C=10, gamma= 0.01, epsilon=0.1)

# Fit on training data
svr.fit(X_train, y_train)

# Predict on validation and test sets
y_val_pred_svr = svr.predict(X_val)
y_test_pred_svr = svr.predict(X_test)

# Evaluate performance
print_metrics(y_val, y_val_pred_svr, 'SVR Validation (C=10, gamma=1)')
print_metrics(y_test, y_test_pred_svr, 'SVR Test (C=10, gamma=1)')

# Plot Actual vs Predicted
import matplotlib.pyplot as plt
plt.figure(figsize=(12,5))
plt.plot(y_test.index, y_test, label='Actual', alpha=0.8)
plt.plot(y_test.index, y_test_pred_svr, label='SVR Predicted (C=10, gamma=0.01)', alpha=0.8)
plt.legend()
plt.title('SVR (C=10, gamma=0.01): Actual vs Predicted Total kWh (Test Set)')
plt.xlabel('Datetime')
plt.ylabel('Total kWh')
plt.tight_layout()
plt.savefig("SVR_visualisation.png", dpi=1200, bbox_inches='tight')
plt.show()

""""With proper scaling and kernel parameter selection, SVR was able to capture the periodic trends and moderate fluctuations in hourly EV charging demand. However, it still underpredicts the most extreme demand peaks, a limitation typical of SVR when forecasting highly volatile, event-driven time series. These results support the need for more flexible models, such as tree-based ensembles or deep neural networks, to better capture rare but impactful spikes in demand."

**Install Updated XGBoost**
"""

!pip install --upgrade xgboost

import xgboost as xgb
print(xgb.__version__)

"""## **Optimising the XGBoost Model**

To find the best possible configuration for my XGBoost model, I am performing hyperparameter tuning using a randomised search. I have defined a range of potential settings for key parameters like the number of trees, their maximum depth, and the learning rate. The RandomizedSearchCV function will then automatically test 20 different combinations of these settings, using 3-fold cross-validation to evaluate each one robustly. This process automates the search for the most effective model configuration, and at the end, it will report the set of parameters that yielded the best performance on my training data.
"""

from sklearn.model_selection import RandomizedSearchCV
import xgboost as xgb

params = {
    'n_estimators': [100, 250, 500],
    'max_depth': [3, 5, 7, 10],
    'learning_rate': [0.01, 0.05, 0.1],
    'subsample': [0.7, 0.8, 1.0],
    'colsample_bytree': [0.7, 0.8, 1.0],
    'min_child_weight': [1, 3, 5],
    'gamma': [0, 0.1, 1]
}

xgb_model = xgb.XGBRegressor(objective='reg:squarederror', random_state=42)
random_search = RandomizedSearchCV(
    xgb_model, params,
    scoring='neg_mean_squared_error',
    cv=3,
    n_iter=20,
    verbose=1,
    n_jobs=-1
)

random_search.fit(X_train, y_train)
print("Best parameters:", random_search.best_params_)

print("Best parameters:", random_search.best_params_)

"""## **Evaluating the Optimised Model**

Now that the hyperparameter search has identified the best configuration for the XGBoost model, I am using that optimal version to make final predictions. I first refit this best estimator on the full training set. Then, I generate forecasts for both the validation and test datasets. Finally, I calculate my standard performance metrics—MAE, RMSE, and R-squared—to get a definitive measure of how accurately the fully optimised model performs on unseen data.
"""

# Refit with Best Parameters and Predict

best_xgb = random_search.best_estimator_
y_val_pred_xgb = best_xgb.predict(X_val)
y_test_pred_xgb = best_xgb.predict(X_test)

def print_metrics(y_true, y_pred, set_name='Set'):
    from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score
    mae = mean_absolute_error(y_true, y_pred)
    rmse = mean_squared_error(y_true, y_pred) ** 0.5
    r2 = r2_score(y_true, y_pred)
    print(f"{set_name} MAE: {mae:.2f}")
    print(f"{set_name} RMSE: {rmse:.2f}")
    print(f"{set_name} R^2: {r2:.3f}")

print_metrics(y_val, y_val_pred_xgb, 'XGBoost Validation')
print_metrics(y_test, y_test_pred_xgb, 'XGBoost Test')

"""## **Visualising the Optimised Model's Performance**

To provide a final, intuitive assessment of the optimised XGBoost model, I am plotting its predictions against the actual values from the test set. This graph directly compares the forecast energy demand with the real-world data, offering a clear visual representation of the model's accuracy. I have customised the plot with a title and labels for clarity before saving the final visualisation as a high-resolution image for my dissertation.
"""

# Plot Actual vs Predicted
import matplotlib.pyplot as plt

plt.figure(figsize=(12,5))
plt.plot(y_test.index, y_test, label='Actual', alpha=0.8)
plt.plot(y_test.index, y_test_pred_xgb, label='XGBoost Predicted', alpha=0.8)
plt.legend()
plt.title('XGBoost (Best Parameters): Actual vs Predicted Total kWh (Test Set)')
plt.xlabel('Datetime')
plt.ylabel('Total kWh')
plt.tight_layout()
plt.savefig("XGBoost_visualisation.png", dpi=1200, bbox_inches='tight')
plt.show()

"""## **Training and Optimising a Random Forest Model**

I am now training a Random Forest model, which is another powerful ensemble learning method. To ensure the model is as accurate as possible, I first define a grid of potential hyperparameter settings. I then use a randomised search with cross-validation to automatically test 20 different combinations of these settings, which efficiently identifies the best-performing configuration. Once the optimal parameters are found, I use that best model to make predictions on my validation and test sets and then evaluate its performance using my standard set of metrics.
"""

from sklearn.ensemble import RandomForestRegressor
from sklearn.model_selection import RandomizedSearchCV
from sklearn.metrics import mean_absolute_error, mean_squared_error, r2_score

# --- 1. Define Hyperparameter Grid for Random Forest ---
rf_params = {
    'n_estimators': [100, 250, 500],
    'max_depth': [5, 10, 15, None],
    'min_samples_split': [2, 5, 10],
    'min_samples_leaf': [1, 2, 4],
    'max_features': [1.0, 'sqrt', 'log2']
}

# --- 2. Initialize the Model and Randomized Search ---
# Note: RandomForestRegressor is imported from sklearn.ensemble
rf_model = RandomForestRegressor(random_state=42)

random_search_rf = RandomizedSearchCV(
    rf_model, rf_params,
    scoring='neg_mean_squared_error',
    cv=3,
    n_iter=20, # Same number of iterations for a fair comparison
    verbose=1,
    n_jobs=-1
)

# --- 3. Fit, Predict, and Evaluate ---
random_search_rf.fit(X_train, y_train)
print("Best Random Forest parameters:", random_search_rf.best_params_)

best_rf = random_search_rf.best_estimator_
y_val_pred_rf = best_rf.predict(X_val)
y_test_pred_rf = best_rf.predict(X_test)

def print_metrics(y_true, y_pred, set_name='Set'):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = mean_squared_error(y_true, y_pred) ** 0.5
    r2 = r2_score(y_true, y_pred)
    print(f"{set_name} MAE: {mae:.2f}")
    print(f"{set_name} RMSE: {rmse:.2f}")
    print(f"{set_name} R^2: {r2:.3f}")

print("\n--- Random Forest Results ---")
print_metrics(y_val, y_val_pred_rf, 'Random Forest Validation')
print_metrics(y_test, y_test_pred_rf, 'Random Forest Test')

!pip install lightgbm

!pip install optuna

"""## **Optimising the LightGBM Model with Optuna**

I am now using a more advanced optimisation framework called Optuna to find the best possible hyperparameters for a LightGBM model. I first define an 'objective' function that Optuna will try to minimise; this function trains a LightGBM model with a specific set of parameters and evaluates its performance on the validation set. Optuna then intelligently runs 20 trials, each time selecting a new combination of parameters to try and achieve the lowest possible error. Once this automated search is complete, I retrain a final model using the best-performing parameters on the combined training and validation data. Finally, I evaluate this fully optimised model on the test set to get a definitive measure of its performance.
"""

import lightgbm as lgb
import optuna
import numpy as np
import pandas as pd
from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score

# Silence Optuna's trial-by-trial logging ---
optuna.logging.set_verbosity(optuna.logging.WARNING)

# Define the Objective Function for Optuna with Early Stopping ---
def objective_lgbm(trial):
    """Defines the LightGBM training process for a single Optuna trial."""

    params = {
        'objective': 'regression_l1',
        'metric': 'rmse',
        'n_estimators': trial.suggest_int('n_estimators', 400, 2000),
        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.2, log=True),
        'max_depth': trial.suggest_int('max_depth', 3, 12),
        'num_leaves': trial.suggest_int('num_leaves', 20, 50),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'random_state': 42,
        'n_jobs': -1
    }

    # --- MODIFIED: Added verbose=-1 to silence LightGBM's internal messages ---
    model = lgb.LGBMRegressor(**params, verbose=-1)

    model.fit(
        X_train, y_train,
        eval_set=[(X_val, y_val)],
        eval_metric='rmse',
        callbacks=[lgb.early_stopping(15, verbose=False)]
    )

    preds = model.predict(X_val)
    rmse = np.sqrt(mean_squared_error(y_val, preds))

    return rmse

# --- 2. Create and Run the Optuna Study ---
print("--- Starting LightGBM Hyperparameter Optimization with Optuna ---")
study_lgbm = optuna.create_study(direction='minimize')
study_lgbm.optimize(objective_lgbm, n_trials=20)

# --- 3. Display the Best Results ---
print("\n--- Optimization Complete ---")
print("Best LightGBM trial:")
best_trial_lgbm = study_lgbm.best_trial
print(f"  Value (Validation RMSE): {best_trial_lgbm.value:.4f}")
print("  Params: ")
for key, value in best_trial_lgbm.params.items():
    print(f"    {key}: {value}")

# --- 4. Train Final Model with Best Parameters ---
print("\n--- Training Final Model with Optimal Parameters ---")
final_lgbm_model = lgb.LGBMRegressor(**best_trial_lgbm.params, random_state=42, n_jobs=-1, verbose=-1)

X_train_val = pd.concat([X_train, X_val])
y_train_val = pd.concat([y_train, y_val])
final_lgbm_model.fit(X_train_val, y_train_val)

# --- 5. Evaluate on Validation and Test Sets ---
y_val_pred_lgbm = final_lgbm_model.predict(X_val)
y_test_pred_lgbm = final_lgbm_model.predict(X_test)

val_mae = mean_absolute_error(y_val, y_val_pred_lgbm)
val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred_lgbm))
val_r2 = r2_score(y_val, y_val_pred_lgbm)

test_mae = mean_absolute_error(y_test, y_test_pred_lgbm)
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred_lgbm))
test_r2 = r2_score(y_test, y_test_pred_lgbm)

print("\n--- Final Optimized LightGBM Performance on Validation Set ---")
print(f"MAE: {val_mae:.4f}")
print(f"RMSE: {val_rmse:.4f}")
print(f"R^2:  {val_r2:.4f}")

print("\n--- Final Optimized LightGBM Performance on Test Set ---")
print(f"MAE: {test_mae:.4f}")
print(f"RMSE: {test_rmse:.4f}")
print(f"R^2:  {test_r2:.4f}")

"""# CNN-LSTM for Time Series Forecasting

## **Preparing Data for Sequential Models**

I am now restructuring the data into overlapping sequences, a crucial step for preparing it for time-series forecasting with neural networks. I have created a function that transforms the flat data into a 3D format by creating a sliding window. Each sequence contains 24 hours of historical data, which will be used to predict the single hour that follows it. Finally, I confirm the new 3D shape of my training, validation, and test sets to ensure they are correctly formatted for the model.
"""

import numpy as np

# Define the Sequencing Function for NumPy arrays ---
# This function converts your 2D data (samples, features) into
# 3D sequences (samples, window_size, features).

def create_sequences_np(X_data, y_data, window_size=24):
    """
    Creates 3D sequences from 2D data.
    Works with NumPy arrays.
    """
    Xs, ys = [], []
    for i in range(len(X_data) - window_size):
        # Standard NumPy slicing is used here
        v = X_data[i:(i + window_size)]
        Xs.append(v)
        ys.append(y_data[i + window_size])
    return np.array(Xs), np.array(ys)


# --- Step 2: Create the Sequenced Data ---
# Use the final imputed and scaled NumPy arrays from the previous steps.
# (X_train_scaled, y_train_scaled, etc.)

window_size = 24 # e.g., use 24 hours of data to predict the next hour

print("Creating sequences from scaled data...")

# Note: We use .flatten() on the y arrays to make them 1D, which is what the function expects
X_train_seq, y_train_seq = create_sequences_np(X_train_scaled, y_train_scaled.flatten(), window_size)
X_val_seq, y_val_seq = create_sequences_np(X_val_scaled, y_val_scaled.flatten(), window_size)
X_test_seq, y_test_seq = create_sequences_np(X_test_scaled, y_test_scaled.flatten(), window_size)


# --- Step 3: Verify the Shapes of the New Variables ---
# The X shapes should be 3D and the y shapes should be 1D.
print(f"Shape of X_train_seq: {X_train_seq.shape}")
print(f"Shape of y_train_seq: {y_train_seq.shape}")
print(f"Shape of X_val_seq: {X_val_seq.shape}")
print(f"Shape of y_val_seq: {y_val_seq.shape}")

print("\nData has been successfully converted into sequences.")

!pip install tensorflow

"""## **Building and Training a CNN-LSTM Network**

I am now constructing a hybrid neural network designed specifically for time-series forecasting. The architecture begins with a convolutional layer (Conv1D) to efficiently extract key features from the input sequences, followed by an LSTM layer which is adept at learning long-range dependencies and temporal patterns. To ensure the model trains effectively and avoids memorising the training data, a phenomenon known as overfitting, I have incorporated regularisation techniques such as BatchNormalization and Dropout.

After defining the architecture, I compile the model, specifying the 'Adam' optimiser to guide its learning process and 'Mean Squared Error' as the loss function to measure its accuracy. The model is then trained on the prepared sequential data. I have also implemented an 'Early Stopping' mechanism, which automatically halts the training if performance on the validation set ceases to improve, ensuring an efficient process and preventing overfitting. Finally, I plot the training and validation loss to visually inspect the model's learning progress and confirm that it is generalising well to new data.
"""

import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.callbacks import EarlyStopping
import matplotlib.pyplot as plt

# Get the input shape from your sequenced training data
# This assumes you have already run the create_sequences function on your
# final imputed and scaled data.
window_size = X_train_seq.shape[1]
n_features = X_train_seq.shape[2]


# Define a Simpler, More Robust CNN-LSTM Architecture ---
print("Building a simplified CNN-LSTM model to combat overfitting...")

model_v2 = Sequential([
    # A single Conv1D layer is often enough to extract features from sequences.
    Conv1D(filters=64, kernel_size=3, activation='relu', input_shape=(window_size, n_features)),
    BatchNormalization(),
    Dropout(0.3),

    # A single LSTM layer. Note the 'tanh' activation, which is standard.
    # return_sequences=False because it's the last recurrent layer before the Dense layers.
    LSTM(50, activation='tanh'),
    Dropout(0.4), # A slightly higher dropout for better regularization

    # A smaller Dense layer before the final output.
    Dense(25, activation='relu'),

    # Final output neuron for regression.
    Dense(1)
])


# Compile the Model ---
# We'll use a slightly lower learning rate for more stable training.
optimizer = tf.keras.optimizers.Adam(learning_rate=0.0005)
model_v2.compile(optimizer=optimizer, loss='mse') # Mean Squared Error for regression
model_v2.summary()


# Set Up Early Stopping ---
# This prevents the model from training for too long once it stops improving.
early_stop = EarlyStopping(monitor='val_loss', patience=10, restore_best_weights=True)


# Train the Model ---
history_v2 = model_v2.fit(
    X_train_seq, y_train_seq,
    epochs=100, # Early stopping will likely finish before 100 epochs
    batch_size=32,
    validation_data=(X_val_seq, y_val_seq),
    callbacks=[early_stop],
    verbose=1
)


# Plot Training and Validation Loss ---
# This plot is the most important indicator of whether we've reduced overfitting.
plt.figure(figsize=(10, 6))
plt.plot(history_v2.history['loss'], label='Training Loss')
plt.plot(history_v2.history['val_loss'], label='Validation Loss')
plt.title('Revised CNN-LSTM Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss (MSE)')
plt.legend()
plt.savefig("CNN_LSTM_Loss_visualisation.png", dpi=1200, bbox_inches='tight')
plt.show()

"""## **Evaluating the Neural Network's Performance**

Now that my neural network is trained, I am evaluating its performance on the unseen data. First, I use the trained model to generate predictions on the validation and test sets. Since these predictions are in a normalised format (between 0 and 1), I then reverse the scaling process to convert both the predicted and the actual values back to their original units of kWh. This allows for a meaningful, real-world comparison. Finally, I calculate the Mean Absolute Error, Root Mean Squared Error, and R-squared score to quantitatively measure the model's accuracy on both the validation and test datasets.
"""

# Predict on the validation and test sequences ---
print("Making predictions on validation and test sets...")
y_val_pred_scaled = model_v2.predict(X_val_seq)
y_test_pred_scaled = model_v2.predict(X_test_seq)


# Inverse scale ALL values to their original units (e.g., kWh) ---
print("Inverse-scaling predictions and true values...")

# Inverse scale the model's predictions
y_val_pred = scaler_y.inverse_transform(y_val_pred_scaled)
y_test_pred_original_lstm = scaler_y.inverse_transform(y_test_pred_scaled)

# Inverse scale the true values for a fair comparison
# This is the step that creates the missing 'y_val_true' variable
y_val_true = scaler_y.inverse_transform(y_val_seq.reshape(-1, 1))
y_test_true = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1))


# Define metrics function and evaluate performance ---
def print_metrics(y_true, y_pred, set_name='Set'):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)

    print(f"\n--- {set_name} Metrics ---")
    print(f"MAE:  {mae:.2f}")
    print(f"RMSE: {rmse:.2f}")
    print(f"R^2:  {r2:.3f}")

# Print metrics for both validation and test sets
print_metrics(y_val_true, y_val_pred, 'CNN-LSTM Validation')
print_metrics(y_test_true, y_test_pred_original_lstm, 'CNN-LSTM Test')

"""## **Visualising the Neural Network's Performance**

To provide a final, intuitive assessment of my neural network, I am plotting its predictions against the actual values from the test set. This graph directly compares the forecast energy demand with the real-world data, offering a clear visual representation of the model's accuracy on unseen data. I have customised the plot with a title and labels for clarity before saving the final visualisation as a high-resolution image for my dissertation.
"""

import matplotlib.pyplot as plt

# Plot Actual vs. Predicted for the Test Set ---
plt.figure(figsize=(12, 5))
plt.plot(y_test_true, label='Actual Total kWh', alpha=0.8)
plt.plot(y_test_pred, label='CNN-LSTM Predicted kWh', alpha=0.8)

plt.title('CNN-LSTM: Actual vs. Predicted Total kWh (Test Set)')
plt.xlabel('DateTime')
plt.ylabel('Total kWh')
plt.legend()
plt.tight_layout()
plt.savefig("CNN-LSTM_visualisation.png", dpi=1200, bbox_inches='tight')
plt.show()

"""## **Hyperparameter Tuning for CNN-LSTM**"""

!pip install keras-tuner

"""## **Optimising the Neural Network with Keras Tuner**

To ensure my neural network performs as accurately as possible, I am now using the Keras Tuner library to automatically find the best hyperparameter configuration. I first define a function that builds the model but leaves key parameters—such as the number of filters, the size of the LSTM and dense layers, the dropout rates, and the learning rate—as variables for the tuner to adjust.

I then configure a RandomSearch tuner to test 10 different combinations of these parameters, training each one and evaluating its performance on the validation set. This automated process systematically searches for the most effective model architecture. Once the search is complete, I retrieve the best-performing model and print a summary of the optimal hyperparameter values it discovered. This data-driven approach ensures that my final model is not just based on an educated guess, but has been systematically optimised for this specific forecasting task.
"""

import tensorflow as tf
import keras_tuner as kt
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Conv1D, LSTM, Dense, Dropout, BatchNormalization
from tensorflow.keras.optimizers import Adam


# Ensure prerequisite variables exist
# This code assumes the following variables are already defined and populated from previous steps:
# X_train_seq, y_train_seq, X_val_seq, y_val_seq

# Define the Model Building Function ---
# This function creates a Keras model but uses the 'hp' object to define
# hyperparameters that we want to tune. Keras Tuner will call this function
# repeatedly with different hyperparameter values.

def build_model(hp):
    """Builds a CNN-LSTM model for hyperparameter tuning."""
    model = Sequential()

    # Get the input shape from the training data
    window_size = X_train_seq.shape[1]
    n_features = X_train_seq.shape[2]

    # Tune the Conv1D Layer ---
    hp_filters = hp.Int('filters', min_value=32, max_value=128, step=32)
    model.add(Conv1D(
        filters=hp_filters,
        kernel_size=3,
        activation='relu',
        input_shape=(window_size, n_features)
    ))
    model.add(BatchNormalization())

    # Tune the Dropout Rate after Conv1D ---
    hp_conv_dropout = hp.Float('conv_dropout', min_value=0.1, max_value=0.5, step=0.1)
    model.add(Dropout(hp_conv_dropout))

    # Tune the LSTM Layer ---
    hp_lstm_units = hp.Int('lstm_units', min_value=30, max_value=100, step=20)
    model.add(LSTM(
        units=hp_lstm_units,
        activation='tanh' # Using tanh is standard for LSTMs
    ))

    # Tune the Dropout Rate after LSTM ---
    hp_lstm_dropout = hp.Float('lstm_dropout', min_value=0.2, max_value=0.6, step=0.1)
    model.add(Dropout(hp_lstm_dropout))

    # Tune the Dense Layer ---
    hp_dense_units = hp.Int('dense_units', min_value=20, max_value=80, step=20)
    model.add(Dense(units=hp_dense_units, activation='relu'))

    # Output layer
    model.add(Dense(1))

    # Tune the Learning Rate for the Optimizer ---
    hp_learning_rate = hp.Choice('learning_rate', values=[1e-3, 5e-4, 1e-4])

    model.compile(
        optimizer=Adam(learning_rate=hp_learning_rate),
        loss='mean_squared_error'
    )

    return model

# Instantiate the Tuner ---
# We will use RandomSearch, which randomly tries different combinations.
# 'objective' tells the tuner what to minimize (validation loss).
# 'max_trials' is the number of different models to test.
tuner = kt.RandomSearch(
    build_model,
    objective='val_loss',
    max_trials=10,  # Number of model configurations to test
    executions_per_trial=1, # Number of times to train each model
    directory='keras_tuner_dir',
    project_name='ev_demand_forecasting'
)

# You can clear previous results with the following line (optional)
# tuner.reload()

# Run the Hyperparameter Search ---
# This is the main training step. It will take some time as it trains multiple models.
print("Starting hyperparameter search...")
tuner.search(
    X_train_seq, y_train_seq,
    epochs=20, # Use a smaller number of epochs for the search
    validation_data=(X_val_seq, y_val_seq)
)
print("Hyperparameter search complete.")


# Get the Best Model ---
# Retrieve the best model found by the tuner.
best_model = tuner.get_best_models(num_models=1)[0]

# Display the Results ---
print("\n--- Best Model Found ---")
best_model.summary()

# You can also get the best hyperparameters found
best_hyperparameters = tuner.get_best_hyperparameters(num_trials=1)[0]
print("\n--- Best Hyperparameters ---")
print(f"Filters: {best_hyperparameters.get('filters')}")
print(f"Conv Dropout: {best_hyperparameters.get('conv_dropout'):.2f}")
print(f"LSTM Units: {best_hyperparameters.get('lstm_units')}")
print(f"LSTM Dropout: {best_hyperparameters.get('lstm_dropout'):.2f}")
print(f"Dense Units: {best_hyperparameters.get('dense_units')}")
print(f"Learning Rate: {best_hyperparameters.get('learning_rate')}")

"""## **Final Training and Evaluation of the Optimised Neural Network**

Now that the tuner has identified the best architecture, I am taking that optimal model and training it more thoroughly. I train it on my data for a higher number of epochs, using an early stopping mechanism to ensure it stops once its performance on the validation set no longer improves.

After this final training is complete, I use the model to make predictions on the test set. I then convert these predictions from their scaled format back into their original kWh units, which allows for a direct, real-world comparison. Finally, I calculate the definitive performance metrics for this fully optimised model and generate a plot to visually assess its accuracy on the unseen test data, saving the chart for my dissertation.
"""

# Retrieve the Best Model Architecture ---
# Get the single best model configuration found by the tuner.
best_model = tuner.get_best_models(num_models=1)[0]
print("--- Best Model Summary ---")
best_model.summary()


# Train the Best Model Properly ---
# We train the best model for more epochs to allow it to fully converge.
# EarlyStopping will monitor the validation loss and stop when it no longer improves.
print("\nTraining the best model for more epochs...")
early_stop = EarlyStopping(monitor='val_loss', patience=15, restore_best_weights=True, verbose=1)

history_best = best_model.fit(
    X_train_seq, y_train_seq,
    epochs=150,  # A higher number of epochs
    batch_size=32,
    validation_data=(X_val_seq, y_val_seq),
    callbacks=[early_stop]
)


# Make Predictions ---
print("\nMaking predictions on validation and test sets...")
y_val_pred_scaled = best_model.predict(X_val_seq)
y_test_pred_scaled = best_model.predict(X_test_seq)


# Inverse-Scale Predictions and True Values ---
# Convert the scaled data back to its original units (e.g., kWh) for interpretation.
print("Inverse-scaling the data to original units...")
y_val_pred = scaler_y.inverse_transform(y_val_pred_scaled)
y_test_pred = scaler_y.inverse_transform(y_test_pred_scaled)

y_val_true = scaler_y.inverse_transform(y_val_seq.reshape(-1, 1))
y_test_true = scaler_y.inverse_transform(y_test_seq.reshape(-1, 1))


# Evaluate Performance Metrics ---
def print_metrics(y_true, y_pred, set_name='Set'):
    mae = mean_absolute_error(y_true, y_pred)
    rmse = np.sqrt(mean_squared_error(y_true, y_pred))
    r2 = r2_score(y_true, y_pred)

    print(f"\n--- {set_name} Metrics ---")
    print(f"MAE:  {mae:.2f}")
    print(f"RMSE: {rmse:.2f}")
    print(f"R^2:  {r2:.3f}")

# Print metrics for both validation and test sets
print_metrics(y_val_true, y_val_pred, 'Tuned CNN-LSTM Validation')
print_metrics(y_test_true, y_test_pred, 'Tuned CNN-LSTM Test')


# Plot Actual vs. Predicted for the Test Set ---
plt.figure(figsize=(12, 5))
plt.plot(y_test_true, label='Actual Total kWh', alpha=0.8)
plt.plot(y_test_pred, label='Tuned CNN-LSTM Predicted kWh', alpha=0.8)

plt.title('Tuned CNN-LSTM: Actual vs. Predicted Total kWh (Test Set)')
plt.xlabel('DateTime')
plt.ylabel('Total kWh')
plt.legend()
plt.tight_layout()
plt.savefig("CNN-LSTM Tuned.png", dpi=1200, bbox_inches='tight')
plt.show()


# Plot Training & Validation Loss
# This plot shows how the model learned over the epochs.
plt.figure(figsize=(12, 6))
plt.plot(history_best.history['loss'], label='Training Loss')
plt.plot(history_best.history['val_loss'], label='Validation Loss')
plt.title('Tuned Model Training and Validation Loss')
plt.xlabel('Epoch')
plt.ylabel('Loss (MSE)')
plt.legend()
plt.grid(True, which='both', linestyle='--', linewidth=0.5)
plt.savefig("CNN-LSTM Tuned Loss.png", dpi=1200, bbox_inches='tight')
plt.show()

"""## **Final Model Comparison**
To conclude my analysis, I am now systematically comparing the performance of every model I have trained. I am calculating the final MAE, RMSE, and R-squared metrics for each model on the test set and compiling them into a single summary table. To provide a clear visual summary, I am also generating a bar chart that directly compares the Root Mean Squared Error across all models. This final step allows me to definitively identify the most accurate forecasting model for my dissertation based on quantitative evidence.
"""

# Final Model Comparison ---
try:
    # Calculate metrics for the Tuned CNN-LSTM model
    tuned_lstm_mae = mean_absolute_error(y_test_true, y_test_pred)
    tuned_lstm_rmse = np.sqrt(mean_squared_error(y_test_true, y_test_pred))
    tuned_lstm_r2 = r2_score(y_test_true, y_test_pred)

    # Calculate metrics for the Original CNN-LSTM model
    # This now uses 'y_test_pred_original_lstm', which should hold the unscaled predictions from the original model
    original_lstm_mae = mean_absolute_error(y_test_true, y_test_pred_original_lstm)
    original_lstm_rmse = np.sqrt(mean_squared_error(y_test_true, y_test_pred_original_lstm))
    original_lstm_r2 = r2_score(y_test_true, y_test_pred_original_lstm)

    # Calculate metrics for other models
    # Note: These models used the original y_test, which has a different length
    # than the y_test_true used for the sequenced LSTM model.
    sarima_mae = mean_absolute_error(y_test, y_test_pred_sarima)
    sarima_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred_sarima))
    sarima_r2 = r2_score(y_test, y_test_pred_sarima)

    svr_mae = mean_absolute_error(y_test, y_test_pred_svr)
    svr_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred_svr))
    svr_r2 = r2_score(y_test, y_test_pred_svr)

    xgb_mae = mean_absolute_error(y_test, y_test_pred_xgb)
    xgb_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred_xgb))
    xgb_r2 = r2_score(y_test, y_test_pred_xgb)

    # Create a DataFrame for comparison
    results_data = {
        'Model': ['SARIMA', 'SVR', 'XGBoost', 'Original CNN-LSTM', 'Tuned CNN-LSTM'],
        'MAE': [sarima_mae, svr_mae, xgb_mae, original_lstm_mae, tuned_lstm_mae],
        'RMSE': [sarima_rmse, svr_rmse, xgb_rmse, original_lstm_rmse, tuned_lstm_rmse],
        'R²': [sarima_r2, svr_r2, xgb_r2, original_lstm_r2, tuned_lstm_r2]
    }
    results_df = pd.DataFrame(results_data).sort_values(by='RMSE').reset_index(drop=True)

    print("\n--- Summary of All Model Performances ---")
    print(results_df.to_string())

    # Create a bar plot for visual comparison of RMSE
    plt.figure(figsize=(10, 6))
    plt.bar(results_df['Model'], results_df['RMSE'], color=['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd'])
    plt.title('Model Comparison: Root Mean Squared Error (RMSE)', fontsize=16)
    plt.ylabel('RMSE (lower is better)')
    plt.xticks(rotation=15)
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.savefig("RMSE Comparison_visualisation.png", dpi=1200, bbox_inches='tight')
    plt.savefig("Model_comparison_plot.png", dpi=1200, bbox_inches='tight')
    plt.show()

except NameError as e:
    print(f"\nCould not generate final comparison table: {e}")
    print("Please ensure that predictions from all models (e.g., 'y_test_pred_sarima', 'y_test_pred_original_lstm') are available in memory.")

"""## **Saving the Final Models**

As the final step in my process, I am saving all the machine learning models that I have trained and evaluated. I first create a dedicated directory to store these files neatly. Then, using appropriate methods for each type of model—joblib for scikit-learn, .save for SARIMA, and native functions for XGBoost and TensorFlow/Keras—I save each trained model to a distinct file. This ensures that the fully trained models are preserved and can be reloaded later for future use without needing to repeat the entire training and optimisation process.
"""

import joblib
import os

# Save All Trained Models ---
print("\n--- Saving All Trained Models with Proper Names ---")

# Create a directory to save the models if it doesn't exist
models_dir = '/content/keras_tuner_dir/ev_demand_forecasting'
if not os.path.exists(models_dir):
    os.makedirs(models_dir)

try:
    # Save scikit-learn and statsmodels models using joblib
    joblib.dump(svr, os.path.join(models_dir, 'ev_demand_svr_model.joblib'))
    print("SVR model saved successfully.")

    sarima_fit.save(os.path.join(models_dir, 'ev_demand_sarima_model.pkl'))
    print("SARIMA model saved successfully.")

    # Save XGBoost model
    best_xgb.save_model(os.path.join(models_dir, 'ev_demand_xgboost_model.json'))
    print("XGBoost model saved successfully.")

    # Save TensorFlow/Keras models
    model_v2.save(os.path.join(models_dir, 'ev_demand_original_cnnlstm_model.keras'))
    print("Original CNN-LSTM model saved successfully.")

    best_model.save(os.path.join(models_dir, 'ev_demand_tuned_cnnlstm_model.keras'))
    print("Tuned CNN-LSTM model saved successfully.")

except NameError as e:
    print(f"\nCould not save all models: {e}")
    print("Please ensure that all model objects (e.g., 'svr', 'best_xgb', 'sarima_fit', 'model_v2') are available in memory.")

"""## **Loading the Saved Models**

I am now creating a helper function to reload all the models I have previously saved. This function will read the model files from the specified directory and load them back into memory, storing them in a dictionary for easy access. This is a crucial step for reproducibility, as it allows me to use the trained models for future analysis or deployment without needing to run the entire training process again. I have also included error handling to manage any issues, for instance, if a model file is missing from the directory.
"""

# Helper Function to Load All Models ---
# This function loads all the saved models from the 'saved_models' directory.
def load_all_models(models_dir='/content/keras_tuner_dir/ev_demand_forecasting'):
    """Loads all saved models from the specified directory."""
    models = {}
    print("--- Loading Saved Models ---")
    try:
        # Load scikit-learn and statsmodels models
        models['SVR'] = joblib.load(os.path.join(models_dir, 'ev_demand_svr_model.joblib'))
        print("SVR model loaded.")

        # Load XGBoost model
        xgb_model = xgb.XGBRegressor()
        xgb_model.load_model(os.path.join(models_dir, 'ev_demand_xgboost_model.json'))
        models['XGBoost'] = xgb_model
        print("XGBoost model loaded.")

        # Load TensorFlow/Keras models
        models['Original CNN-LSTM'] = tf.keras.models.load_model(os.path.join(models_dir, 'ev_demand_original_cnnlstm_model.keras'))
        print("Original CNN-LSTM model loaded.")

        models['Tuned CNN-LSTM'] = tf.keras.models.load_model(os.path.join(models_dir, 'ev_demand_tuned_cnnlstm_model.keras'))
        print("Tuned CNN-LSTM model loaded.")

    except Exception as e:
        print(f"An error occurred while loading models: {e}")
        print("Please ensure all model files exist in the 'saved_models' directory.")
        return None

    return models

"""## **Creating a Master Visualisation Function**

To compare the performance of all my models on a specific day, I am creating a master plotting function. This function takes a single date as input, prepares the necessary data for that day, and then generates predictions from every loaded model—SVR, XGBoost, and both the original and tuned neural networks. It then plots all these predictions on a single graph against the actual energy demand, allowing for a direct, visual comparison of their forecasting accuracy for a 24-hour period. This provides an intuitive and powerful way to assess which model performs best on a given day.
"""

# Master Plotting Function ---
# This function handles the data preparation and plotting for a specific day.
def plot_day_comparison(target_date_str, models, data_dict, title_prefix=""):
    """
    Generates and plots predictions from all models for a single day.

    Args:
        target_date_str (str): The date to plot, in 'YYYY-MM-DD' format.
        models (dict): A dictionary of the loaded model objects.
        data_dict (dict): A dictionary containing necessary data arrays.
        title_prefix (str): A string to prepend to the plot title.
    """

    # Extract necessary data from the dictionary for easier access
    X_test = data_dict['X_test']
    y_test = data_dict['y_test']
    X_test_scaled = data_dict['X_test_scaled']
    X_test_seq = data_dict['X_test_seq']
    scaler_y = data_dict['scaler_y']
    window_size = X_test_seq.shape[1]

    # --- Data Preparation for the Target Day ---
    target_date = pd.to_datetime(target_date_str)
    day_mask = (X_test.index.date == target_date.date())

    if not day_mask.any():
        print(f"Error: No data found for {target_date_str} in the test set.")
        return

    # Get actual values for the day
    y_actual_day = y_test[day_mask]

    # Get feature data for non-sequential models
    X_test_day = X_test[day_mask]

    # Find the start index for sequential models
    start_idx = np.where(X_test.index == y_actual_day.index[0])[0][0]

    # Create the sequences required for the LSTM models for that day
    X_test_day_seq = []
    for i in range(len(y_actual_day)):
        seq_start_index = start_idx + i - window_size
        if seq_start_index < 0: continue

        sequence = X_test_scaled[seq_start_index : start_idx + i]
        X_test_day_seq.append(sequence)

    if not X_test_day_seq:
        print(f"Could not generate sequences for {target_date_str}. Not enough preceding data.")
        return

    X_test_day_seq = np.array(X_test_day_seq)

    # --- Generate Predictions ---
    predictions = {}
    for name, model in models.items():
        print(f"Generating predictions for {name}...")
        if "CNN-LSTM" in name:
            pred_scaled = model.predict(X_test_day_seq, verbose=0)
            predictions[name] = scaler_y.inverse_transform(pred_scaled).flatten()
        else:
            predictions[name] = model.predict(X_test_day)

    # --- Plotting ---
    plt.style.use('seaborn-v0_8-whitegrid')
    plt.figure(figsize=(14, 7))

    plt.plot(y_actual_day.values, 'o-', label='Actual Demand', color='black', linewidth=2.5, markersize=8)

    colors = {'SVR': 'green', 'XGBoost': 'blue', 'Original CNN-LSTM': 'purple', 'Tuned CNN-LSTM': 'red'}
    for name, pred_values in predictions.items():
        plt.plot(pred_values, 's--', label=f'{name} Prediction', color=colors.get(name, 'gray'), linewidth=1.5, markersize=5, alpha=0.9)

    plt.title(f'{title_prefix}: {target_date.strftime("%A, %Y-%m-%d")}', fontsize=18)
    plt.xlabel('Hour of the Day', fontsize=14)
    plt.ylabel('Total kWh', fontsize=14)
    plt.xticks(ticks=np.arange(0, 24, 2))
    plt.legend(fontsize=12)
    plt.ylim(bottom=0)
    plt.savefig("Saved prediction_visualisation.png", dpi=1200, bbox_inches='tight')
    plt.show()

"""## **Preparing for Final Visualisation**
I am now setting the stage for the final visual comparison of my models. I first call the function I just created to load all the saved models back into the programming environment. I then group all the necessary data arrays—such as the various test sets and the data scaler—into a single dictionary. This organises the data neatly, making it easy to pass all the required information to the master plotting function in the next step.
"""

# Load Models and Prepare Data
# This block must be run first. It assumes all the necessary data variables
# from your notebook (X_test, y_test, etc.) are available.
try:
    loaded_models = load_all_models()

    data_for_plotting = {
        'X_test': X_test,
        'y_test': y_test,
        'X_test_scaled': X_test_scaled,
        'X_test_seq': X_test_seq,
        'scaler_y': scaler_y
    }
except NameError as e:
    print(f"\nExecution failed: {e}")
    print("Please ensure you have run all previous cells in your notebook to define the necessary data and model variables.")
    loaded_models = None

"""## **Generating Final Comparison Plots**

I am now using the master plotting function to generate a series of visualisations that directly compare the performance of all my trained models. I am deliberately selecting three distinct and representative days from the test set: a typical working day, a weekend day, and a bank holiday. This allows me to assess how each model performs under different real-world demand patterns and provides a final, qualitative evaluation of their forecasting capabilities in a range of scenarios.
"""

# Generate All Plots ---
if loaded_models:
    # Plot for a Working Day
    plot_day_comparison(
        target_date_str='2019-07-16',
        models=loaded_models,
        data_dict=data_for_plotting,
        title_prefix="Working Day Prediction"
    )

    # Plot for a Weekend Day
    plot_day_comparison(
        target_date_str='2019-07-20',
        models=loaded_models,
        data_dict=data_for_plotting,
        title_prefix="Weekend Day Prediction"
    )

    # Plot for a Bank Holiday
    plot_day_comparison(
        target_date_str='2019-08-26',
        models=loaded_models,
        data_dict=data_for_plotting,
        title_prefix="Bank Holiday Prediction"
    )

# This function handles the data preparation and plotting for a specific 7-day period.
def plot_week_comparison(start_date_str, models, data_dict, title_prefix=""):
    """
    Generates and plots predictions from all models for a single week.

    Args:
        start_date_str (str): The start date of the week to plot, in 'YYYY-MM-DD' format.
        models (dict): A dictionary of the loaded model objects.
        data_dict (dict): A dictionary containing necessary data arrays.
        title_prefix (str): A string to prepend to the plot title.
    """

    # Extract necessary data from the dictionary
    X_test = data_dict['X_test']
    y_test = data_dict['y_test']
    X_test_scaled = data_dict['X_test_scaled']
    X_test_seq = data_dict['X_test_seq']
    scaler_y = data_dict['scaler_y']
    window_size = X_test_seq.shape[1]

    # --- Data Preparation for the Target Week ---
    start_date = pd.to_datetime(start_date_str)
    end_date = start_date + pd.Timedelta(days=6)
    week_mask = (X_test.index.date >= start_date.date()) & (X_test.index.date <= end_date.date())

    if not week_mask.any():
        print(f"Error: No data found for the week starting {start_date_str} in the test set.")
        return

    # Get actual values and feature data for the week
    y_actual_week = y_test[week_mask]
    X_test_week = X_test[week_mask]

    # Find the start index for sequential models
    try:
        start_idx = np.where(X_test.index == y_actual_week.index[0])[0][0]
    except IndexError:
        print(f"Error: Could not find the start of the week {start_date_str} in the test data index.")
        return

    # Create the sequences required for the LSTM models for the entire week
    X_test_week_seq = []
    for i in range(len(y_actual_week)):
        seq_start_index = start_idx + i - window_size
        if seq_start_index < 0: continue

        sequence = X_test_scaled[seq_start_index : start_idx + i]
        X_test_week_seq.append(sequence)

    if not X_test_week_seq:
        print(f"Could not generate sequences for the week starting {start_date_str}.")
        return

    X_test_week_seq = np.array(X_test_week_seq)

    # --- Generate Predictions ---
    predictions = {}
    for name, model in models.items():
        print(f"Generating predictions for {name}...")
        if "CNN-LSTM" in name:
            pred_scaled = model.predict(X_test_week_seq, verbose=0)
            predictions[name] = scaler_y.inverse_transform(pred_scaled).flatten()
        else:
            predictions[name] = model.predict(X_test_week)

    # --- Plotting ---
    plt.style.use('seaborn-v0_8-whitegrid')
    plt.figure(figsize=(18, 8))

    plt.plot(y_actual_week.values, 'o-', label='Actual Demand', color='black', linewidth=2, markersize=5)

    colors = {'SVR': 'green', 'XGBoost': 'blue', 'Original CNN-LSTM': 'purple', 'Tuned CNN-LSTM': 'red'}
    for name, pred_values in predictions.items():
        plt.plot(pred_values, 's--', label=f'{name} Prediction', color=colors.get(name, 'gray'), linewidth=1.5, markersize=4, alpha=0.8)

    plt.title(f'{title_prefix}: Week of {start_date.strftime("%Y-%m-%d")} to {end_date.strftime("%Y-%m-%d")}', fontsize=18)
    plt.xlabel('Hour of the Week (0-167)', fontsize=14)
    plt.ylabel('Total kWh', fontsize=14)
    plt.xticks(ticks=np.arange(0, 168, 24), labels=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])
    plt.legend(fontsize=12)
    plt.ylim(bottom=0)
    plt.grid(True, which='both', linestyle='--', linewidth=0.5)
    plt.savefig("Saved visuals_visualisation.png", dpi=1200, bbox_inches='tight')
    plt.show()

if loaded_models:
    # Plot for a sample week from the test data
    plot_week_comparison(
        start_date_str='2019-07-15', # A sample week in July
        models=loaded_models,
        data_dict=data_for_plotting,
        title_prefix="Weekly Prediction Comparison"
    )

if loaded_models:
    # Plot for a sample week from the test data
    plot_week_comparison(
        start_date_str='2019-04-19', # A sample week in July
        models=loaded_models,
        data_dict=data_for_plotting,
        title_prefix="Weekly Prediction Comparison"
    )

!pip install optuna

"""## **Optimising the XGBoost Model with Optuna**

To find the best possible configuration for my XGBoost model, I am using an advanced optimisation framework called Optuna. I first define an 'objective' function that trains an XGBoost model using a specific set of hyperparameters suggested by Optuna and then returns the model's error on the validation set.

Optuna then automatically runs 50 trials, intelligently choosing new hyperparameter combinations each time to try and minimise this error. Once the search is complete and the best parameters are identified, I train a final model on the combined training and validation data. I then evaluate this fully optimised model on the test set to get a definitive measure of its performance and save the final model to a file for future use.
"""

import optuna
# Define the Objective Function for Optuna ---
def objective(trial):
    params = {
        'objective': 'reg:squarederror', 'eval_metric': 'rmse',
        'n_estimators': trial.suggest_int('n_estimators', 400, 1500),
        'learning_rate': trial.suggest_float('learning_rate', 1e-3, 0.1, log=True),
        'max_depth': trial.suggest_int('max_depth', 3, 10),
        'subsample': trial.suggest_float('subsample', 0.6, 1.0),
        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),
        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),
        'gamma': trial.suggest_float('gamma', 1e-8, 1.0, log=True),
        'lambda': trial.suggest_float('lambda', 1e-8, 1.0, log=True),
        'alpha': trial.suggest_float('alpha', 1e-8, 1.0, log=True),
        'seed': 42
    }
    model = xgb.XGBRegressor(**params)
    model.fit(X_train, y_train, eval_set=[(X_val, y_val)], verbose=False)
    preds = model.predict(X_val)
    rmse = np.sqrt(mean_squared_error(y_val, preds))
    return rmse

# Create and Run the Optuna Study ---
print("--- Starting XGBoost Hyperparameter Optimization with Optuna ---")
study = optuna.create_study(direction='minimize')
study.optimize(objective, n_trials=50)

# Display the Best Results ---
print("\n--- Optimization Complete ---")
print(f"Number of finished trials: {len(study.trials)}")
print("Best trial:")
best_trial = study.best_trial
print(f"  Value (Validation RMSE): {best_trial.value:.4f}")
print("  Params: ")
for key, value in best_trial.params.items():
    print(f"    {key}: {value}")

# Train Final Model with Best Parameters and Evaluate ---
print("\n--- Training Final Model with Optimal Parameters ---")
best_params = best_trial.params
final_xgb_model = xgb.XGBRegressor(**best_params, seed=42)
X_train_val = pd.concat([X_train, X_val])
y_train_val = pd.concat([y_train, y_val])
final_xgb_model.fit(X_train_val, y_train_val, verbose=False)

# Evaluate on Validation and Test Sets ---
y_val_pred_optimized = final_xgb_model.predict(X_val)
y_test_pred_optimized = final_xgb_model.predict(X_test)

val_mae = mean_absolute_error(y_val, y_val_pred_optimized)
val_rmse = np.sqrt(mean_squared_error(y_val, y_val_pred_optimized))
val_r2 = r2_score(y_val, y_val_pred_optimized)

test_mae = mean_absolute_error(y_test, y_test_pred_optimized)
test_rmse = np.sqrt(mean_squared_error(y_test, y_test_pred_optimized))
test_r2 = r2_score(y_test, y_test_pred_optimized)

print("\n--- Final Optimized Model Performance on Validation Set ---")
print(f"MAE: {val_mae:.4f}")
print(f"RMSE: {val_rmse:.4f}")
print(f"R^2:  {val_r2:.4f}")

print("\n--- Final Optimized Model Performance on Test Set ---")
print(f"MAE: {test_mae:.4f}")
print(f"RMSE: {test_rmse:.4f}")
print(f"R^2:  {test_r2:.4f}")

# Save the Optimized Model ---
print("\n--- Saving the Optimized XGBoost Model ---")
models_dir = '/content/keras_tuner_dir/ev_demand_forecasting'
if not os.path.exists(models_dir):
    os.makedirs(models_dir)
final_xgb_model.save_model(os.path.join(models_dir, 'ev_demand_optimized_xgboost_model.json'))
print(f"Optimized model saved to '{os.path.join(models_dir, 'ev_demand_optimized_xgboost_model.json')}'")

"""## **Final Plot Customisation for Optimised XGBoost Model**

I am now applying the final formatting to the visualisation. I set the main title and the labels for the x and y axes to ensure the plot is clearly understood. A legend is also added to differentiate between the actual and predicted data. Finally, I save the completed chart as a high-resolution image for my dissertation before displaying it.
"""

# Plot Actual vs. Predicted on Test Set ---
print("\n--- Generating Actual vs. Predicted Plot ---")

# Create a pandas Series for the predictions with the same index as y_test
y_test_pred_series = pd.Series(y_test_pred_optimized, index=y_test.index)

plt.figure(figsize=(12, 5))

# Plot the actual values from the test set
plt.plot(y_test, label='Actual Demand KWh', alpha=0.8)

# Plot the predictions from the final XGBoost model
plt.plot(y_test_pred_series, label='Optimised XGBoost Prediction', alpha=0.8)

# --- Formatting ---
plt.title('Optimised XGBoost: Actual vs. Predicted Demand (Test Set)')
plt.xlabel('Date')
plt.ylabel('Total KWh')
plt.legend()
plt.tight_layout()
plt.grid(False)

# --- Save the Figure ---
plt.savefig("XGBoost_final_timeseries_visualisation_no_grid.png", dpi=1200)

plt.show()

"""## **Analysing Feature Importance**

I am creating a function to analyse which features are most influential in the optimised XGBoost model's predictions. The function first loads the saved model from its file. It then extracts the feature importance scores, using the 'gain' metric which measures each feature's relative contribution to the model's performance. These scores are then sorted and used to generate a horizontal bar chart, providing a clear, ranked visualisation of which data points the model relies on most. Finally, the plot is saved as a high-resolution image for my dissertation. This entire process is wrapped in a main execution block to ensure it runs cleanly as a script.
"""

import os

# Define the path to your saved, optimized XGBoost model
MODELS_DIR = "/content/keras_tuner_dir/ev_demand_forecasting"
MODEL_PATH = os.path.join(MODELS_DIR, 'ev_demand_optimized_xgboost_model.json')

# Define where to save the output plot
SAVE_DIR = "project_visualizations"
if not os.path.exists(SAVE_DIR):
    os.makedirs(SAVE_DIR)

# load model and extract importance

def plot_feature_importance(model_path, save_dir):
    """
    Loads a trained XGBoost model and plots its feature importance.
    """
    print(f"--- Loading model from: {model_path} ---")

    try:
        # Load the trained XGBoost model
        model = xgb.XGBRegressor()
        model.load_model(model_path)
        print(" Model loaded successfully.")

        # Get feature importance scores. 'gain' is a good metric as it represents
        # the relative contribution of the feature to the model.
        importance_scores = model.get_booster().get_score(importance_type='gain')

        if not importance_scores:
            print(" Error: Could not retrieve feature importance scores from the model.")
            return

        # Create a pandas DataFrame for easier sorting and plotting
        importance_df = pd.DataFrame({
            'Feature': list(importance_scores.keys()),
            'Importance': list(importance_scores.values())
        }).sort_values(by='Importance', ascending=True)


        # Visualisation

        print("\n--- Generating Feature Importance Plot ---")

        plt.style.use('seaborn-v0_8-whitegrid')
        plt.figure(figsize=(12, 8))

        # Create a horizontal bar plot
        plt.barh(importance_df['Feature'], importance_df['Importance'], color='skyblue')

        plt.xlabel("Feature Importance (Gain)", fontsize=12)
        plt.ylabel("Features", fontsize=12)
        plt.title("XGBoost Model Feature Importance", fontsize=16)
        plt.grid(axis='x', linestyle='--', alpha=0.7)
        plt.tight_layout()

        # Save the plot
        save_path = os.path.join(save_dir, "xgboost_feature_importance.png")
        plt.savefig(save_path, dpi=1200, bbox_inches='tight')
        plt.show()

        print(f"\n Feature importance plot saved to: {save_path}")

    except FileNotFoundError:
        print(f" CRITICAL ERROR: Model file not found at '{model_path}'.")
        print("Please ensure you have run the Optuna optimization pipeline to save the model.")
    except Exception as e:
        print(f"An unexpected error occurred: {e}")

# Main execution

if __name__ == "__main__":
    plot_feature_importance(MODEL_PATH, SAVE_DIR)

"""## **Final Visualisation Pipeline**

To produce the final visual evidence for my dissertation, I have created a comprehensive plotting pipeline.

First, I define a function to load my two best-performing models: the optimised XGBoost and the tuned CNN-LSTM. This keeps the main script clean and organised.

Next, I create two powerful and reusable plotting functions. The first, plot_day_comparison_optimized, is designed to generate a detailed 24-hour comparison for any given date, plotting the actual demand against the predictions from my best models. The second function, plot_week_comparison_optimized, extends this capability to visualise performance over an entire seven-day period.

Finally, in the main execution block, I use these functions to systematically generate all the key comparison charts for my analysis. I select specific dates representing a typical working day, a weekend, a bank holiday, and also special periods like the Easter and Christmas weeks. This allows me to create a robust visual comparison of how my final models perform across a variety of real-world demand scenarios.
"""

# Visualization and Comparison Pipeline ---
print("\n--- Generating Comparison Plots for Optimized Model ---")

def load_comparison_models(models_dir='/content/keras_tuner_dir/ev_demand_forecasting'):
    """Loads all models needed for final comparison plots."""
    models = {}
    print("--- Loading Models for Comparison ---")
    try:
        # Load the newly optimized XGBoost model
        xgb_opt_model = xgb.XGBRegressor()
        xgb_opt_model.load_model(os.path.join(models_dir, 'ev_demand_optimized_xgboost_model.json'))
        models['Optimized XGBoost'] = xgb_opt_model
        print("Optimized XGBoost model loaded.")

        # Load the tuned CNN-LSTM model
        models['Tuned CNN-LSTM'] = tf.keras.models.load_model(os.path.join(models_dir, 'ev_demand_tuned_cnnlstm_model.keras'))
        print("Tuned CNN-LSTM model loaded.")

    except Exception as e:
        print(f"An error occurred while loading models for plotting: {e}")
        return None
    return models

def plot_day_comparison_optimized(target_date_str, models, data_dict, title_prefix="", data_split='test'):
    """Generates and plots predictions from optimized models for a single day."""
    if data_split == 'validation':
        X_data, y_data, X_scaled = data_dict['X_val'], data_dict['y_val'], data_dict['X_val_scaled']
    else:
        X_data, y_data, X_scaled = data_dict['X_test'], data_dict['y_test'], data_dict['X_test_scaled']

    scaler_y, window_size = data_dict['scaler_y'], 24

    target_date = pd.to_datetime(target_date_str)
    day_mask = (X_data.index.date == target_date.date())

    if not day_mask.any():
        print(f"Error: No data for {target_date_str} in '{data_split}' set.")
        return

    y_actual_day, X_day = y_data[day_mask], X_data[day_mask]

    try:
        start_idx = np.where(X_data.index == y_actual_day.index[0])[0][0]
    except IndexError:
        print(f"Error: Could not find start of day {target_date_str} in index.")
        return

    X_day_seq = np.array([X_scaled[start_idx + i - window_size : start_idx + i] for i in range(len(y_actual_day))])

    predictions = {}
    for name, model in models.items():
        print(f"Generating predictions for {name}...")
        if "CNN-LSTM" in name:
            pred_scaled = model.predict(X_day_seq, verbose=0)
            predictions[name] = scaler_y.inverse_transform(pred_scaled).flatten()
        else:
            predictions[name] = model.predict(X_day)

    plt.style.use('seaborn-v0_8-whitegrid')
    plt.figure(figsize=(14, 7))
    plt.plot(y_actual_day.values, 'o-', label='Actual Demand', color='black', linewidth=2.5, markersize=8)

    colors = {'Optimized XGBoost': 'blue', 'Tuned CNN-LSTM': 'red'}
    for name, pred_values in predictions.items():
        plt.plot(pred_values, 's--', label=f'{name} Prediction', color=colors.get(name, 'gray'), linewidth=1.5, markersize=5, alpha=0.9)

    plt.title(f'{title_prefix}: {target_date.strftime("%A, %Y-%m-%d")}', fontsize=18)
    plt.xlabel('Hour of the Day', fontsize=14)
    plt.ylabel('Total kWh', fontsize=14)
    plt.xticks(ticks=np.arange(0, 24, 2))
    plt.legend(fontsize=12)
    plt.ylim(bottom=0)
    plt.grid(True, which='both', linestyle='--', linewidth=0.5)
    filename = f"{title_prefix.replace(' ', '_')}_{target_date.strftime('%Y%m%d')}.png"
    plt.savefig(filename, dpi=1200, bbox_inches='tight')
    print(f"Plot saved as: {filename}")
    plt.show()

def plot_week_comparison_optimized(start_date_str, models, data_dict, title_prefix="", data_split='test'):
    """Generates and plots predictions from optimized models for a single week."""
    if data_split == 'validation':
        X_data, y_data, X_scaled = data_dict['X_val'], data_dict['y_val'], data_dict['X_val_scaled']
    else:
        X_data, y_data, X_scaled = data_dict['X_test'], data_dict['y_test'], data_dict['X_test_scaled']

    scaler_y, window_size = data_dict['scaler_y'], 24

    start_date = pd.to_datetime(start_date_str)
    end_date = start_date + pd.Timedelta(days=6)
    week_mask = (X_data.index.date >= start_date.date()) & (X_data.index.date <= end_date.date())

    if not week_mask.any():
        print(f"Error: No data for week starting {start_date_str} in '{data_split}' set.")
        return

    y_actual_week, X_week = y_data[week_mask], X_data[week_mask]

    try:
        start_idx = np.where(X_data.index == y_actual_week.index[0])[0][0]
    except IndexError:
        print(f"Error: Could not find start of week {start_date_str} in index.")
        return

    X_week_seq = np.array([X_scaled[start_idx + i - window_size : start_idx + i] for i in range(len(y_actual_week))])

    predictions = {}
    for name, model in models.items():
        print(f"Generating predictions for {name}...")
        if "CNN-LSTM" in name:
            pred_scaled = model.predict(X_week_seq, verbose=0)
            predictions[name] = scaler_y.inverse_transform(pred_scaled).flatten()
        else:
            predictions[name] = model.predict(X_week)

    plt.style.use('seaborn-v0_8-whitegrid')
    plt.figure(figsize=(18, 8))
    plt.plot(y_actual_week.values, 'o-', label='Actual Demand', color='black', linewidth=2, markersize=5)

    colors = {'Optimized XGBoost': 'blue', 'Tuned CNN-LSTM': 'red'}
    for name, pred_values in predictions.items():
        plt.plot(pred_values, 's--', label=f'{name} Prediction', color=colors.get(name, 'gray'), linewidth=1.5, markersize=4, alpha=0.8)

    plt.title(f'{title_prefix}: Week of {start_date.strftime("%Y-%m-%d")} to {end_date.strftime("%Y-%m-%d")}', fontsize=18)
    plt.xlabel('Hour of the Week (0-167)', fontsize=14)
    plt.ylabel('Total kWh', fontsize=14)
    plt.xticks(ticks=np.arange(0, 168, 24), labels=['Mon', 'Tue', 'Wed', 'Thu', 'Fri', 'Sat', 'Sun'])
    plt.legend(fontsize=12)
    plt.ylim(bottom=0)
    plt.grid(True, which='both', linestyle='--', linewidth=0.5)
    # Save the plot
    filename = f"{title_prefix.replace(' ', '_')}_{start_date.strftime('%Y%m%d')}.png"
    plt.savefig(filename, dpi=1200, bbox_inches='tight')
    print(f"Plot saved as: {filename}")
    plt.show()

try:
    comparison_models = load_comparison_models()
    data_for_plotting = {
        'X_test': X_test, 'y_test': y_test, 'X_test_scaled': X_test_scaled,
        'X_val': X_val, 'y_val': y_val, 'X_val_scaled': X_val_scaled,
        'scaler_y': scaler_y
    }

    if comparison_models:
        # Generate Daily Comparison Plots ---
        print("\n--- Generating Daily Comparison Plots ---")
        plot_day_comparison_optimized(
            target_date_str='2019-07-16',
            models=comparison_models,
            data_dict=data_for_plotting,
            title_prefix="Optimized Model Comparison (Working Day)",
            data_split='test'
        )
        plot_day_comparison_optimized(
            target_date_str='2019-07-20',
            models=comparison_models,
            data_dict=data_for_plotting,
            title_prefix="Optimized Model Comparison (Weekend Day)",
            data_split='test'
        )
        plot_day_comparison_optimized(
            target_date_str='2019-08-26',
            models=comparison_models,
            data_dict=data_for_plotting,
            title_prefix="Optimized Model Comparison (Bank Holiday)",
            data_split='test'
        )

        # Generate Weekly Comparison Plots ---
        print("\n--- Generating Weekly Comparison Plots ---")
        plot_week_comparison_optimized(
            start_date_str='2019-07-15',
            models=comparison_models,
            data_dict=data_for_plotting,
            title_prefix="Optimized Model Comparison (Typical Week)",
            data_split='test'
        )
        plot_week_comparison_optimized(
            start_date_str='2019-04-19',
            models=comparison_models,
            data_dict=data_for_plotting,
            title_prefix="Optimized Model Comparison (Easter Wek)",
            data_split='test'
        )
        plot_week_comparison_optimized(
            start_date_str='2018-12-24',
            models=comparison_models,
            data_dict=data_for_plotting,
            title_prefix="Optimized Model Comparison (Christmas Week)",
            data_split='validation'
        )
except NameError as e:
    print(f"\nPlotting failed: {e}. Ensure all data variables (X_train, y_val, etc.) are defined.")

"""## **Quantifying Forecast Uncertainty**

To create a forecast that accounts for real-world uncertainty, I am now moving beyond single-point predictions. Instead of one model that predicts the single most likely outcome, I am training a set of XGBoost models using a technique called quantile regression.

I have defined a pipeline that first trains three separate models, each one optimised to predict a specific percentile of the energy demand: the 10th percentile (a low-demand scenario), the 50th percentile (the median or most likely outcome), and the 90th percentile (a high-demand scenario).

Once these models are trained, I use them to generate a 24-hour forecast for a specific day. The final visualisation plots the actual demand against the median forecast, and critically, it includes a shaded area between the 10th and 90th percentile predictions. This shaded region represents the prediction interval, providing a clear, quantitative measure of the forecast's uncertainty.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import xgboost as xgb
import os
from datetime import datetime, timedelta


# This assumes 'best_params', 'X_train', 'y_train', 'X_val', 'y_val', 'X_test',
# and 'y_test' are already defined in your notebook's memory.
MODELS_DIR = "/content/keras_tuner_dir/ev_demand_forecasting"
QUANTILES_TO_TRAIN = [0.10, 0.50, 0.90] # Lower bound, median, upper bound

# Model Training
def train_and_save_quantile_models(params, X_train, y_train, X_val, y_val, quantiles, models_dir):
    """Trains and saves XGBoost models for specified quantiles."""
    print("--- Training Quantile Regression Models ---")
    base_params = params.copy()

    if not os.path.exists(models_dir):
        os.makedirs(models_dir)

    for q in quantiles:
        print(f"Training model for quantile: {q:.2f}")
        base_params['objective'] = 'reg:quantileerror'
        base_params['quantile_alpha'] = q

        model = xgb.XGBRegressor(**base_params)

        X_train_full = pd.concat([X_train, X_val])
        y_train_full = pd.concat([y_train, y_val])

        model.fit(X_train_full, y_train_full, verbose=False)

        model_path = os.path.join(models_dir, f'ev_demand_xgb_quantile_{int(q*100)}.json')
        model.save_model(model_path)
        print(f"Model for quantile {q:.2f} saved to {model_path}")

# Forecast generation
def generate_forecasts(models_dir, data_source, start_date_str, end_date_str, quantiles):
    """
    Loads pre-trained models and generates forecasts for a specified date range.
    """
    print(f"\n🔮 Generating forecasts from {start_date_str} to {end_date_str}...")

    start_date = pd.to_datetime(start_date_str)
    end_date = pd.to_datetime(end_date_str)
    period_mask = (data_source.index.date >= start_date.date()) & (data_source.index.date <= end_date.date())

    if not period_mask.any():
        print(f"Error: No data found for the specified period in the data source.")
        return None

    X_period = data_source[period_mask]

    forecasts = {}
    for q in quantiles:
        try:
            model_path = os.path.join(models_dir, f'ev_demand_xgb_quantile_{int(q*100)}.json')
            model = xgb.XGBRegressor()
            model.load_model(model_path)
        except Exception as e:
            raise FileNotFoundError(f"Error loading model for quantile {q}: {e}")

        forecasted_demand = model.predict(X_period)
        forecasted_demand = np.maximum(forecasted_demand, 0)
        forecasts[f'q_{q}'] = forecasted_demand

    return pd.DataFrame(forecasts, index=X_period.index)

# Define visualisations
def visualize_forecast_vs_actual(forecast_df, actual_series, title):
    """
    Generates a plot comparing the forecast (with uncertainty) against actual demand.
    """
    print(f"--- Visualizing: {title} ---")

    actual_demand = actual_series.loc[forecast_df.index]

    plt.style.use('seaborn-v0_8-whitegrid')
    plt.figure(figsize=(15, 7))

    time_index = np.arange(len(forecast_df))

    # Plot the actual demand
    plt.plot(time_index, actual_demand.values, 'o-', color='black', label='Actual Demand', zorder=10, markersize=5)

    # Plot the uncertainty interval
    plt.fill_between(
        time_index,
        forecast_df['q_0.1'],
        forecast_df['q_0.9'],
        color='skyblue',
        alpha=0.5,
        label='Forecast Uncertainty (10th-90th Percentile)'
    )

    # Plot the median forecast
    plt.plot(time_index, forecast_df['q_0.5'], 'o-', color='red', label='Median Forecast (50th Percentile)', markersize=5)

    plt.title(title, fontsize=18)
    plt.xlabel('Time', fontsize=14)
    plt.ylabel('Power (kW)', fontsize=14)

    # Adjust x-axis ticks based on the period length
    if len(time_index) <= 24: # Daily plot
        plt.xticks(time_index)
        plt.xlabel('Hour of the Day', fontsize=14)
    elif len(time_index) <= 24 * 7: # Weekly plot
        num_days = int(np.ceil(len(time_index)/24))
        plt.xticks(np.arange(0, len(time_index), 24), [f'Day {i+1}' for i in range(num_days)])

    plt.legend(fontsize=12)
    plt.grid(axis='y', linestyle='--', alpha=0.7)
    plt.tight_layout()
    filename = f"{title.replace(' ', '_').replace('(', '').replace(')', '')}.png"
    plt.savefig(filename, dpi=1200, bbox_inches='tight')
    print(f"Plot saved as: {filename}")
    plt.show()

# Main execution
if __name__ == "__main__":
    try:
        # Step A: Train the quantile models (run this once)
        # Assumes 'best_params' from Optuna study is available
        train_and_save_quantile_models(best_params, X_train, y_train, X_val, y_val, QUANTILES_TO_TRAIN, MODELS_DIR)

        # --- Scenario 1: Typical Working Day ---
        working_day = '2019-07-16'
        forecast_working_day = generate_forecasts(MODELS_DIR, X_test, working_day, working_day, QUANTILES_TO_TRAIN)
        if forecast_working_day is not None:
            visualize_forecast_vs_actual(forecast_working_day, y_test, f"Forecast vs. Actual for Working Day ({working_day})")

        # --- Scenario 2: Typical Weekend Day ---
        weekend_day = '2019-07-20'
        forecast_weekend_day = generate_forecasts(MODELS_DIR, X_test, weekend_day, weekend_day, QUANTILES_TO_TRAIN)
        if forecast_weekend_day is not None:
            visualize_forecast_vs_actual(forecast_weekend_day, y_test, f"Forecast vs. Actual for Weekend Day ({weekend_day})")

        # --- Scenario 3: Bank Holiday ---
        bank_holiday = '2019-08-26' # Summer Bank Holiday
        forecast_bank_holiday = generate_forecasts(MODELS_DIR, X_test, bank_holiday, bank_holiday, QUANTILES_TO_TRAIN)
        if forecast_bank_holiday is not None:
            visualize_forecast_vs_actual(forecast_bank_holiday, y_test, f"Forecast vs. Actual for Bank Holiday ({bank_holiday})")

        # --- Scenario 4: Typical Week ---
        week_start = '2019-07-15'
        week_end = '2019-07-21'
        forecast_week = generate_forecasts(MODELS_DIR, X_test, week_start, week_end, QUANTILES_TO_TRAIN)
        if forecast_week is not None:
            visualize_forecast_vs_actual(forecast_week, y_test, f"Forecast vs. Actual for Week ({week_start} to {week_end})")

    except NameError as e:
        print(f"\n EXECUTION FAILED: A required variable is not defined: {e}")
        print("Please ensure 'best_params', 'X_train', 'y_train', 'X_val', 'y_val', 'X_test', and 'y_test' are all available in your notebook's memory before running.")
    except Exception as e:
        print(f"\n An unexpected error occurred: {e}")

"""# **OPTIMISATION**

## **Implementing the Cost Optimisation**
This script brings together the forecasting and optimisation stages to find the most cost-effective 24-hour charging schedule.

First, I define the core optimisation function. This function builds a linear programming model whose objective is to minimise the total electricity cost, which is a combination of the energy consumed (priced at a time-of-use tariff) and a demand charge for the highest power peak. I give this model a set of rules, or constraints, it must follow: it has to deliver the total amount of energy predicted by my median forecast, it cannot exceed the hourly demand predicted by my 90th-percentile forecast, and it must respect the physical ramp-rate limits of the charging hardware.

In the main part of the script, I define the economic parameters, such as the hourly electricity prices and the demand charge. I then run the entire pipeline for a sample day:

- I generate the demand forecast with its uncertainty bounds.

- I run the optimisation twice: once with the ramp-rate limits applied and once without, to analyse the impact of this constraint.

- Finally, I produce a visualisation that plots the original forecast against the two optimised schedules, all overlaid on a graph of the electricity prices. This clearly demonstrates how the optimiser strategically shifts energy usage to cheaper off-peak hours while staying within all the required operational limits. I also calculate and print the precise cost savings achieved by each optimisation strategy compared to a naive, un-optimised approach.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import xgboost as xgb
import pulp


# --- Prerequisites ---
# This code assumes the following variables from your notebook are available:
# - X_test, y_test: The test data split.
# It also assumes you have saved your quantile regression models.

# --- 1. Generate Forecast with Uncertainty ---
def generate_quantile_forecasts(models_dir, data_source, target_date_str, quantiles):
    """
    Loads the optimized quantile models and generates a 24-hour forecast
    for a specific target day, including prediction intervals.
    """
    print(f"--- Generating quantile forecasts for {target_date_str} ---")
    forecasts = {}
    target_date = pd.to_datetime(target_date_str)
    day_mask = (data_source.index.date == target_date.date())

    if not day_mask.any():
        print(f"Error: No data found for {target_date_str} in the data source.")
        return None

    X_day = data_source[day_mask]

    for q in quantiles:
        try:
            model_path = os.path.join(models_dir, f'ev_demand_xgb_quantile_{int(q*100)}.json')
            model = xgb.XGBRegressor()
            model.load_model(model_path)
        except Exception as e:
            print(f"Error loading model for quantile {q}: {e}")
            return None

        forecasted_demand = model.predict(X_day)
        forecasted_demand[forecasted_demand < 0] = 0
        forecasts[f'q_{q}'] = forecasted_demand

    return pd.DataFrame(forecasts, index=X_day.index)

# --- 2. Corrected Optimization Function with Upper Bounds ---
def solve_cost_optimization(tou_prices, demand_charge_price, max_ramp_rate,
                            total_energy_constraint, num_intervals, upper_bounds=None):
    """
    Formulates and solves the cost optimization problem, now including the
    hourly upper bound constraint for a more realistic scenario.
    """
    print(f"\n--- Formulating optimization (Ramp Rate <= {max_ramp_rate} kW/hr) ---")

    prob = pulp.LpProblem("Cost_Minimization", pulp.LpMinimize)

    # Variables
    scheduled_power = pulp.LpVariable.dicts("ScheduledPower", range(num_intervals), lowBound=0, cat='Continuous')
    peak_power = pulp.LpVariable("PeakPower", lowBound=0, cat='Continuous')

    # Objective
    energy_cost = pulp.lpSum(scheduled_power[i] * tou_prices[i] for i in range(num_intervals))
    demand_charge = peak_power * demand_charge_price
    prob += energy_cost + demand_charge, "Minimize_Total_Cost"

    # Constraints
    prob += pulp.lpSum(scheduled_power[i] for i in range(num_intervals)) >= total_energy_constraint, "Energy_Constraint"

    for i in range(num_intervals):
        prob += peak_power >= scheduled_power[i], f"Peak_Constraint_{i}"

    # Ramp constraints
    for i in range(1, num_intervals):
        prob += scheduled_power[i] - scheduled_power[i-1] <= max_ramp_rate, f"Ramp_Up_{i}"
        prob += scheduled_power[i-1] - scheduled_power[i] <= max_ramp_rate, f"Ramp_Down_{i}"

    # RE-INTRODUCED: Hourly upper bounds from the 90th percentile forecast
    if upper_bounds is not None:
        for i in range(num_intervals):
            prob += scheduled_power[i] <= upper_bounds[i], f"Upper_Limit_{i}"

    prob.solve(pulp.PULP_CBC_CMD(msg=0))

    if pulp.LpStatus[prob.status] == 'Optimal':
        print("Optimal solution found!")
        optimized_schedule = [pulp.value(scheduled_power[i]) for i in range(num_intervals)]
        optimized_peak = pulp.value(peak_power)
        return optimized_schedule, optimized_peak
    else:
        print("Status:", pulp.LpStatus[prob.status])
        return None, None

# --- 3. EVALUATION with Fair Comparison ---
def visualize_and_compare(median_forecast, upper_forecast,
                          optimized_no_ramp,
                          optimized_with_ramp,
                          tou_prices, sample_day):
    """
    Visualize results and calculate cost savings fairly.
    """
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(14, 10), sharex=True, gridspec_kw={'height_ratios': [3, 1]})
    time_index = np.arange(24)

    # Top: Power schedules
    ax1.plot(time_index, median_forecast, 'b--', label='Median Forecast (Expected Demand)', linewidth=2)
    ax1.plot(time_index, optimized_no_ramp, 'g-o', label='Optimized (No Ramp Limits)', alpha=0.7)
    ax1.plot(time_index, optimized_with_ramp, 'r-o', label='Optimised (With Ramp Limits)', alpha=0.7)
    ax1.fill_between(time_index, median_forecast, upper_forecast,
                     color='lightblue', alpha=0.5, label='Forecast Uncertainty (Median to 90th Percentile)')
    ax1.set_ylabel('Power (kW)', fontsize=12)
    ax1.set_title(f"Cost Optimisation Results for {sample_day}", fontsize=16)
    ax1.legend()
    ax1.grid(True, which='both', linestyle='--', linewidth=0.5)

    # Bottom: TOU Prices
    ax2.plot(time_index, tou_prices, 'k-s', label='TOU Price (£/kWh)')
    ax2.set_ylabel('Price (£/kWh)', fontsize=12)
    ax2.set_xlabel('Hour of the Day', fontsize=12)
    ax2.legend()
    ax2.grid(True, which='both', linestyle='--', linewidth=0.5)
    ax2.set_xticks(time_index)
    plt.savefig("Cost-Optimisation Result for {sample_day}.png", dpi=1200, bbox_inches='tight')
    plt.tight_layout()
    plt.show()

# --- RUN CORRECTED PIPELINE ---

# Use the correct path for your saved models
MODELS_DIR_ABSOLUTE = '/content/keras_tuner_dir/ev_demand_forecasting'

# Define Economic and Physical Parameters
tou_prices_pence = {
    0: 12, 1: 12, 2: 12, 3: 12, 4: 12, 5: 12,
    6: 22, 7: 22, 8: 22, 9: 22, 10: 22, 11: 22, 12: 22, 13: 22, 14: 22, 15: 22,
    16: 40, 17: 40, 18: 40, 19: 40,
    20: 22, 21: 22, 22: 22, 23: 22
}
tou_prices_pounds = [p / 100 for p in tou_prices_pence.values()]
demand_charge_pounds_per_kw = 12.00
# --- CORRECTED: Relaxing the ramp rate to restore feasibility ---
MAX_RAMP_RATE_KW = 15.0
quantiles_to_use = [0.10, 0.50, 0.90]

sample_day = '2019-07-16'

demand_forecasts_df = generate_quantile_forecasts(
    models_dir=MODELS_DIR_ABSOLUTE,
    data_source=X_test,
    target_date_str=sample_day,
    quantiles=quantiles_to_use
)

if demand_forecasts_df is not None:
    median_forecast = demand_forecasts_df['q_0.5']
    upper_forecast = demand_forecasts_df['q_0.9']

    total_energy = median_forecast.sum()
    num_intervals = len(median_forecast)
    upper_bounds = upper_forecast.values

    # Baseline: No optimization (naive median schedule)
    naive_energy_cost = np.sum(median_forecast.values * np.array(tou_prices_pounds))
    naive_peak_charge = median_forecast.max() * demand_charge_pounds_per_kw
    naive_total_cost = naive_energy_cost + naive_peak_charge

    # Optimization without ramp constraints (but with hourly upper bounds)
    optimized_no_ramp, new_peak_no_ramp = solve_cost_optimization(
        tou_prices=tou_prices_pounds,
        demand_charge_price=demand_charge_pounds_per_kw,
        max_ramp_rate=10000.0,  # No effective ramp constraint (large value)
        total_energy_constraint=total_energy,
        num_intervals=num_intervals,
        upper_bounds=upper_bounds
    )

    # Optimization with ramp constraints
    optimized_with_ramp, new_peak_with_ramp = solve_cost_optimization(
        tou_prices=tou_prices_pounds,
        demand_charge_price=demand_charge_pounds_per_kw,
        max_ramp_rate=MAX_RAMP_RATE_KW,
        total_energy_constraint=total_energy,
        num_intervals=num_intervals,
        upper_bounds=upper_bounds
    )

    if optimized_no_ramp is not None and optimized_with_ramp is not None:
        # Calculate costs
        opt_no_ramp_energy = np.sum(np.array(optimized_no_ramp) * np.array(tou_prices_pounds))
        opt_no_ramp_demand = new_peak_no_ramp * demand_charge_pounds_per_kw
        opt_no_ramp_total = opt_no_ramp_energy + opt_no_ramp_demand

        opt_with_ramp_energy = np.sum(np.array(optimized_with_ramp) * np.array(tou_prices_pounds))
        opt_with_ramp_demand = new_peak_with_ramp * demand_charge_pounds_per_kw
        opt_with_ramp_total = opt_with_ramp_energy + opt_with_ramp_demand

        # Savings calculations
        savings_no_ramp = naive_total_cost - opt_no_ramp_total
        savings_with_ramp = naive_total_cost - opt_with_ramp_total

        # Print results
        print("\n--- Cost Analysis ---")
        print(f"Naive Total Cost (Median Forecast): £{naive_total_cost:.2f}")
        print(f"Optimized (No Ramp):                £{opt_no_ramp_total:.2f} | Savings: £{savings_no_ramp:.2f} ({savings_no_ramp/naive_total_cost*100:.1f}%)")
        print(f"Optimized (With Ramp):              £{opt_with_ramp_total:.2f} | Savings: £{savings_with_ramp:.2f} ({savings_with_ramp/naive_total_cost*100:.1f}%)")

        # Visualize
        visualize_and_compare(median_forecast=median_forecast.values,
                              upper_forecast=upper_forecast.values,
                              optimized_no_ramp=optimized_no_ramp,
                              optimized_with_ramp=optimized_with_ramp,
                              tou_prices=tou_prices_pounds,
                              sample_day=sample_day)

"""## **Generalised Long-Term Optimisation Pipeline**

I have now extended the optimisation logic into a more powerful and flexible pipeline that can handle longer time horizons beyond a single day. The script is structured into three main, reusable components.

- Generalised Forecasting: I have created a function that loads the saved models and generates an uncertainty-aware forecast for any specified period, whether it is a single day, a week, or a month.

- Long-Term Optimisation: The core optimisation function is now capable of taking a forecast of any length and finding the most cost-effective charging schedule over that entire period. It automatically extends the daily time-of-use prices to match the forecast horizon and applies all the necessary constraints (total energy, hourly limits, and ramp rates) across the full duration.

- Analysis and Visualisation: I have combined the cost calculation and plotting into a single, adaptable function. It calculates the total cost savings and generates a final visualisation that compares the optimised schedule against the original forecast, with a dynamic x-axis that smartly adjusts its labels for daily, weekly, or monthly views.

Finally, in the main execution block, I use this new pipeline to run the full end-to-end process for two key long-term scenarios: a full week and a full month, demonstrating the scalability and practical application of the framework.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import xgboost as xgb
import pulp
import os

# --- Prerequisites ---
# This code assumes the following variables from your notebook are available:
# - X_test, y_test: The test data split.
# It also assumes you have saved your quantile regression models.

# --- 1. Generalized Forecasting Function ---
def generate_period_forecasts(models_dir, data_source, start_date_str, end_date_str, quantiles):
    """
    Loads quantile models and generates a forecast for a specified date range.
    """
    print(f"\n--- Generating forecasts from {start_date_str} to {end_date_str} ---")

    start_date = pd.to_datetime(start_date_str)
    end_date = pd.to_datetime(end_date_str)
    period_mask = (data_source.index.date >= start_date.date()) & (data_source.index.date <= end_date.date())

    if not period_mask.any():
        print(f"Error: No data found for the specified period in the data source.")
        return None

    X_period = data_source[period_mask]

    forecasts = {}
    for q in quantiles:
        try:
            model_path = os.path.join(models_dir, f'ev_demand_xgb_quantile_{int(q*100)}.json')
            model = xgb.XGBRegressor()
            model.load_model(model_path)
        except Exception as e:
            print(f"Error loading model for quantile {q}: {e}")
            return None

        forecasted_demand = model.predict(X_period)
        forecasted_demand[forecasted_demand < 0] = 0
        forecasts[f'q_{q}'] = forecasted_demand

    return pd.DataFrame(forecasts, index=X_period.index)

# --- 2. Corrected Generalized Optimization Function ---
def solve_long_term_optimization(total_energy_constraint, upper_bounds, tou_prices_daily, demand_charge_price, max_ramp_rate):
    """
    Formulates and solves the cost optimization problem for a multi-day period,
    using the median forecast for the energy target and the upper forecast for hourly limits.
    """
    num_intervals = len(total_energy_constraint)
    print(f"\n--- Formulating optimization for {num_intervals} hours ({num_intervals/24:.1f} days) ---")

    # Tile the daily TOU prices to match the forecast length
    num_days = int(np.ceil(num_intervals / 24))
    tou_prices_period = tou_prices_daily * num_days
    tou_prices_period = tou_prices_period[:num_intervals]

    prob = pulp.LpProblem("Long_Term_Cost_Minimization", pulp.LpMinimize)

    # Variables
    scheduled_power = pulp.LpVariable.dicts("ScheduledPower", range(num_intervals), lowBound=0, cat='Continuous')
    peak_power = pulp.LpVariable("PeakPower", lowBound=0, cat='Continuous')

    # Objective
    energy_cost = pulp.lpSum([scheduled_power[i] * tou_prices_period[i] for i in range(num_intervals)])
    demand_charge = peak_power * demand_charge_price
    prob += energy_cost + demand_charge, "Minimize_Total_Cost"

    # Constraints
    prob += pulp.lpSum(scheduled_power[i] for i in range(num_intervals)) >= total_energy_constraint.sum(), "Total_Energy_Constraint"
    for i in range(num_intervals):
        prob += peak_power >= scheduled_power[i], f"Peak_Constraint_{i}"
        # Add the hourly upper bound constraint
        prob += scheduled_power[i] <= upper_bounds[i], f"Upper_Limit_Constraint_{i}"
    for i in range(1, num_intervals):
        prob += scheduled_power[i] - scheduled_power[i-1] <= max_ramp_rate, f"Ramp_Up_{i}"
        prob += scheduled_power[i-1] - scheduled_power[i] <= max_ramp_rate, f"Ramp_Down_{i}"

    prob.solve(pulp.PULP_CBC_CMD(msg=0))

    if pulp.LpStatus[prob.status] == 'Optimal':
        print("Optimal solution found!")
        optimized_schedule = [pulp.value(scheduled_power[i]) for i in range(num_intervals)]
        optimized_peak = pulp.value(peak_power)
        return optimized_schedule, optimized_peak
    else:
        print("Status:", pulp.LpStatus[prob.status])
        return None, None

# --- 3. Generalized Visualization and Cost Analysis Function ---
def analyze_and_visualize_results(forecast_df, optimized_schedule, optimized_peak, tou_prices_daily, demand_charge_price, title):
    """Calculates costs and visualizes the results of a long-term optimization."""

    median_forecast = forecast_df['q_0.5'].values
    tou_prices_period = (tou_prices_daily * int(np.ceil(len(median_forecast) / 24)))[:len(median_forecast)]

    # --- Cost Calculation ---
    # Naive cost (following the median forecast)
    naive_energy_cost = np.sum(median_forecast * tou_prices_period)
    naive_peak_charge = median_forecast.max() * demand_charge_price
    naive_total_cost = naive_energy_cost + naive_peak_charge

    # Optimized cost
    optimized_energy_cost = np.sum(np.array(optimized_schedule) * tou_prices_period)
    optimized_demand_charge = optimized_peak * demand_charge_price
    optimized_total_cost = optimized_energy_cost + optimized_demand_charge

    # Savings
    cost_saving = naive_total_cost - optimized_total_cost
    cost_saving_percent = (cost_saving / naive_total_cost) * 100 if naive_total_cost > 0 else 0

    print("\n--- Cost Analysis ---")
    print(f"Naive Total Cost (Median Forecast): £{naive_total_cost:.2f}")
    print(f"Optimized Total Cost:               £{optimized_total_cost:.2f}")
    print(f"Total Savings:                      £{cost_saving:.2f} ({cost_saving_percent:.2f}%)")

    # --- Visualization ---
    time_index = np.arange(len(median_forecast))
    fig, (ax1, ax2) = plt.subplots(2, 1, figsize=(18, 10), sharex=True, gridspec_kw={'height_ratios': [3, 1]})

    # Top plot: Power schedules
    ax1.plot(time_index, median_forecast, 'b--', label='Median Forecast (Expected Demand)', linewidth=2)
    ax1.plot(time_index, optimized_schedule, 'r-', label='Cost-Optimized Schedule', alpha=0.8)
    ax1.fill_between(time_index, forecast_df['q_0.1'], forecast_df['q_0.9'],
                     color='lightblue', alpha=0.5, label='Forecast Uncertainty (10th-90th Percentile)')
    ax1.set_ylabel('Power (kW)', fontsize=12)
    ax1.set_title(title, fontsize=16)
    ax1.legend()
    ax1.grid(True, which='both', linestyle='--', linewidth=0.5)

    # Bottom plot: TOU Prices
    ax2.plot(time_index, tou_prices_period, 'k-s', label='TOU Price (£/kWh)')
    ax2.set_ylabel('Price (£/kWh)', fontsize=12)
    ax2.set_xlabel('Hour of the Period', fontsize=12)
    ax2.legend()
    ax2.grid(True, which='both', linestyle='--', linewidth=0.5)

    # Set appropriate x-axis ticks based on period length
    if len(time_index) <= 24 * 7: # Weekly
        ax2.set_xticks(np.arange(0, len(time_index), 24))
        ax2.set_xticklabels([f'Day {i+1}' for i in range(int(np.ceil(len(time_index)/24)))])
    else: # Monthly
        num_weeks = int(np.ceil(len(time_index) / (24 * 7)))
        ax2.set_xticks(np.arange(0, len(time_index), 24 * 7))
        ax2.set_xticklabels([f'Week {i+1}' for i in range(num_weeks)])

    plt.tight_layout()
    # Save the plot
    filename = f"{title.replace(' ', '_').replace(':', '').replace('(', '').replace(')', '')}.png"
    plt.savefig(filename, dpi=1200, bbox_inches='tight')
    print(f"Plot saved as: {filename}")
    plt.show()

# --- RUN THE PIPELINES ---

# Define Paths and Parameters
MODELS_DIR_ABSOLUTE = '/content/keras_tuner_dir/ev_demand_forecasting'
tou_prices_pence = {
    0: 12, 1: 12, 2: 12, 3: 12, 4: 12, 5: 12,
    6: 22, 7: 22, 8: 22, 9: 22, 10: 22, 11: 22, 12: 22, 13: 22, 14: 22, 15: 22,
    16: 40, 17: 40, 18: 40, 19: 40,
    20: 22, 21: 22, 22: 22, 23: 22
}
tou_prices_pounds_daily = [p / 100 for p in tou_prices_pence.values()]
demand_charge_pounds_per_kw = 12.00
MAX_RAMP_RATE_KW = 15.0
quantiles_to_use = [0.10, 0.50, 0.90]

# --- A. Weekly Optimization ---
week_start = '2019-07-15'
week_end = '2019-07-21'

weekly_forecasts_df = generate_period_forecasts(
    models_dir=MODELS_DIR_ABSOLUTE,
    data_source=X_test,
    start_date_str=week_start,
    end_date_str=week_end,
    quantiles=quantiles_to_use
)

if weekly_forecasts_df is not None:
    weekly_optimized_schedule, weekly_optimized_peak = solve_long_term_optimization(
        total_energy_constraint=weekly_forecasts_df['q_0.5'], # Use median for energy target
        upper_bounds=weekly_forecasts_df['q_0.9'].values, # Use 90th percentile for hourly limits
        tou_prices_daily=tou_prices_pounds_daily,
        demand_charge_price=demand_charge_pounds_per_kw,
        max_ramp_rate=MAX_RAMP_RATE_KW
    )
    if weekly_optimized_schedule is not None:
        analyze_and_visualize_results(
            weekly_forecasts_df,
            weekly_optimized_schedule,
            weekly_optimized_peak,
            tou_prices_pounds_daily,
            demand_charge_pounds_per_kw,
            f"Weekly Cost Optimization: {week_start} to {week_end}"
        )

# --- B. Monthly Optimization ---
month_start = '2019-07-01'
month_end = '2019-07-31'

monthly_forecasts_df = generate_period_forecasts(
    models_dir=MODELS_DIR_ABSOLUTE,
    data_source=X_test,
    start_date_str=month_start,
    end_date_str=month_end,
    quantiles=quantiles_to_use
)

if monthly_forecasts_df is not None:
    monthly_optimized_schedule, monthly_optimized_peak = solve_long_term_optimization(
        total_energy_constraint=monthly_forecasts_df['q_0.5'], # Use median for energy target
        upper_bounds=monthly_forecasts_df['q_0.9'].values, # Use 90th percentile for hourly limits
        tou_prices_daily=tou_prices_pounds_daily,
        demand_charge_price=demand_charge_pounds_per_kw,
        max_ramp_rate=MAX_RAMP_RATE_KW
    )
    if monthly_optimized_schedule is not None:
        analyze_and_visualize_results(
            monthly_forecasts_df,
            monthly_optimized_schedule,
            monthly_optimized_peak,
            tou_prices_pounds_daily,
            demand_charge_pounds_per_kw,
            f"Monthly Cost Optimization: {month_start} to {month_end}"
        )

"""## **Comparing Charging Strategies**

This final script is designed to simulate and compare the performance of three distinct EV charging strategies to demonstrate the value of my forecast-informed approach.

First, I define functions to simulate each strategy:

- Uncontrolled Charging: This serves as the real-world baseline, simply using the actual, historical demand data for a given day.

- Rule-Based Charging: This simulates a simple, non-intelligent strategy that shifts all the required charging to a pre-defined block of "solar-rich" hours, spreading the load as flatly as possible within that window.

- Forecast-Informed Optimisation: This is the core strategy developed in my dissertation. It uses the full pipeline of forecasting the demand with uncertainty bounds and then running the linear programming model to find the most cost-effective schedule that respects all operational constraints.

In the main execution block, I run these three simulations for a specific sample day. I then calculate the total cost—including both energy and demand charges—for each of the resulting 24-hour schedules. Finally, I present the results in two ways: first, as a formatted table that clearly shows the total cost and the percentage savings of the intelligent strategies over the uncontrolled baseline, and second, as a comparative line graph that visualises the different charging profiles over the 24-hour period. This provides a clear, quantitative, and visual conclusion to the analysis.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import xgboost as xgb
import pulp
import os

# Setup & Parameters
# This code assumes 'X_test' and 'y_test' DataFrames are available in memory.

# --- Economic Parameters ---
TOU_PRICES_POUNDS_DICT = {
    0: 0.12, 1: 0.12, 2: 0.12, 3: 0.12, 4: 0.12, 5: 0.12,
    6: 0.22, 7: 0.22, 8: 0.22, 9: 0.22, 10: 0.22, 11: 0.22,
    12: 0.22, 13: 0.22, 14: 0.22, 15: 0.22, 16: 0.40, 17: 0.40,
    18: 0.40, 19: 0.40, 20: 0.22, 21: 0.22, 22: 0.22, 23: 0.22
}
DEMAND_CHARGE_POUNDS_PER_KW = 12.00
MAX_RAMP_RATE_KW = 15.0
QUANTILES = [0.10, 0.50, 0.90]
SAMPLE_DAY = '2019-07-16'
MODELS_DIR = "/content/keras_tuner_dir/ev_demand_forecasting"


# Strategy simulation function

def simulate_uncontrolled_charging(actual_demand_series):
    """Represents the baseline scenario where charging follows actual demand."""
    print("Simulating: 1. Uncontrolled Charging")
    return actual_demand_series.values

def simulate_rule_based_charging(total_energy_needed, solar_hours=[11, 12, 13, 14, 15]):
    """Simulates charging only during specified 'solar-rich' hours."""
    print("Simulating: 2. Rule-Based Charging (Solar Hours)")
    num_intervals = 24

    # Simple optimization to spread the load flatly across solar hours
    prob = pulp.LpProblem("Rule_Based_Charging", pulp.LpMinimize)
    power = pulp.LpVariable.dicts("P", range(num_intervals), lowBound=0)
    peak = pulp.LpVariable("Peak", lowBound=0)

    prob += peak # Objective is just to find the flattest profile

    # Energy constraint
    prob += pulp.lpSum(power) >= total_energy_needed

    # Constraints
    for i in range(num_intervals):
        prob += peak >= power[i]
        if i not in solar_hours:
            prob += power[i] == 0 # Force power to be zero outside solar hours

    prob.solve(pulp.PULP_CBC_CMD(msg=False))

    if pulp.LpStatus[prob.status] == 'Optimal':
        return [power[i].value() for i in range(num_intervals)]
    else:
        return [0] * num_intervals # Return zero if no solution

def simulate_forecast_informed_charging(forecast_df):
    """Runs the full forecast-informed optimization pipeline."""
    print("Simulating: 3. Forecast-Informed Optimization")
    median = forecast_df['q_0.5'].values
    upper_bounds = forecast_df['q_0.9'].values
    total_energy = median.sum()
    num_intervals = len(median)

    prob = pulp.LpProblem("Forecast_Informed_Optimization", pulp.LpMinimize)

    power = pulp.LpVariable.dicts("P", range(num_intervals), lowBound=0)
    peak = pulp.LpVariable("Peak", lowBound=0)

    tou_prices_list = [TOU_PRICES_POUNDS_DICT[i] for i in range(num_intervals)]
    energy_cost = pulp.lpSum(power[i] * tou_prices_list[i] for i in range(num_intervals))
    demand_cost = peak * DEMAND_CHARGE_POUNDS_PER_KW
    prob += energy_cost + demand_cost

    prob += pulp.lpSum(power) >= total_energy
    for i in range(num_intervals):
        prob += power[i] <= upper_bounds[i]
        prob += peak >= power[i]
    for i in range(1, num_intervals):
        prob += power[i] - power[i-1] <= MAX_RAMP_RATE_KW
        prob += power[i-1] - power[i] <= MAX_RAMP_RATE_KW

    prob.solve(pulp.PULP_CBC_CMD(msg=False))

    if pulp.LpStatus[prob.status] == 'Optimal':
        return [power[i].value() for i in range(num_intervals)]
    else:
        raise RuntimeError("Forecast-Informed Optimization failed!")

# Helper function

def generate_forecasts(data_source, target_date_str):
    """Loads pre-trained models to generate a forecast for a target day."""
    print(f"\n🔮 Generating REAL forecasts for {target_date_str}...")
    forecasts = {}
    target_date = pd.to_datetime(target_date_str)
    day_mask = (data_source.index.date == target_date.date())
    X_day = data_source[day_mask]

    for q in QUANTILES:
        try:
            model_path = os.path.join(MODELS_DIR, f'ev_demand_xgb_quantile_{int(q*100)}.json')
            model = xgb.XGBRegressor()
            model.load_model(model_path)
            forecasted_demand = model.predict(X_day)
            forecasts[f'q_{q}'] = np.maximum(forecasted_demand, 0)
        except Exception as e:
            raise FileNotFoundError(f"Error loading model for quantile {q}: {e}")

    return pd.DataFrame(forecasts, index=X_day.index)

def calculate_costs(schedule, tou_prices, demand_charge):
    """Calculates the total daily cost of a given 24-hour schedule."""
    # Convert schedule to a NumPy array to handle both lists and arrays robustly
    schedule_np = np.array(schedule)

    energy_cost = sum(p * tou_prices[h] for h, p in enumerate(schedule_np))

    # CORRECTED: The 'if schedule' check is ambiguous for a NumPy array.
    # Check the size of the array instead.
    peak_demand = np.max(schedule_np) if schedule_np.size > 0 else 0

    demand_cost = peak_demand * demand_charge
    total_cost = energy_cost + demand_cost
    return {'Energy Cost': energy_cost, 'Demand Charge': demand_cost, 'Total Cost': total_cost}

# Main execution and visualisation
if __name__ == "__main__":
    try:
        # --- Data Preparation ---
        actual_demand_day = y_test[y_test.index.date == pd.to_datetime(SAMPLE_DAY).date()]
        if actual_demand_day.empty:
            raise ValueError(f"No actual demand data found for {SAMPLE_DAY} in y_test.")
        total_energy_actual = actual_demand_day.sum()

        # --- Run Simulations ---
        uncontrolled_schedule = simulate_uncontrolled_charging(actual_demand_day)
        rule_based_schedule = simulate_rule_based_charging(total_energy_actual)
        forecast_df = generate_forecasts(X_test, SAMPLE_DAY)
        forecast_informed_schedule = simulate_forecast_informed_charging(forecast_df)

        # --- Cost Calculation ---
        costs = {
            "Uncontrolled": calculate_costs(uncontrolled_schedule, TOU_PRICES_POUNDS_DICT, DEMAND_CHARGE_POUNDS_PER_KW),
            "Rule-Based": calculate_costs(rule_based_schedule, TOU_PRICES_POUNDS_DICT, DEMAND_CHARGE_POUNDS_PER_KW),
            "Forecast-Informed": calculate_costs(forecast_informed_schedule, TOU_PRICES_POUNDS_DICT, DEMAND_CHARGE_POUNDS_PER_KW),
        }

        # --- Display Results Table ---
        results_df = pd.DataFrame(costs).T
        baseline_cost = results_df.loc['Uncontrolled', 'Total Cost']
        results_df['Savings (£)'] = baseline_cost - results_df['Total Cost']
        results_df['Savings (%)'] = (results_df['Savings (£)'] / baseline_cost) * 100

        print("\n" + "="*60)
        print(f" COST & SAVINGS COMPARISON FOR {SAMPLE_DAY}")
        print("="*60)
        print(results_df.to_string(formatters={'Total Cost':'£{:.2f}'.format, 'Energy Cost':'£{:.2f}'.format, 'Demand Charge':'£{:.2f}'.format, 'Savings (£)':'£{:.2f}'.format, 'Savings (%)':'{:.1f}%'.format}))
        print("="*60)

        # --- Visualization ---
        plt.style.use('seaborn-v0_8-whitegrid')
        plt.figure(figsize=(14, 7))
        hours = np.arange(24)

        plt.plot(hours, uncontrolled_schedule, 'o-', color='black', label='Uncontrolled (Actual Demand)')
        plt.plot(hours, rule_based_schedule, 's--', color='blue', label='Rule-Based (Solar Hours)')
        plt.plot(hours, forecast_informed_schedule, '^-', color='red', label='Forecast-Informed Optimization')

        plt.title(f"Comparison of EV Charging Strategies for {SAMPLE_DAY}", fontsize=16)
        plt.xlabel("Hour of the Day", fontsize=12)
        plt.ylabel("Power (kW)", fontsize=12)
        plt.xticks(hours)
        plt.legend()
        plt.grid(True, which='both', linestyle='--', alpha=0.7)
        plt.tight_layout()
        plt.savefig("Comparison of EV Charging Strategies.png", dpi=1200, bbox_inches='tight')
        plt.show()

    except Exception as e:
        print(f"\n PIPELINE FAILED: {e}")

"""## **Generalised Long-Term Strategy Comparison**

I have refactored the entire simulation and comparison process into a robust, generalised pipeline that can analyse performance over any time horizon, not just a single day.

This final script is built around three key, reusable functions:

- A forecasting function that can generate uncertainty-aware predictions for any specified date range.

- A simulation function that takes the forecast and actual demand for a given period (e.g., a week or month) and runs all three charging strategies—Uncontrolled, Rule-Based, and Forecast-Informed—over that entire duration.

- An analysis and visualisation function that calculates the costs and savings for the period and generates a final comparison plot, with an intelligent x-axis that automatically adjusts its labels to be meaningful for weekly or monthly views.

In the main execution block, I orchestrate this new pipeline to run two long-term simulations: a full week and a full month. This demonstrates the scalability of my framework and provides a comprehensive comparison of the strategies' performance over extended, real-world periods, concluding the practical implementation section of my dissertation.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import xgboost as xgb
import pulp
import os

# Setup parameters
# This code assumes 'X_test' and 'y_test' DataFrames are available in memory.

# --- Economic Parameters ---
TOU_PRICES_POUNDS_DICT = {
    0: 0.12, 1: 0.12, 2: 0.12, 3: 0.12, 4: 0.12, 5: 0.12,
    6: 0.22, 7: 0.22, 8: 0.22, 9: 0.22, 10: 0.22, 11: 0.22,
    12: 0.22, 13: 0.22, 14: 0.22, 15: 0.22, 16: 0.40, 17: 0.40,
    18: 0.40, 19: 0.40, 20: 0.22, 21: 0.22, 22: 0.22, 23: 0.22
}
DEMAND_CHARGE_POUNDS_PER_KW = 12.00
MAX_RAMP_RATE_KW = 15.0
QUANTILES = [0.10, 0.50, 0.90]
MODELS_DIR = "/content/keras_tuner_dir/ev_demand_forecasting"

# Generalised simulation and helper function

def generate_period_forecasts(data_source, start_date_str, end_date_str):
    """Loads pre-trained models to generate forecasts for a specified date range."""
    print(f"\n🔮 Generating REAL forecasts from {start_date_str} to {end_date_str}...")
    forecasts = {}
    start_date = pd.to_datetime(start_date_str)
    end_date = pd.to_datetime(end_date_str)
    period_mask = (data_source.index.date >= start_date.date()) & (data_source.index.date <= end_date.date())

    if not period_mask.any():
        raise ValueError(f"No data found for the period from {start_date_str} to {end_date_str}.")

    X_period = data_source[period_mask]

    for q in QUANTILES:
        try:
            model_path = os.path.join(MODELS_DIR, f'ev_demand_xgb_quantile_{int(q*100)}.json')
            model = xgb.XGBRegressor()
            model.load_model(model_path)
            forecasted_demand = model.predict(X_period)
            forecasts[f'q_{q}'] = np.maximum(forecasted_demand, 0)
        except Exception as e:
            raise FileNotFoundError(f"Error loading model for quantile {q}: {e}")

    return pd.DataFrame(forecasts, index=X_period.index)

def simulate_strategies_for_period(actual_demand, forecast_df):
    """Runs all three charging strategy simulations for a given period."""
    schedules = {}
    total_energy_actual = actual_demand.sum()
    num_intervals = len(actual_demand)

    # --- Strategy 1: Uncontrolled ---
    print("Simulating: 1. Uncontrolled Charging")
    schedules['Uncontrolled'] = actual_demand.values

    # --- Strategy 2: Rule-Based (Solar) ---
    print("Simulating: 2. Rule-Based Charging (Solar Hours)")
    solar_hours_daily = [11, 12, 13, 14, 15]
    num_days_in_period = int(np.ceil(num_intervals / 24))
    solar_hours_period = [h for day in range(num_days_in_period) for h in [hr + day*24 for hr in solar_hours_daily]]

    prob_rule = pulp.LpProblem("Rule_Based_Period", pulp.LpMinimize)
    power_rule = pulp.LpVariable.dicts("P_rule", range(num_intervals), lowBound=0)
    peak_rule = pulp.LpVariable("Peak_rule", lowBound=0)
    prob_rule += peak_rule
    prob_rule += pulp.lpSum(power_rule) >= total_energy_actual
    for i in range(num_intervals):
        prob_rule += peak_rule >= power_rule[i]
        if i not in solar_hours_period:
            prob_rule += power_rule[i] == 0
    prob_rule.solve(pulp.PULP_CBC_CMD(msg=False))
    schedules['Rule-Based'] = [power_rule[i].value() for i in range(num_intervals)] if pulp.LpStatus[prob_rule.status] == 'Optimal' else [0]*num_intervals

    # --- Strategy 3: Forecast-Informed ---
    print("Simulating: 3. Forecast-Informed Optimization")
    prob_opt = pulp.LpProblem("Forecast_Informed_Period", pulp.LpMinimize)
    power_opt = pulp.LpVariable.dicts("P_opt", range(num_intervals), lowBound=0)
    peak_opt = pulp.LpVariable("Peak_opt", lowBound=0)

    tou_prices_list = [TOU_PRICES_POUNDS_DICT[i % 24] for i in range(num_intervals)]
    energy_cost = pulp.lpSum(power_opt[i] * tou_prices_list[i] for i in range(num_intervals))
    demand_cost = peak_opt * DEMAND_CHARGE_POUNDS_PER_KW
    prob_opt += energy_cost + demand_cost

    prob_opt += pulp.lpSum(power_opt) >= forecast_df['q_0.5'].sum()
    for i in range(num_intervals):
        prob_opt += power_opt[i] <= forecast_df['q_0.9'].iloc[i]
        prob_opt += peak_opt >= power_opt[i]
    for i in range(1, num_intervals):
        prob_opt += power_opt[i] - power_opt[i-1] <= MAX_RAMP_RATE_KW
        prob_opt += power_opt[i-1] - power_opt[i] <= MAX_RAMP_RATE_KW
    prob_opt.solve(pulp.PULP_CBC_CMD(msg=False))
    schedules['Forecast-Informed'] = [power_opt[i].value() for i in range(num_intervals)] if pulp.LpStatus[prob_opt.status] == 'Optimal' else [0]*num_intervals

    return schedules

def analyze_and_visualize(schedules, period_title):
    """Calculates costs and visualizes the comparison for a given period."""
    costs = {}
    num_intervals = len(schedules['Uncontrolled'])
    tou_prices_list = [TOU_PRICES_POUNDS_DICT[i % 24] for i in range(num_intervals)]

    for name, schedule in schedules.items():
        schedule_np = np.array(schedule)
        energy_cost = sum(p * tou_prices_list[h] for h, p in enumerate(schedule_np))
        peak_demand = np.max(schedule_np) if schedule_np.size > 0 else 0
        demand_cost = peak_demand * DEMAND_CHARGE_POUNDS_PER_KW
        costs[name] = {'Energy Cost': energy_cost, 'Demand Charge': demand_cost, 'Total Cost': energy_cost + demand_cost}

    results_df = pd.DataFrame(costs).T
    baseline_cost = results_df.loc['Uncontrolled', 'Total Cost']
    results_df['Savings (£)'] = baseline_cost - results_df['Total Cost']
    results_df['Savings (%)'] = (results_df['Savings (£)'] / baseline_cost) * 100 if baseline_cost > 0 else 0

    print("\n" + "="*60)
    print(f" COST & SAVINGS COMPARISON FOR {period_title}")
    print("="*60)
    print(results_df.to_string(formatters={'Total Cost':'£{:.2f}'.format, 'Energy Cost':'£{:.2f}'.format, 'Demand Charge':'£{:.2f}'.format, 'Savings (£)':'£{:.2f}'.format, 'Savings (%)':'{:.1f}%'.format}))
    print("="*60)

    plt.style.use('seaborn-v0_8-whitegrid')
    fig, ax = plt.subplots(figsize=(18, 8))
    hours = np.arange(num_intervals)

    ax.plot(hours, schedules['Uncontrolled'], color='black', label='Uncontrolled (Actual Demand)', alpha=0.7)
    ax.plot(hours, schedules['Rule-Based'], '--', color='blue', label='Rule-Based (Solar Hours)')
    ax.plot(hours, schedules['Forecast-Informed'], '-', color='red', label='Forecast-Informed Optimization', linewidth=2)

    ax.set_title(f"Comparison of EV Charging Strategies: {period_title}", fontsize=18)
    ax.set_xlabel("Time", fontsize=12)
    ax.set_ylabel("Power (kW)", fontsize=12)


    if num_intervals <= 24 * 7: # Weekly
        num_days = int(np.ceil(num_intervals / 24))
        ax.set_xticks(np.arange(0, num_intervals, 24))
        ax.set_xticklabels([f'Day {i+1}' for i in range(num_days)])
    else: # Monthly
        num_weeks = int(np.ceil(num_intervals / (24 * 7)))
        tick_locations = np.arange(0, num_weeks * 24 * 7, 24 * 7)
        ax.set_xticks(tick_locations)
        ax.set_xticklabels([f'Week {i+1}' for i in range(num_weeks)])


    # Moved inside the plot to the upper left, which is usually less crowded.
    ax.legend(loc='upper left', fancybox=True, shadow=True, fontsize='large')

    ax.grid(True, which='both', linestyle='--', alpha=0.7)
    plt.tight_layout()
    plt.savefig("Comparison of EV Charging Strategies 2.png", dpi=1200, bbox_inches='tight')
    plt.show()

# Main execution
if __name__ == "__main__":
    try:
        # --- A. Weekly Simulation ---
        week_start = '2019-07-15'
        week_end = '2019-07-21'
        actual_demand_week = y_test[(y_test.index.date >= pd.to_datetime(week_start).date()) & (y_test.index.date <= pd.to_datetime(week_end).date())]
        forecast_df_week = generate_period_forecasts(X_test, week_start, week_end)
        if not actual_demand_week.empty and forecast_df_week is not None:
            weekly_schedules = simulate_strategies_for_period(actual_demand_week, forecast_df_week)
            analyze_and_visualize(weekly_schedules, f"Typical Week ({week_start} to {week_end})")

        # Monthly Simulation ---
        month_start = '2019-07-01'
        month_end = '2019-07-31'
        actual_demand_month = y_test[(y_test.index.date >= pd.to_datetime(month_start).date()) & (y_test.index.date <= pd.to_datetime(month_end).date())]
        forecast_df_month = generate_period_forecasts(X_test, month_start, month_end)
        if not actual_demand_month.empty and forecast_df_month is not None:
            monthly_schedules = simulate_strategies_for_period(actual_demand_month, forecast_df_month)
            analyze_and_visualize(monthly_schedules, f"Typical Month ({month_start} to {month_end})")

    except Exception as e:
        print(f"\n PIPELINE FAILED: {e}")

"""# **SENSITIVITY ANALYSIS**

## **Sensitivity Analysis of Demand Charge**

To understand how the financial incentive for peak shaving affects my optimisation results, I am now performing a sensitivity analysis on the demand charge.

First, I create a dedicated, reusable optimisation function that takes the demand charge price as an input parameter. This allows me to easily re-run the optimisation with different values.

In the main execution block, I define a list of different demand charge prices to test. I then loop through these values, and for each one, I re-run the full cost optimisation. I calculate the total cost savings for each scenario against a consistent, uncontrolled baseline. Finally, I compile all the results into a single summary table, which clearly shows how the potential savings change as the demand charge becomes a more significant part of the total cost. This analysis is crucial for understanding the economic viability of this optimisation strategy under different tariff structures.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import xgboost as xgb
import pulp
import os

# Setup parameters

TOU_PRICES_POUNDS_DICT = {
    0: 0.12, 1: 0.12, 2: 0.12, 3: 0.12, 4: 0.12, 5: 0.12,
    6: 0.22, 7: 0.22, 8: 0.22, 9: 0.22, 10: 0.22, 11: 0.22,
    12: 0.22, 13: 0.22, 14: 0.22, 15: 0.22, 16: 0.40, 17: 0.40,
    18: 0.40, 19: 0.40, 20: 0.22, 21: 0.22, 22: 0.22, 23: 0.22
}
MAX_RAMP_RATE_KW = 15.0
QUANTILES = [0.10, 0.50, 0.90]
SAMPLE_DAY = '2019-07-16'
MODELS_DIR = "/content/keras_tuner_dir/ev_demand_forecasting"

# Optimisation helper function

# A dedicated optimization function that takes demand_charge_price as an argument ---
def run_optimization(forecast_df, demand_charge_price):
    """Runs the forecast-informed optimization for a given demand charge price."""
    median = forecast_df['q_0.5'].values
    upper_bounds = forecast_df['q_0.9'].values
    total_energy = median.sum()
    num_intervals = len(median)

    prob = pulp.LpProblem("Forecast_Informed_Optimization", pulp.LpMinimize)
    power = pulp.LpVariable.dicts("P", range(num_intervals), lowBound=0)
    peak = pulp.LpVariable("Peak", lowBound=0)

    tou_prices_list = [TOU_PRICES_POUNDS_DICT[i] for i in range(num_intervals)]
    energy_cost = pulp.lpSum(power[i] * tou_prices_list[i] for i in range(num_intervals))

    # Use the passed demand_charge_price parameter here
    demand_cost = peak * demand_charge_price
    prob += energy_cost + demand_cost

    # Constraints
    prob += pulp.lpSum(power) >= total_energy
    for i in range(num_intervals):
        prob += power[i] <= upper_bounds[i]
        prob += peak >= power[i]
    for i in range(1, num_intervals):
        prob += power[i] - power[i-1] <= MAX_RAMP_RATE_KW
        prob += power[i-1] - power[i] <= MAX_RAMP_RATE_KW

    prob.solve(pulp.PULP_CBC_CMD(msg=False))

    if pulp.LpStatus[prob.status] == 'Optimal':
        # Return the total cost directly from the solved problem
        return pulp.value(prob.objective)
    else:
        raise RuntimeError("Optimization failed!")

def generate_forecasts(data_source, target_date_str):
    """Loads pre-trained models to generate a forecast for a target day."""
    forecasts = {}
    target_date = pd.to_datetime(target_date_str)
    day_mask = (data_source.index.date == target_date.date())
    X_day = data_source[day_mask]

    for q in QUANTILES:
        try:
            model_path = os.path.join(MODELS_DIR, f'ev_demand_xgb_quantile_{int(q*100)}.json')
            model = xgb.XGBRegressor()
            model.load_model(model_path)
            forecasted_demand = model.predict(X_day)
            forecasts[f'q_{q}'] = np.maximum(forecasted_demand, 0)
        except Exception as e:
            raise FileNotFoundError(f"Error loading model for quantile {q}: {e}")
    return pd.DataFrame(forecasts, index=X_day.index)

def calculate_costs(schedule, tou_prices, demand_charge):
    """Calculates the total daily cost of a given 24-hour schedule."""
    schedule_np = np.array(schedule)
    energy_cost = sum(p * tou_prices[h] for h, p in enumerate(schedule_np))
    peak_demand = np.max(schedule_np) if schedule_np.size > 0 else 0
    demand_cost = peak_demand * demand_charge
    total_cost = energy_cost + demand_cost
    return {'Energy Cost': energy_cost, 'Demand Charge': demand_cost, 'Total Cost': total_cost}

# Sensitivity analysis loop
if __name__ == "__main__":
    try:
        # --- Parameters to test ---
        demand_charges_to_test = [10.00, 12.00, 15.00]
        results_list = []

        # --- Prepare data ONCE before the loop ---
        print("--- Preparing Data for Analysis ---")
        actual_demand_day = y_test[y_test.index.date == pd.to_datetime(SAMPLE_DAY).date()]
        if actual_demand_day.empty:
            raise ValueError(f"No actual demand data found for {SAMPLE_DAY} in y_test.")

        # Calculate the baseline uncontrolled cost
        uncontrolled_costs = calculate_costs(actual_demand_day.values, TOU_PRICES_POUNDS_DICT, 12.00) # Using 12 as the reference
        uncontrolled_total_cost = uncontrolled_costs['Total Cost']

        # Generate the forecast ONCE before the loop
        forecast_df = generate_forecasts(X_test, SAMPLE_DAY)
        print("\n--- Running Sensitivity Analysis for Demand Charge ---")

        # --- Loop through each parameter value ---
        for charge in demand_charges_to_test:
            print(f"Testing demand charge: £{charge:.2f}/kW...")

            # Run the optimization with the current demand charge value
            optimised_cost = run_optimization(forecast_df, demand_charge_price=charge)

            # Calculate savings against the consistent baseline
            cost_savings_abs = uncontrolled_total_cost - optimised_cost
            cost_savings_pct = (cost_savings_abs / uncontrolled_total_cost) * 100

            results_list.append({
                'Demand Charge (£/kW)': f"£{charge:.2f}",
                'Total Optimised Cost (£)': f"£{optimised_cost:,.2f}",
                'Total Savings (%)': f"{cost_savings_pct:.1f}%"
            })

        # --- Display the final results table ---
        sensitivity_df = pd.DataFrame(results_list)
        print("\n" + "="*50)
        print(" SENSITIVITY ANALYSIS RESULTS")
        print("="*50)
        print(sensitivity_df.to_string(index=False))
        print("="*50)

    except Exception as e:
        print(f"\n SCRIPT FAILED: {e}")

"""## **Training the 95th Percentile Model**

I am now training a dedicated XGBoost model specifically to predict a high-demand scenario, which will represent the 95th percentile in my uncertainty analysis.

First, I define the model's configuration using the optimal set of hyperparameters that I have already identified through the Optuna optimisation process. I then train this model on a combined dataset that includes both my original training and validation sets; this approach uses a larger amount of data to build the most robust and accurate final model possible.

Finally, once the training is complete, I save the finished model to a file. This ensures that this specific high-demand forecasting model is preserved and can be reloaded for the final stages of the optimisation and simulation without needing to be retrained.
"""

import pandas as pd
import numpy as np
import xgboost as xgb
import os

# --- 1. Define the Optimal Hyperparameters ---
# These are the best parameters you found from your Optuna study.
best_params = {
    'n_estimators': 530,
    'learning_rate': 0.01,
    'max_depth': 7,
    'subsample': 0.9,
    'colsample_bytree': 0.67,
    'min_child_weight': 6,
    'gamma': 0.66,
    'lambda': 0.01,
    'alpha': 4.6e-05
}

# --- 2. Initialize and Train the 95th Percentile Model ---
print("--- Training the 95th Percentile XGBoost Model ---")
# We use the same parameters but this model's output will be used to represent the 95th percentile
quantile_95_model = xgb.XGBRegressor(**best_params, seed=42)

# For a robust final model, train on the combined training and validation data
# This assumes X_train, y_train, X_val, y_val are available in your notebook
X_train_val = pd.concat([X_train, X_val])
y_train_val = pd.concat([y_train, y_val])

quantile_95_model.fit(X_train_val, y_train_val, verbose=False)

print("✅ Training complete.")

# --- 3. Save the Trained Model ---
MODELS_DIR = "/content/keras_tuner_dir/ev_demand_forecasting"
if not os.path.exists(MODELS_DIR):
    os.makedirs(MODELS_DIR)

model_filename = 'ev_demand_xgb_quantile_95.json'
model_filepath = os.path.join(MODELS_DIR, model_filename)

quantile_95_model.save_model(model_filepath)
print(f"✅ Model successfully saved to: '{model_filepath}'")

"""## **Sensitivity Analysis of Uncertainty Cap**

To determine the optimal level of risk for the optimisation, I am now performing a sensitivity analysis on the uncertainty cap. This analysis tests how the total cost and potential savings are affected when I use different percentile forecasts to set the maximum allowable power in any given hour.

First, I create a dedicated optimisation function that is parameterised by the specific quantile used for the upper bound. This allows me to easily re-run the optimisation using, for example, the 90th percentile forecast as a cap versus the more conservative 95th percentile.

In the main execution block, I loop through these different cap scenarios. For each one, I re-run the full cost optimisation and calculate the resulting total cost. Finally, I compile the results into a single summary table. This table clearly shows the trade-off: a tighter, less conservative cap (like the 90th percentile) might offer greater cost savings, but it carries a higher risk of being insufficient if actual demand exceeds this prediction. This analysis provides the final piece of evidence needed to make a recommendation on the most appropriate risk posture for operating the charging network.
"""

import pandas as pd
import numpy as np
import xgboost as xgb
import pulp
import os

# Setup & Parameters
# This code assumes 'X_test' and 'y_test' DataFrames are available in memory.

TOU_PRICES_POUNDS_DICT = {
    0: 0.12, 1: 0.12, 2: 0.12, 3: 0.12, 4: 0.12, 5: 0.12, 6: 0.22, 7: 0.22,
    8: 0.22, 9: 0.22, 10: 0.22, 11: 0.22, 12: 0.22, 13: 0.22, 14: 0.22,
    15: 0.22, 16: 0.40, 17: 0.40, 18: 0.40, 19: 0.40, 20: 0.22, 21: 0.22,
    22: 0.22, 23: 0.22
}
DEMAND_CHARGE_POUNDS_PER_KW = 12.00
SAMPLE_DAY = '2019-07-16'
MODELS_DIR = "/content/keras_tuner_dir/ev_demand_forecasting"

# Analysis function

def generate_forecasts_for_quantiles(data_source, target_date_str, quantiles_needed):
    """
    Generates forecasts for a list of specified quantiles.
    Requires pre-trained models named 'ev_demand_xgb_quantile_{quantile*100}.json'.
    """
    print(f"🔮 Generating forecasts for quantiles: {quantiles_needed}")
    forecasts = {}
    target_date = pd.to_datetime(target_date_str)
    day_mask = (data_source.index.date == target_date.date())
    X_day = data_source[day_mask]

    for q in quantiles_needed:
        model_path = os.path.join(MODELS_DIR, f'ev_demand_xgb_quantile_{int(q*100)}.json')
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"Model file not found: {model_path}. Please ensure it is trained and saved.")

        model = xgb.XGBRegressor()
        model.load_model(model_path)
        forecasted_demand = model.predict(X_day)
        forecasts[f'q_{q}'] = np.maximum(forecasted_demand, 0)

    return pd.DataFrame(forecasts, index=X_day.index)

def run_optimization_with_cap(forecast_df, upper_bound_quantile):
    """Runs optimization using a specific quantile for the upper power bound."""
    median_forecast = forecast_df['q_0.5'].values
    upper_bounds = forecast_df[f'q_{upper_bound_quantile}'].values
    total_energy_needed = median_forecast.sum()
    num_intervals = len(median_forecast)

    prob = pulp.LpProblem(f"Optimization_{upper_bound_quantile}_Cap", pulp.LpMinimize)
    power = pulp.LpVariable.dicts("P", range(num_intervals), lowBound=0)
    peak = pulp.LpVariable("Peak", lowBound=0)

    tou_prices_list = [TOU_PRICES_POUNDS_DICT[i] for i in range(num_intervals)]
    energy_cost = pulp.lpSum(power[i] * tou_prices_list[i] for i in range(num_intervals))
    demand_cost = peak * DEMAND_CHARGE_POUNDS_PER_KW
    prob += energy_cost + demand_cost

    # Constraints
    prob += pulp.lpSum(power) >= total_energy_needed
    for i in range(num_intervals):
        prob += power[i] <= upper_bounds[i] # Uses the specified quantile cap
        prob += peak >= power[i]
    # (Add ramp rate constraints here if needed)

    prob.solve(pulp.PULP_CBC_CMD(msg=False))

    if pulp.LpStatus[prob.status] == 'Optimal':
        return pulp.value(prob.objective)
    else:
        return -1 # Return an error code if optimization fails

# Main script execution

if __name__ == "__main__":
    try:
        # --- Define scenarios to test ---
        caps_to_test = [0.90, 0.95]
        results_list = []

        # --- Prepare data ONCE before the loop ---
        print("--- Preparing Data for Analysis ---")
        actual_demand_day = y_test[y_test.index.date == pd.to_datetime(SAMPLE_DAY).date()]

        # Calculate the baseline uncontrolled cost for savings comparison
        uncontrolled_total_cost = 1868.75 # Using the value from your dissertation's Table 4.2

        # Generate all necessary forecasts (50th, 90th, 95th) at once
        all_quantiles_needed = [0.50] + caps_to_test
        forecast_df = generate_forecasts_for_quantiles(X_test, SAMPLE_DAY, all_quantiles_needed)

        print("\n--- Running Sensitivity Analysis for Uncertainty Cap ---")

        # --- Loop through each scenario ---
        for cap in caps_to_test:
            print(f"Testing uncertainty cap: {int(cap*100)}th percentile...")

            optimised_cost = run_optimization_with_cap(forecast_df, upper_bound_quantile=cap)

            if optimised_cost == -1:
                print(f"  Optimization failed for {int(cap*100)}th percentile cap.")
                continue

            savings_pct = ((uncontrolled_total_cost - optimised_cost) / uncontrolled_total_cost) * 100

            results_list.append({
                'Uncertainty Cap': f"{int(cap*100)}th Percentile",
                'Total Optimised Cost (£)': f"£{optimised_cost:,.2f}",
                'Total Savings (%)': f"{savings_pct:.1f}%"
            })

        # --- Display the final results table ---
        sensitivity_df = pd.DataFrame(results_list)
        print("\n" + "="*60)
        print(" UNCERTAINTY CAP SENSITIVITY RESULTS")
        print("="*60)
        print(sensitivity_df.to_string(index=False))
        print("="*60)

    except Exception as e:
        print(f"\n SCRIPT FAILED: {e}")

"""## **Sensitivity Analysis of TOU Price**

To understand how the financial incentive to shift load impacts my results, I am performing a sensitivity analysis on the time-of-use (TOU) tariff.

First, I define three distinct pricing scenarios: a baseline, one with a low differential between peak and off-peak prices, and one with a high differential. My core optimisation function is then modified to accept any of these TOU price structures as an input.

In the main part of the script, I loop through each of these scenarios. For each tariff, I first calculate the baseline cost of the uncontrolled, actual demand. Then, I re-run the full cost optimisation using that same tariff to find the new optimal schedule and its cost. Finally, I compile the results into a summary table. This table clearly demonstrates how the percentage of potential cost savings changes as the financial incentive to shift energy usage to cheaper periods becomes stronger. This analysis is vital for understanding the effectiveness of the optimisation strategy under different market conditions.
"""

import pandas as pd
import numpy as np
import xgboost as xgb
import pulp
import os

# Setup parameters
# This code assumes 'X_test' and 'y_test' DataFrames are available in memory,
# along with a trained 90th percentile model.

# --- Define the Three TOU Price Scenarios ---
TOU_BASELINE = { # Your original prices
    0: 0.12, 1: 0.12, 2: 0.12, 3: 0.12, 4: 0.12, 5: 0.12, 6: 0.22, 7: 0.22,
    8: 0.22, 9: 0.22, 10: 0.22, 11: 0.22, 12: 0.22, 13: 0.22, 14: 0.22,
    15: 0.22, 16: 0.40, 17: 0.40, 18: 0.40, 19: 0.40, 20: 0.22, 21: 0.22,
    22: 0.22, 23: 0.22
}
TOU_LOW_DIFFERENTIAL = { # Peak is cheaper, off-peak is more expensive (less incentive)
    0: 0.15, 1: 0.15, 2: 0.15, 3: 0.15, 4: 0.15, 5: 0.15, 6: 0.22, 7: 0.22,
    8: 0.22, 9: 0.22, 10: 0.22, 11: 0.22, 12: 0.22, 13: 0.22, 14: 0.22,
    15: 0.22, 16: 0.35, 17: 0.35, 18: 0.35, 19: 0.35, 20: 0.22, 21: 0.22,
    22: 0.22, 23: 0.22
}
TOU_HIGH_DIFFERENTIAL = { # Peak is more expensive, off-peak is cheaper (more incentive)
    0: 0.10, 1: 0.10, 2: 0.10, 3: 0.10, 4: 0.10, 5: 0.10, 6: 0.22, 7: 0.22,
    8: 0.22, 9: 0.22, 10: 0.22, 11: 0.22, 12: 0.22, 13: 0.22, 14: 0.22,
    15: 0.22, 16: 0.50, 17: 0.50, 18: 0.50, 19: 0.50, 20: 0.22, 21: 0.22,
    22: 0.22, 23: 0.22
}
DEMAND_CHARGE_POUNDS_PER_KW = 12.00
SAMPLE_DAY = '2019-07-16'
MODELS_DIR = "/content/keras_tuner_dir/ev_demand_forecasting"

# Analysis Functions

def generate_forecasts(data_source, target_date_str, quantiles_needed=[0.5, 0.9]):
    """Generates forecasts for a list of specified quantiles."""
    forecasts = {}
    target_date = pd.to_datetime(target_date_str)
    day_mask = (data_source.index.date == target_date.date())
    X_day = data_source[day_mask]
    for q in quantiles_needed:
        model_path = os.path.join(MODELS_DIR, f'ev_demand_xgb_quantile_{int(q*100)}.json')
        model = xgb.XGBRegressor()
        model.load_model(model_path)
        forecasted_demand = model.predict(X_day)
        forecasts[f'q_{q}'] = np.maximum(forecasted_demand, 0)
    return pd.DataFrame(forecasts, index=X_day.index)

def run_optimization_with_tou(forecast_df, tou_prices_dict):
    """Runs optimization using a specific TOU price dictionary."""
    median = forecast_df['q_0.5'].values
    upper_bounds = forecast_df['q_0.9'].values
    total_energy = median.sum()
    num_intervals = len(median)

    prob = pulp.LpProblem("Optimization_TOU", pulp.LpMinimize)
    power = pulp.LpVariable.dicts("P", range(num_intervals), lowBound=0)
    peak = pulp.LpVariable("Peak", lowBound=0)

    tou_prices_list = [tou_prices_dict[i] for i in range(num_intervals)]
    energy_cost = pulp.lpSum(power[i] * tou_prices_list[i] for i in range(num_intervals))
    demand_cost = peak * DEMAND_CHARGE_POUNDS_PER_KW
    prob += energy_cost + demand_cost

    prob += pulp.lpSum(power) >= total_energy
    for i in range(num_intervals):
        prob += power[i] <= upper_bounds[i]
        prob += peak >= power[i]

    prob.solve(pulp.PULP_CBC_CMD(msg=False))
    return [power[i].value() for i in range(num_intervals)] if pulp.LpStatus[prob.status] == 'Optimal' else []

def calculate_total_cost(schedule, tou_prices_dict, demand_charge_price):
    """Calculates the total daily cost of a given schedule and tariff."""
    schedule_np = np.array(schedule)
    energy_cost = sum(p * tou_prices_dict[h] for h, p in enumerate(schedule_np))
    peak_demand = np.max(schedule_np) if schedule_np.size > 0 else 0
    demand_cost = peak_demand * demand_charge_price
    return energy_cost + demand_cost

# Main script execution

if __name__ == "__main__":
    try:
        tou_scenarios = {
            "Low Differential": TOU_LOW_DIFFERENTIAL,
            "Baseline Differential": TOU_BASELINE,
            "High Differential": TOU_HIGH_DIFFERENTIAL
        }
        results_list = []

        # Prepare data ONCE
        actual_demand_day = y_test[y_test.index.date == pd.to_datetime(SAMPLE_DAY).date()]
        forecast_df = generate_forecasts(X_test, SAMPLE_DAY)

        print("\n--- Running Sensitivity Analysis for TOU Price Differential ---")

        for name, tou_prices in tou_scenarios.items():
            print(f"Testing scenario: {name}...")

            # 1. Recalculate the baseline cost for THIS specific tariff
            uncontrolled_cost = calculate_total_cost(actual_demand_day.values, tou_prices, DEMAND_CHARGE_POUNDS_PER_KW)

            # 2. Run the optimization with THIS specific tariff
            optimised_schedule = run_optimization_with_tou(forecast_df, tou_prices)
            optimised_cost = calculate_total_cost(optimised_schedule, tou_prices, DEMAND_CHARGE_POUNDS_PER_KW)

            # 3. Calculate the savings against the correct baseline
            savings_pct = ((uncontrolled_cost - optimised_cost) / uncontrolled_cost) * 100

            results_list.append({
                'TOU Profile': name,
                'Total Optimised Cost (£)': f"£{optimised_cost:,.2f}",
                'Total Savings (%)': f"{savings_pct:.1f}%"
            })

        # --- Display the final results table ---
        sensitivity_df = pd.DataFrame(results_list)
        print("\n" + "="*60)
        print(" TOU PRICE DIFFERENTIAL SENSITIVITY RESULTS")
        print("="*60)
        print(sensitivity_df.to_string(index=False))
        print("="*60)

    except Exception as e:
        print(f"\n SCRIPT FAILED: {e}")

"""## **Sensitivity Analysis of Rule-Based Charging**

To provide a fair comparison with my forecast-informed approach, I am now performing a sensitivity analysis on a simpler, rule-based charging strategy.

First, I define the logic for this strategy: for any given time-of-use (TOU) tariff, the simulation identifies the hours with the absolute lowest electricity price and then shifts the entire day's energy demand to be spread evenly across only those cheapest hours.

In the main part of the script, I loop through the same three TOU pricing scenarios as before (low, baseline, and high differential). For each scenario, I calculate the baseline cost of uncontrolled charging and the cost of the rule-based schedule. Finally, I compile the results into a summary table. This analysis clearly demonstrates how the effectiveness of a simple, price-chasing strategy is directly influenced by the financial incentive offered by the tariff structure, providing a crucial point of comparison for the more advanced, forecast-informed optimisation.
"""

import pandas as pd
import numpy as np
import pulp

# Setup & Parameters
# This code assumes 'y_test' DataFrame is available in your notebook.

TOU_BASELINE = {
    0: 0.12, 1: 0.12, 2: 0.12, 3: 0.12, 4: 0.12, 5: 0.12, 6: 0.22, 7: 0.22,
    8: 0.22, 9: 0.22, 10: 0.22, 11: 0.22, 12: 0.22, 13: 0.22, 14: 0.22,
    15: 0.22, 16: 0.40, 17: 0.40, 18: 0.40, 19: 0.40, 20: 0.22, 21: 0.22,
    22: 0.22, 23: 0.22
}
TOU_LOW_DIFFERENTIAL = {
    0: 0.15, 1: 0.15, 2: 0.15, 3: 0.15, 4: 0.15, 5: 0.15, 6: 0.22, 7: 0.22,
    8: 0.22, 9: 0.22, 10: 0.22, 11: 0.22, 12: 0.22, 13: 0.22, 14: 0.22,
    15: 0.22, 16: 0.35, 17: 0.35, 18: 0.35, 19: 0.35, 20: 0.22, 21: 0.22,
    22: 0.22, 23: 0.22
}
TOU_HIGH_DIFFERENTIAL = {
    0: 0.10, 1: 0.10, 2: 0.10, 3: 0.10, 4: 0.10, 5: 0.10, 6: 0.22, 7: 0.22,
    8: 0.22, 9: 0.22, 10: 0.22, 11: 0.22, 12: 0.22, 13: 0.22, 14: 0.22,
    15: 0.22, 16: 0.50, 17: 0.50, 18: 0.50, 19: 0.50, 20: 0.22, 21: 0.22,
    22: 0.22, 23: 0.22
}
DEMAND_CHARGE_POUNDS_PER_KW = 12.00
SAMPLE_DAY = '2019-07-16'

# Analysis function

def simulate_rule_based_charging(total_energy_needed, tou_prices_dict):
    """
    Simulates charging by spreading demand evenly across the cheapest hours.
    """
    num_intervals = 24

    # Find the lowest price in the tariff
    min_price = min(tou_prices_dict.values())

    # Identify all hours that have this lowest price
    cheapest_hours = [hour for hour, price in tou_prices_dict.items() if price == min_price]

    # Spread the load evenly across these cheapest hours
    power_per_hour = total_energy_needed / len(cheapest_hours) if cheapest_hours else 0

    schedule = [0] * num_intervals
    for hour in cheapest_hours:
        schedule[hour] = power_per_hour

    return schedule

def calculate_total_cost(schedule, tou_prices_dict, demand_charge_price):
    """Calculates the total daily cost of a given schedule and tariff."""
    schedule_np = np.array(schedule)
    energy_cost = sum(p * tou_prices_dict[h] for h, p in enumerate(schedule_np))
    peak_demand = np.max(schedule_np) if schedule_np.size > 0 else 0
    demand_cost = peak_demand * demand_charge_price
    return energy_cost + demand_cost

# main execution

if __name__ == "__main__":
    try:
        tou_scenarios = {
            "Low Differential": TOU_LOW_DIFFERENTIAL,
            "Baseline Differential": TOU_BASELINE,
            "High Differential": TOU_HIGH_DIFFERENTIAL
        }
        results_list = []

        # Prepare data ONCE
        actual_demand_day = y_test[y_test.index.date == pd.to_datetime(SAMPLE_DAY).date()]
        total_energy_needed = actual_demand_day.sum()

        print("\n--- Running Sensitivity Analysis for Rule-Based Charging ---")

        for name, tou_prices in tou_scenarios.items():
            print(f"Testing scenario: {name}...")

            # 1. Recalculate the baseline cost for THIS specific tariff
            uncontrolled_cost = calculate_total_cost(actual_demand_day.values, tou_prices, DEMAND_CHARGE_POUNDS_PER_KW)

            # 2. Run the RULE-BASED simulation with THIS specific tariff
            rule_based_schedule = simulate_rule_based_charging(total_energy_needed, tou_prices)
            rule_based_cost = calculate_total_cost(rule_based_schedule, tou_prices, DEMAND_CHARGE_POUNDS_PER_KW)

            # 3. Calculate the savings against the correct baseline
            savings_pct = ((uncontrolled_cost - rule_based_cost) / uncontrolled_cost) * 100

            results_list.append({
                'TOU Profile': name,
                'Total Rule-Based Cost (£)': f"£{rule_based_cost:,.2f}",
                'Total Savings (%)': f"{savings_pct:.1f}%"
            })

        # --- Display the final results table ---
        sensitivity_df = pd.DataFrame(results_list)
        print("\n" + "="*60)
        print(" RULE-BASED CHARGING SENSITIVITY RESULTS")
        print("="*60)
        print(sensitivity_df.to_string(index=False))
        print("="*60)

    except Exception as e:
        print(f"\n SCRIPT FAILED: {e}")

"""## **Evaluating Forecast Calibration**

To ensure my uncertainty predictions are reliable, I am now formally evaluating the calibration of my probabilistic forecasts. This process checks whether the predicted uncertainty range accurately reflects the true uncertainty in the real-world data.

First, I define several functions to calculate key performance metrics for probabilistic forecasts. This includes the pinball loss, which is a specialised error metric for quantile regression, and the empirical coverage, which measures how often the actual energy demand falls within my 10th-90th percentile prediction interval. I also create a function to generate a reliability curve, which provides a clear visual assessment of how well-calibrated the forecasts are.

In the main execution block, I first load my trained quantile models and generate forecasts for the entire test set. I then use my helper functions to calculate and print the final calibration metrics. The goal is for the empirical coverage to be as close as possible to the nominal coverage of 80%, which would indicate that the model's representation of uncertainty is highly reliable. Finally, I generate the reliability curve plot for a clear visual confirmation.
"""

import pandas as pd
import numpy as np
import xgboost as xgb
import os
import matplotlib.pyplot as plt

# Setup parameters
# This assumes X_test and y_test are available in memory.
MODELS_DIR = "/content/keras_tuner_dir/ev_demand_forecasting"
QUANTILES = [0.10, 0.50, 0.90]

# ===============================================
# 2. CREATE THE FORECASTS DATAFRAME
# ===============================================
# This section was missing. It loads your pre-trained models to create the forecasts.
print("--- Loading pre-trained quantile models and generating forecasts ---")
forecast_data = {}
for q in QUANTILES:
    model_path = os.path.join(MODELS_DIR, f'ev_demand_xgb_quantile_{int(q*100)}.json')
    if not os.path.exists(model_path):
        raise FileNotFoundError(f"Model file not found: {model_path}")

    model = xgb.XGBRegressor()
    model.load_model(model_path)
    forecast_data[f'q_{q}'] = model.predict(X_test)

test_forecasts = pd.DataFrame(forecast_data, index=y_test.index)
print(" Forecasts DataFrame created successfully.")

# Caliberation metric functions

def calculate_pinball_loss(y_true, y_pred, quantile):
    """Calculates the pinball loss for a single quantile."""
    error = y_true - y_pred
    return np.mean(np.maximum(quantile * error, (quantile - 1) * error))

def compute_calibration_stats(y_true, forecasts_df):
    """Computes coverage and average pinball loss for 10-90 quantiles."""
    actuals = y_true.values
    q10 = forecasts_df['q_0.1'].values
    q50 = forecasts_df['q_0.5'].values
    q90 = forecasts_df['q_0.9'].values

    is_covered = (actuals >= q10) & (actuals <= q90)
    coverage = np.mean(is_covered) * 100

    loss_q10 = calculate_pinball_loss(actuals, q10, 0.10)
    loss_q50 = calculate_pinball_loss(actuals, q50, 0.50)
    loss_q90 = calculate_pinball_loss(actuals, q90, 0.90)
    avg_pinball_loss = np.mean([loss_q10, loss_q50, loss_q90])

    return {'coverage_80_percent': coverage, 'avg_pinball_loss': avg_pinball_loss}

def plot_reliability_curve(y_true, forecasts_df):
    """Plots a reliability curve for the 10-90 prediction interval."""
    actuals = y_true.values
    q10 = forecasts_df['q_0.1'].values
    q90 = forecasts_df['q_0.9'].values

    coverage = np.mean((actuals >= q10) & (actuals <= q90))

    plt.figure(figsize=(6, 6))
    plt.plot([0, 1], [0, 1], 'k:', label='Perfect Calibration')
    plt.plot([0.8], [coverage], 'o', markersize=10, label=f'Empirical Coverage = {coverage:.1%}')

    plt.title('Reliability Curve for 80% Prediction Interval')
    plt.xlabel('Nominal Coverage (Confidence Level)')
    plt.ylabel('Empirical Coverage (Actual Frequency)')
    plt.xlim(0, 1)
    plt.ylim(0, 1)
    plt.grid(True, linestyle='--')
    plt.legend()
    plt.gca().set_aspect('equal', adjustable='box')
    plt.savefig("reliability_curve.png", dpi=300)
    plt.show()

# main script execution

if __name__ == "__main__":
    # Calculate stats
    calibration_metrics = compute_calibration_stats(y_test, test_forecasts)

    print("\n" + "="*50)
    print(" PROBABILISTIC FORECAST CALIBRATION RESULTS")
    print("="*50)
    print(f"Target Coverage (10-90 Interval): 80.0%")
    print(f"Empirical Coverage: {calibration_metrics['coverage_80_percent']:.1f}%")
    print(f"Average Pinball Loss (q10, q50, q90): {calibration_metrics['avg_pinball_loss']:.4f}")
    print("="*50)

    # Generate plot
    plot_reliability_curve(y_test, test_forecasts)

"""# **Analysing the Socio-Technical Dimensions**

This final script is dedicated to creating the key visualisations for the socio-technical analysis in my dissertation. It moves beyond the purely technical aspects of forecasting and optimisation to explore the human and societal factors that shape the charging network. The script is organised into three main analyses:

First, I define a new data cleaning function to standardise the names of the charging locations. This is a crucial preparatory step to ensure that my analysis of geographic usage is accurate.

The script then generates three specific plots:

- Geographic Equity: This function aggregates the charging data by location to produce a bar chart showing the total energy delivered at each site. This allows me to analyse the distribution of charging infrastructure usage, providing evidence for discussions about the equity of access and the balance between high-capacity urban hubs and smaller, more remote spokes.

- Cost vs. Convenience: This function plots the un-optimised, convenience-driven charging schedule directly against the cost-optimised schedule. By overlaying the time-of-use electricity prices, this visualisation provides a clear illustration of the central behavioural-economic trade-off, showing how the optimisation algorithm must shift demand away from peak times to achieve savings.

- Social Drivers: This final function compares the average hourly demand profiles for weekdays versus weekends. This plot highlights how predictable social routines, such as the typical work week, are a primary driver of the charging patterns that the forecasting model must learn.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os

# Setup

# Define the directory where all plots will be saved
SAVE_DIR = "project_visualizations"
if not os.path.exists(SAVE_DIR):
    os.makedirs(SAVE_DIR)
print(f"All socio-technical visualizations will be saved to the '{SAVE_DIR}' directory.")

# --- Prerequisites ---
# This script assumes the following variables are available in your notebook's memory:
# - df: The original, cleaned DataFrame with individual charging session data.
# - df_hourly: The hourly aggregated DataFrame.
# - weekly_schedules: The dictionary containing the three strategy schedules for a week.
# - TOU_PRICES_POUNDS_DICT: The dictionary of hourly prices.

# Data cleaning function

def clean_site_names(df_sessions):
    """
    Standardizes charging site names to merge duplicates.
    e.g., 'Broxden Park & Ride, Perth' becomes 'Broxden Park & Ride'.
    """
    print("\n--- Cleaning and standardizing site names for consistent analysis ---")

    # Define the mapping from old, inconsistent names to a standard name
    name_mapping = {
        'Broxden Park & Ride, Perth': 'Broxden Park & Ride',
        'Kinross Park and Ride, Kinross': 'Kinross Park and Ride',
        'South Inch Car Park, Perth': 'South Inch Car Park',
        'Canal Street Car Park 3rd floor, Perth': 'Canal Street Car Park',
        'Mill Street, Perth': 'Mill Street Car Park' # Standardizing based on common naming
    }

    # Apply the mapping to the 'Site' column
    df_sessions['Site'] = df_sessions['Site'].replace(name_mapping)
    print("✅ Site names cleaned successfully.")
    return df_sessions


# Plotting functions

def plot_geographic_equity(df_sessions, save_dir):
    """
    Analyzes and plots charging patterns by location to assess equity of access.
    """
    print("\n--- Generating Plot 1: Geographic Equity Analysis ---")
    try:
        # Use the standardized column name 'CP_ID'
        if 'CP_ID' in df_sessions.columns:
            cpid_col = 'CP_ID'
        else:
            raise KeyError("Could not find a charge point ID column ('CP_ID').")

        site_usage = df_sessions.groupby('Site').agg(
            TotalEnergy_GWh=('Total_kWh', lambda x: x.sum() / 1000000),
            AverageDuration_Hrs=('ChargingDurationHrs', 'mean'),
            NumberOfSessions=(cpid_col, 'count')
        ).sort_values(by='TotalEnergy_GWh', ascending=False)

        # Plot 1: Total Energy Delivered per Site
        plt.style.use('seaborn-v0_8-whitegrid')
        fig, ax = plt.subplots(figsize=(12, 8))

        top_n = 15
        site_usage.head(top_n).sort_values(by='TotalEnergy_GWh', ascending=True)['TotalEnergy_GWh'].plot(
            kind='barh', ax=ax, color='teal'
        )

        ax.set_title(f'Total Energy Delivered by Site (Top {top_n})', fontsize=16)
        ax.set_xlabel('Total Energy Delivered (GWh)', fontsize=12)
        ax.set_ylabel('Charging Site', fontsize=12)
        ax.grid(axis='x', linestyle='--', alpha=0.7)

        plt.tight_layout()
        save_path = os.path.join(save_dir, "socio_geographic_equity.png")
        plt.savefig(save_path)
        plt.show()
        plt.close()
        print(f"Saved: socio_geographic_equity.png")

    except Exception as e:
        print(f"Could not generate Geographic Equity plot. Error: {e}")


def plot_cost_vs_convenience(schedules, tou_prices, save_dir):
    """
    Visualizes the trade-off between the cost-optimized schedule and user convenience.
    """
    print("\n--- Generating Plot 2: Behavioral Economics (Cost vs. Convenience) ---")
    try:
        uncontrolled = schedules['Uncontrolled'][:24]
        forecast_informed = schedules['Forecast-Informed'][:24]
        tou_prices_list = [tou_prices[h] for h in range(24)]

        plt.style.use('seaborn-v0_8-whitegrid')
        fig, ax1 = plt.subplots(figsize=(14, 7))

        hours = np.arange(24)

        ax1.plot(hours, uncontrolled, 'o--', color='black', label='Uncontrolled Demand (User Convenience)')
        ax1.plot(hours, forecast_informed, 'o-', color='red', label='Cost-Optimized Schedule')
        ax1.set_xlabel('Hour of the Day', fontsize=14)
        ax1.set_ylabel('Power (kW)', fontsize=14, color='black')
        ax1.tick_params(axis='y', labelcolor='black')
        ax1.grid(True, which='both', linestyle='--', alpha=0.5)

        ax2 = ax1.twinx()
        ax2.plot(hours, tou_prices_list, 's-', color='green', label='TOU Price (£/kWh)')
        ax2.set_ylabel('Price (£/kWh)', fontsize=14, color='green')
        ax2.tick_params(axis='y', labelcolor='green')
        ax2.set_ylim(bottom=0)

        fig.suptitle('The Cost vs. Convenience Trade-Off', fontsize=18)
        lines, labels = ax1.get_legend_handles_labels()
        lines2, labels2 = ax2.get_legend_handles_labels()
        ax2.legend(lines + lines2, labels + labels2, loc='upper left')

        plt.tight_layout(rect=[0, 0.03, 1, 0.95])
        save_path = os.path.join(save_dir, "socio_cost_vs_convenience.png")
        plt.savefig(save_path)
        plt.show()
        plt.close()
        print(f"Saved: socio_cost_vs_convenience.png")

    except Exception as e:
        print(f"Could not generate Cost vs. Convenience plot. Error: {e}")


def plot_social_drivers(df_hourly, save_dir):
    """
    Plots and compares the average demand profiles for weekdays vs. weekends.
    """
    print("\n--- Generating Plot 3: Social Drivers (Weekday vs. Weekend) ---")
    try:
        target_col = 'Total_kwh'
        df_hourly['DayType'] = df_hourly['WorkingStatus'].apply(lambda x: 'Weekday' if x == 1 else 'Weekend')
        demand_profiles = df_hourly.groupby(['DayType', 'ConnectionHour'])[target_col].mean().unstack(level=0)

        plt.style.use('seaborn-v0_8-whitegrid')
        plt.figure(figsize=(12, 6))

        plt.plot(demand_profiles.index, demand_profiles['Weekday'], 'o-', label='Average Weekday Demand')
        plt.plot(demand_profiles.index, demand_profiles['Weekend'], 's-', label='Average Weekend Demand')

        plt.title('Charging Demand Driven by Social Routines', fontsize=16)
        plt.xlabel('Hour of the Day', fontsize=12)
        plt.ylabel('Average Hourly Demand (kWh)', fontsize=12)
        plt.xticks(np.arange(0, 24, 2))
        plt.legend(fontsize='large')
        plt.grid(True, which='both', linestyle='--', alpha=0.7)

        plt.tight_layout()
        save_path = os.path.join(save_dir, "socio_weekday_vs_weekend.png")
        plt.savefig(save_path)
        plt.show()
        plt.close()
        print(f"Saved: socio_weekday_vs_weekend.png")

    except Exception as e:
        print(f"Could not generate Social Drivers plot. Error: {e}")

# Main execution

if __name__ == "__main__":
    print("\nStarting socio-technical visualization export process...")

    required_vars = ['df', 'df_hourly', 'weekly_schedules', 'TOU_PRICES_POUNDS_DICT']
    missing_vars = [var for var in required_vars if var not in locals() and var not in globals()]

    if not missing_vars:
        # --- NEW STEP: Clean the site names in the 'df' DataFrame first ---
        df_cleaned = clean_site_names(df.copy()) # Use a copy to avoid modifying the original df in memory

        # Run all plotting functions with the cleaned data
        plot_geographic_equity(df_cleaned, SAVE_DIR)
        plot_cost_vs_convenience(weekly_schedules, TOU_PRICES_POUNDS_DICT, SAVE_DIR)
        plot_social_drivers(df_hourly, SAVE_DIR)
        print("\n Socio-technical visualization export process complete!")
    else:
        print(f"\n Could not run script. The following variables are missing: {', '.join(missing_vars)}")
        print("Please ensure you have run all previous notebook cells to generate the required data.")

!pip install contextily pyproj adjustText

"""# **Downloading Geographic Boundary Data for PKC**

To create the choropleth map for my geographic analysis, I first need the map data itself. This script automates the process of downloading the necessary geographic boundary file.

I am using the requests library to fetch a GeoJSON file from a stable online source. This file contains the specific shapes of all the administrative wards in Scotland. The script makes a network request to the URL, checks to ensure the download is successful, and then saves the content to a local file named ward_boundaries.geojson. I have also included error handling to manage any potential network problems that might occur during the download. This step is a prerequisite for creating the final map visualisation.


"""

import requests
import os

# This is a direct link to a raw GeoJSON file and is much more reliable.
wards_file_url = "https://raw.githubusercontent.com/martinjc/UK-GeoJSON/master/json/administrative/sco/lad.json"
# The local file path where the data will be saved
local_file_path = "ward_boundaries.geojson"

print(f"--- Downloading ward boundary data from new, stable URL ---")
print(f"Source: {wards_file_url}")

try:
    # Make the request to download the file with a timeout
    response = requests.get(wards_file_url, timeout=60)
    # Check if the request was successful (e.g., not a 404 or 500 error)
    response.raise_for_status()

    # Save the downloaded content to the local file
    with open(local_file_path, 'wb') as f:
        f.write(response.content)

    print(f"\n Successfully downloaded and saved data to: {local_file_path}")
    print("You can now proceed to run the 'Choropleth Map' script.")

except requests.exceptions.RequestException as e:
    print(f"\n FAILED to download the file. A network error occurred: {e}")
    print("If this error persists, there may be a broader network issue. Please try again in a few minutes.")

"""## **Generating the Final Geographic Usage Map**

This final script is dedicated to creating the choropleth map, which is the key visualisation for the socio-technical analysis of geographic equity. The process is broken down into two main stages: data preparation and map generation.

First, in the data preparation stage, I create a function that takes my raw session data and merges it with an official list of charger locations. This is a critical data wrangling step that involves several transformations:

- I load the official location data, which contains the coordinates for each charging point.

- I perform an essential coordinate transformation, converting the locations from the British National Grid system to the standard WGS84 format used by most mapping software.

- I then carry out a definitive manual mapping to standardise the site names, as they are often inconsistent between the session data and the official location list. This ensures that the usage data is correctly joined to its geographic coordinates.

- Finally, I merge the two datasets to create a single, unified GeoDataFrame that contains both the usage statistics and the precise location for each charging site.

In the second stage, a dedicated map generation function takes this prepared data and creates the final visualisation.

- It begins by loading the previously downloaded GeoJSON file to create a base map showing the administrative boundary of Perth and Kinross.

- The charger locations are then plotted on top of this map as a scatter plot. I use visual encoding to convey two types of information simultaneously: the size of each point represents the total number of charging sessions, while the colour represents the total energy delivered.

- To ensure the map is readable, I add a numbered label to each point and create a custom legend box directly on the map. This legend clearly lists the name of each site corresponding to its number, which is a much more effective solution than a standard legend for a map with this many data points.

- The final output is a high-resolution, data-rich map that provides a clear and immediate understanding of the geographic distribution and usage patterns of the entire charging network, saved as a PNG file for inclusion in my dissertation.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
import geopandas as gpd
from pyproj import Transformer
from adjustText import adjust_text
from matplotlib.offsetbox import AnchoredText

# setup

# Define the directory where the map will be saved
SAVE_DIR = "project_visualizations"
if not os.path.exists(SAVE_DIR):
    os.makedirs(SAVE_DIR)
print(f"The final map will be saved to the '{SAVE_DIR}' directory.")

# --- Prerequisites ---
# This script assumes the 'df' DataFrame is available in your notebook's memory.

# Data preparation and merging

def prepare_map_data(df_sessions, locations_csv_path):
    """
    Cleans, merges, and transforms coordinate data for mapping.
    """
    print("\n--- Preparing data for mapping ---")
    try:
        # Load the official PKC charger locations
        df_locations = pd.read_csv(locations_csv_path)

        # --- Rename the site and coordinate columns to a standard format ---
        df_locations.rename(columns={'CP_Name': 'Site', 'x': 'longitude', 'y': 'latitude'}, inplace=True)

        # --- Force data types and handle missing values ---
        df_locations['Site'] = df_locations['Site'].astype(str).str.strip()
        df_sessions['Site'] = df_sessions['Site'].astype(str).str.strip()
        df_locations['latitude'] = pd.to_numeric(df_locations['latitude'], errors='coerce')
        df_locations['longitude'] = pd.to_numeric(df_locations['longitude'], errors='coerce')
        df_locations.dropna(subset=['latitude', 'longitude'], inplace=True)

        # --- Coordinate Transformation (British National Grid to WGS84) ---
        transformer = Transformer.from_crs("EPSG:27700", "EPSG:4326", always_xy=True)
        df_locations['lon_wgs84'], df_locations['lat_wgs84'] = transformer.transform(
            df_locations['longitude'].values,
            df_locations['latitude'].values
        )
        print(f" Loaded and processed {len(df_locations)} charger locations.")

        # --- Definitive Manual Mapping ---
        location_name_map = {
            'ACCESS ROAD INTO PARK AND RIDE BROXDEN AVENUE': 'Broxden Park & Ride',
            'CAR PARK OFF JUNCTION ROAD PARK AND RIDE': 'Kinross Park and Ride',
            'CAR PARK OFF SHORE ROAD AT SOUTH INCH': 'South Inch Car Park',
            'CANAL STREETMULTY STOREY CAR PARK': 'Canal Street Car Park',
            'CAR PARK MULTI STOREY OFF CANAL STREET': 'Canal Street Car Park',
            'CAR PARK OFF KING STREET TOWARDS GALVELMORE STREET': 'King Street Car Park, Crieff',
            'CAR PARK OFF HIGH STREET': 'Crown Inn Wynd Car Park, Auchterarder',
            'CAR PARK OFF CROFT LANE AT LESLIE STREET': 'Leslie Street Car Park, Blairgowrie',
            'CAR PARK OFF RIE-ACHAN ROAD': 'Rie-Achan Road Car Park, Pitlochry',
            'CAR PARK OFF MILL STREET EAST': 'Mill Street Car Park',
            'CAR PARK OFF MONESS TERRACE': 'Moness Terrace Car Park, Aberfeldy',
            'FRIARTON WASTE TRANSFER STATION': 'Friarton Depot',
            'CAR PARK IN MARKET SQUARE': 'Market Square Alyth',
            'LEADENFLOWER CAR PARK': 'Atholl Street Car Park, Dunkeld'
        }
        df_locations['Site'] = df_locations['Site'].replace(location_name_map)

        session_name_map = {
            'Broxden Park & Ride, Perth': 'Broxden Park & Ride',
            'Kinross Park and Ride, Kinross': 'Kinross Park and Ride',
            'South Inch Car Park, Perth': 'South Inch Car Park',
            'Canal Street Car Park 3rd floor, Perth': 'Canal Street Car Park',
            'Canal Street Car Park 3rd floor': 'Canal Street Car Park',
            'Mill Street, Perth': 'Mill Street Car Park',
            'Friarton Depot, Perth': 'Friarton Depot'
        }
        df_sessions['Site'] = df_sessions['Site'].replace(session_name_map)

        # Calculate usage statistics per site
        site_usage = df_sessions.groupby('Site').agg(
            TotalEnergy_kWh=('Total_kWh', 'sum'),
            NumberOfSessions=('CP_ID', 'count')
        ).reset_index()

        # Merge the usage data with the location data
        merged_data = pd.merge(site_usage, df_locations[['Site', 'lat_wgs84', 'lon_wgs84']], on='Site', how='inner')
        merged_data.drop_duplicates(subset='Site', inplace=True)

        print(f" Successfully merged usage data with {len(merged_data)} geolocated sites.")
        return merged_data

    except Exception as e:
        print(f" Could not prepare map data. Error: {e}")
        return None

# Final Map generation with legends

def create_final_map(map_data, boundary_file_path, save_dir):
    """
    Generates and saves a static map with a real boundary background and a numbered legend for all sites.
    """
    print("\n--- Generating final map visualization with numbered legend ---")
    try:
        # --- Load and prepare the boundary data ---
        gdf_boundaries = gpd.read_file(boundary_file_path)
        council_name_col = 'LAD13NM'
        pkc_boundary = gdf_boundaries[gdf_boundaries[council_name_col] == 'Perth and Kinross']

        # --- Convert charger data to a GeoDataFrame ---
        gdf_chargers = gpd.GeoDataFrame(
            map_data,
            geometry=gpd.points_from_xy(map_data.lon_wgs84, map_data.lat_wgs84),
            crs="EPSG:4326"
        ).sort_values(by='NumberOfSessions', ascending=False).reset_index(drop=True)

        # --- Plotting ---
        plt.style.use('seaborn-v0_8-whitegrid')
        fig, ax = plt.subplots(1, 1, figsize=(12, 12))

        # Plot the main PKC boundary
        pkc_boundary.plot(ax=ax, color='lightgray', edgecolor='black', linewidth=0.7)

        # Overlay all charger locations
        scatter = ax.scatter(
            gdf_chargers.geometry.x, gdf_chargers.geometry.y,
            s=np.log(gdf_chargers['NumberOfSessions'] + 1) * 50,
            c=gdf_chargers['TotalEnergy_kWh'],
            cmap='viridis', alpha=0.8, edgecolors='k', linewidth=0.5, zorder=10
        )
        cbar = plt.colorbar(scatter, ax=ax, shrink=0.5)
        cbar.set_label('Total Energy Delivered (kWh)', fontsize=12)

        # --- Add Numbered Labels to Map and Create Legend Text ---
        legend_texts = ["Site Legend:"]
        texts_to_adjust = []
        for i, (idx, site) in enumerate(gdf_chargers.iterrows()):
            label_num = i + 1
            texts_to_adjust.append(
                ax.text(site.geometry.x, site.geometry.y, str(label_num), color='white',
                        ha='center', va='center', fontsize=8, weight='bold', zorder=11)
            )
            legend_texts.append(f"  {label_num}: {site['Site']}")

        # Add the legend box to the main plot
        legend_box = AnchoredText("\n".join(legend_texts), loc='lower left', frameon=True,
                                  prop=dict(size=8, ha='left'))
        legend_box.patch.set_boxstyle("round,pad=0.5,rounding_size=0.2")
        legend_box.patch.set_facecolor("wheat")
        legend_box.patch.set_alpha(0.9)
        ax.add_artist(legend_box)

        ax.set_title('Geographic Distribution and Usage of EV Charging Stations', fontsize=16)
        ax.set_axis_off()

        # Save the map
        map_path = os.path.join(save_dir, "pkc_final_usage_map_with_legend.png")
        plt.savefig(map_path, dpi=1200, bbox_inches='tight')
        plt.show()
        plt.close()

        print(f"\n Final map with numbered legend saved successfully to: {map_path}")

    except Exception as e:
        print(f" Could not create the final map. Error: {e}")

# Main execution

if __name__ == "__main__":
    locations_file = "/content/drive/MyDrive/CIM/Dissertation/Perth and Kinross Council, UK (Public Charging)/Electric_Vehicle_Charging_Points_2765336306591006216.csv"
    local_wards_file = "ward_boundaries.geojson"

    if 'df' in locals() or 'df' in globals():
        if os.path.exists(locations_file) and os.path.exists(local_wards_file):
            map_data = prepare_map_data(df, locations_file)
            if map_data is not None and not map_data.empty:
                create_final_map(map_data, local_wards_file, SAVE_DIR)
            elif map_data is not None and map_data.empty:
                print("  Warning: No sites could be matched between usage data and location data. Map cannot be generated.")
        else:
            print(f" ERROR: Required file not found. Please ensure both '{locations_file}' and '{local_wards_file}' exist.")
    else:
        print(" ERROR: The main 'df' DataFrame is not available in memory. Please run your data loading and cleaning cells first.")

"""## **Loading the Charger Location Data**

To get the geographic coordinates for each charging station, I am now loading an official dataset provided by the council which lists all public charging points. I load this data from its CSV file into a new pandas DataFrame called df_locations. Immediately after loading, I inspect the first ten rows to understand its structure and then print a summary of all the columns and their data types to ensure the data has been imported correctly.
"""

import pandas as pd

# Define the correct file path to your dataset
locations_file_path = "/content/drive/MyDrive/CIM/Dissertation/Perth and Kinross Council, UK (Public Charging)/Electric_Vehicle_Charging_Points_2765336306591006216.csv"

# Load the dataset into a pandas DataFrame
print(f"--- Loading data from: {locations_file_path} ---")
df_locations = pd.read_csv(locations_file_path)

# --- Preview the data ---

# Display the first 5 rows to see the structure
print("\n--- First 5 Rows of the Dataset ---")
print(df_locations.head(10))

# Display a summary of all columns and their data types
print("\n--- Dataset Info (Columns and Data Types) ---")
df_locations.info()

"""## **Cleaning the Location Data**

As a further data cleaning step, I am removing any rows from the df_locations table that are missing a name for the charging point. The CP_Name is a critical piece of information for merging this dataset with my main session data, so any records without it cannot be used. I am also printing the total number of rows before and after this operation to keep a clear record of how many entries were removed.
"""

# This code assumes 'df_locations' DataFrame from the previous step is in memory.

# Remove rows with missing CP_Name ---

print("\n--- Checking for missing values in CP_Name ---")
# Store the number of rows before this cleaning step
rows_before_cp_name_clean = len(df_locations)

# Remove rows where the 'CP_Name' is missing.
df_locations.dropna(subset=['CP_Name'], inplace=True)

# Print the shape after cleaning
print(f"\nNumber of rows before removing missing CP_Name: {rows_before_cp_name_clean}")
print(f"Number of rows after removing missing CP_Name: {len(df_locations)}")
print(f"Number of rows removed: {rows_before_cp_name_clean - len(df_locations)}")

print("\n Data cleaning for CP_Name complete.")

"""## **Map of EV Charger Types in Perth and Kinross**

I create a folder called project_visualizations for the output, read the council’s charger list from a CSV and the ward boundaries from a GeoJSON, rename the coordinate columns to latitude and longitude, coerce them to numbers, drop any rows without usable coordinates, and convert the British National Grid positions to standard GPS latitude and longitude so points land correctly on the map; I then label each site by speed using a simple rule of thumb where any entry whose CH_SPEED text contains “RAPID” is classed as Rapid and everything else becomes Fast or Standard, which seems a safer choice than guessing from inconsistent power labels; after that I load the boundary shapes and keep only Perth and Kinross, turn the charger table into a geo-enabled table, and draw a clear static map with the council outline in light grey, Rapid chargers shown as larger red stars, Fast or Standard chargers as smaller blue circles, a tidy legend, a title, and hidden axes; finally I save a high-resolution image called map_geographic_equity_by_speed.png in the project_visualizations folder and, if anything goes wrong or a required file is missing, I print a helpful message so the problem is obvious.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import os
import geopandas as gpd
from pyproj import Transformer

# setup

# Define the directory where the map will be saved
SAVE_DIR = "project_visualizations"
if not os.path.exists(SAVE_DIR):
    os.makedirs(SAVE_DIR)
print(f"The map will be saved to the '{SAVE_DIR}' directory.")

# --- File Paths ---
# This script assumes the necessary files are available.
LOCATIONS_FILE = "/content/drive/MyDrive/CIM/Dissertation/Perth and Kinross Council, UK (Public Charging)/Electric_Vehicle_Charging_Points_2765336306591006216.csv"
BOUNDARY_FILE = "ward_boundaries.geojson"

# Data preparation

def load_and_prepare_charger_data(locations_csv_path):
    """
    Loads and prepares the current list of all charger locations for mapping.
    """
    print("\n--- Preparing charger location data ---")
    try:
        # Load the official PKC charger locations
        df_locations = pd.read_csv(locations_csv_path)

        # --- Rename the coordinate columns and force numeric types ---
        df_locations.rename(columns={'x': 'longitude', 'y': 'latitude'}, inplace=True)
        df_locations['latitude'] = pd.to_numeric(df_locations['latitude'], errors='coerce')
        df_locations['longitude'] = pd.to_numeric(df_locations['longitude'], errors='coerce')
        df_locations.dropna(subset=['latitude', 'longitude'], inplace=True)

        # --- Coordinate Transformation (British National Grid to WGS84) ---
        transformer = Transformer.from_crs("EPSG:27700", "EPSG:4326", always_xy=True)
        df_locations['lon_wgs84'], df_locations['lat_wgs84'] = transformer.transform(
            df_locations['longitude'].values,
            df_locations['latitude'].values
        )

        # --- Categorize Charger Type based on Speed ---
        # This is the key step for this specific analysis.
        if 'CH_SPEED' in df_locations.columns:
            # We define 'Rapid' as anything with "RAPID" in the description, as it's more reliable than parsing kW.
            df_locations['ChargerType'] = np.where(df_locations['CH_SPEED'].str.contains("RAPID", case=False),
                                                   'Rapid',
                                                   'Fast/Standard')
        else:
            df_locations['ChargerType'] = 'Unknown'

        print(f" Loaded and processed {len(df_locations)} charger locations.")
        return df_locations

    except Exception as e:
        print(f" Could not prepare charger data. Error: {e}")
        return None

# map generation

def create_distribution_map(charger_data, boundary_file_path, save_dir):
    """
    Generates and saves a static map showing the distribution of charger types.
    """
    print("\n--- Generating charger speed distribution map ---")
    try:
        # --- Load and prepare the boundary data ---
        gdf_boundaries = gpd.read_file(boundary_file_path)
        pkc_boundary = gdf_boundaries[gdf_boundaries['LAD13NM'] == 'Perth and Kinross']

        # --- Convert charger data to a GeoDataFrame ---
        gdf_chargers = gpd.GeoDataFrame(
            charger_data,
            geometry=gpd.points_from_xy(charger_data.lon_wgs84, charger_data.lat_wgs84),
            crs="EPSG:4326"
        )

        # --- Plotting ---
        plt.style.use('seaborn-v0_8-whitegrid')
        fig, ax = plt.subplots(1, 1, figsize=(10, 12))

        # Plot the PKC boundary as the base layer
        pkc_boundary.plot(ax=ax, color='lightgray', edgecolor='black', linewidth=0.7)

        # Define markers and colors for different charger types
        type_aesthetics = {
            'Rapid': {'marker': '*', 'color': 'red', 's': 150, 'label': 'Rapid Charger'},
            'Fast/Standard': {'marker': 'o', 'color': 'blue', 's': 50, 'label': 'Fast/Standard Charger'}
        }

        # Plot each charger type separately to create a clear legend
        for charger_type, aesthetics in type_aesthetics.items():
            gdf_subset = gdf_chargers[gdf_chargers['ChargerType'] == charger_type]
            if not gdf_subset.empty:
                ax.scatter(
                    gdf_subset.geometry.x,
                    gdf_subset.geometry.y,
                    marker=aesthetics['marker'],
                    s=aesthetics['s'],
                    c=aesthetics['color'],
                    label=aesthetics['label'],
                    alpha=0.9,
                    edgecolors='k',
                    linewidth=0.5,
                    zorder=10
                )

        ax.set_title('Geographic Distribution of EV Charger Speeds', fontsize=16)
        ax.set_axis_off() # Hide the lat/lon axes for a clean map
        ax.legend(title="Charger Type", loc='lower left', fancybox=True, shadow=True)

        # Save the map to a file
        map_path = os.path.join(save_dir, "map_geographic_equity_by_speed.png")
        plt.savefig(map_path, dpi=1200, bbox_inches='tight')
        plt.show()
        plt.close()

        print(f"\n Distribution map saved successfully to: {map_path}")

    except Exception as e:
        print(f" Could not create the distribution map. Error: {e}")

# Main execution

if __name__ == "__main__":
    if os.path.exists(LOCATIONS_FILE) and os.path.exists(BOUNDARY_FILE):
        # Run the full pipeline
        charger_data = load_and_prepare_charger_data(LOCATIONS_FILE)
        if charger_data is not None:
            create_distribution_map(charger_data, BOUNDARY_FILE, SAVE_DIR)
    else:
        print(f" ERROR: Required file not found. Please ensure both '{LOCATIONS_FILE}' and '{BOUNDARY_FILE}' exist.")

"""Overall Interpretation
The map reveals a "hub and spoke" strategy for infrastructure deployment. There is a clear concentration of chargers in the urban center of Perth, with "spokes" of infrastructure extending out to serve key towns and tourist routes. While this provides good coverage for major population centers and travel corridors, it also highlights potential gaps in service for more remote, rural areas.

Key Observations and Discussion Points:
Urban Concentration: The densest cluster of both Rapid (red stars) and Fast/Standard (blue circles) chargers is located in and around the city of Perth in the south-east of the region. This is logical, as it serves the largest population center and the highest traffic areas.

Strategic Placement of Rapid Chargers: The red stars are not just in Perth. They are strategically placed in key towns along what are likely major A-roads (e.g., Auchterarder, Crieff, Dunkeld, Pitlochry).

Socio-Technical Insight: This is a deliberate policy to support longer-distance travel and tourism. It ensures that drivers can travel through the region with confidence, knowing they can get a quick top-up in major towns. This is a positive finding regarding enabling EV use beyond local commuting.

The Rural "Charging Desert" Question: While there is coverage in smaller towns, the vast northern and western parts of the region have very sparse infrastructure.

Socio-Technical Insight: This is the core of your geographic equity analysis. A resident living in the north-western part of the council area has significantly poorer access to public charging than someone in Perth. This could be a major barrier to EV adoption for rural communities, creating a two-tiered system where EVs are practical for urban dwellers but not for those in more remote areas.

How to Use This in Your Dissertation:
Argument: You can argue that while the council has successfully supported major transport routes, a significant geographic disparity remains.

Evidence: Use this map as "Figure X" to visually prove your point. You can state, "As shown in Figure X, the distribution of high-speed Rapid chargers is primarily concentrated along the A9 corridor and within the Perth urban area, leaving large rural expanses underserved."

Recommendation: This leads directly to a policy recommendation. For example: "To ensure equitable access and encourage rural EV adoption, a future phase of infrastructure deployment should focus on placing at least one Rapid or Fast charger in key villages in the currently underserved north-western region."

## **Economic Accessibility Chart for EV Charging Sites**

I set up an output folder called project_visualizations, read the official charger list from the CSV, and tidy the data before plotting; first I drop entries with a missing charger name, rename CP_Name to Site, trim stray spaces, and standardise a handful of awkward site labels so locations that are really the same place show up under one clear name; I then create a simple payment flag by reading CP_CHARGE, mapping FREE to Free and CHARGE to Paid, and marking anything unclear as Unknown, which feels safer than guessing; next I build a table that counts, for each site, how many chargers are Free, Paid, or Unknown, add a total column, and keep the 15 busiest sites so the figure stays readable; after sorting by that total, I drop the helper column and draw a horizontal stacked bar chart where each bar shows the mix of free and paid sockets at a site, add a title, axis labels, a neat legend, and light gridlines, and save a high-resolution image called economic_accessibility_by_site.png in the project_visualizations folder; along the way I print friendly status messages so it is obvious what ran, and I leave the plotting function without a try–except so, if something breaks, I see the full error and can fix it quickly.
"""

import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
import os

# Setup

# Define the directory where the plot will be saved
SAVE_DIR = "project_visualizations"
if not os.path.exists(SAVE_DIR):
    os.makedirs(SAVE_DIR)
print(f"The analysis chart will be saved to the '{SAVE_DIR}' directory.")

# --- File Path ---
LOCATIONS_FILE = "/content/drive/MyDrive/CIM/Dissertation/Perth and Kinross Council, UK (Public Charging)/Electric_Vehicle_Charging_Points_2765336306591006216.csv"

# Data preparation

def prepare_economic_analysis_data(locations_csv_path):
    """
    Loads and prepares the current list of all charger locations for economic analysis.
    """
    print("\n--- Preparing data for economic accessibility analysis ---")
    try:
        # Load the official PKC charger locations
        df_locations = pd.read_csv(locations_csv_path)

        # --- NEW: Remove rows where the charger name is missing before any processing ---
        df_locations.dropna(subset=['CP_Name'], inplace=True)

        # --- Standardize column names ---
        df_locations.rename(columns={'CP_Name': 'Site'}, inplace=True)
        df_locations['Site'] = df_locations['Site'].astype(str).str.strip()

        # --- Clean and standardize site names ---
        location_name_map = {
            'ACCESS ROAD INTO PARK AND RIDE BROXDEN AVENUE': 'Broxden Park & Ride',
            'CAR PARK OFF JUNCTION ROAD PARK AND RIDE': 'Kinross Park and Ride',
            'CAR PARK OFF SHORE ROAD AT SOUTH INCH': 'South Inch Car Park',
            'CANAL STREETMULTY STOREY CAR PARK': 'Canal Street Car Park',
            'CAR PARK MULTI STOREY OFF CANAL STREET': 'Canal Street Car Park',
            'CAR PARK OFF KING STREET TOWARDS GALVELMORE STREET': 'King Street Car Park, Crieff',
            'CAR PARK OFF HIGH STREET': 'Crown Inn Wynd Car Park, Auchterarder',
            'CAR PARK OFF CROFT LANE AT LESLIE STREET': 'Leslie Street Car Park, Blairgowrie',
            'CAR PARK OFF RIE-ACHAN ROAD': 'Rie-Achan Road Car Park, Pitlochry',
            'CAR PARK OFF MILL STREET EAST': 'Mill Street Car Park',
            'CAR PARK OFF MONESS TERRACE': 'Moness Terrace Car Park, Aberfeldy',
            'FRIARTON WASTE TRANSFER STATION': 'Friarton Depot',
            'CAR PARK IN MARKET SQUARE': 'Market Square Alyth',
            'LEADENFLOWER CAR PARK': 'Atholl Street Car Park, Dunkeld'
        }
        df_locations['Site'] = df_locations['Site'].replace(location_name_map)

        # --- Use the 'CP_CHARGE' column for payment type ---
        if 'CP_CHARGE' in df_locations.columns:
            df_locations['PaymentType'] = df_locations['CP_CHARGE'].str.strip().replace({
                'FREE': 'Free',
                'CHARGE': 'Paid'
            }).fillna('Unknown') # Handle any missing payment info
        else:
            df_locations['PaymentType'] = 'Unknown'

        print(f" Successfully loaded and processed data for {len(df_locations)} charging points.")
        return df_locations

    except Exception as e:
        print(f" Could not prepare analysis data. Error: {e}")
        return None

# Bar chart generation

def create_economic_accessibility_chart(df_locations, save_dir):
    """
    Generates and saves a stacked bar chart showing the distribution of
    free vs. paid chargers at each major site.
    """
    print("\n--- Generating economic accessibility chart ---")
    # Removed the try-except block as requested to show the full traceback on error.

    # Create a cross-tabulation of site vs. payment type
    crosstab = pd.crosstab(df_locations['Site'], df_locations['PaymentType'])

    # We only want to plot sites with a significant number of chargers
    crosstab['total'] = crosstab.sum(axis=1)
    top_sites_crosstab = crosstab.sort_values('total', ascending=False).head(15)

    # --- CORRECTED: Sort by the total first, then drop the helper column ---
    # This ensures the plot is ordered correctly from busiest to least busy site.
    plot_data = top_sites_crosstab.sort_values(by='total', ascending=True)
    plot_data = plot_data.drop(columns='total')

    # Plotting
    plt.style.use('seaborn-v0_8-whitegrid')
    ax = plot_data.plot(
        kind='barh',
        stacked=True,
        figsize=(12, 8),
        colormap='coolwarm',
        width=0.8
    )

    ax.set_title('Economic Accessibility: Free vs. Paid Chargers by Site', fontsize=18)
    ax.set_xlabel('Number of Charging Points', fontsize=12)
    ax.set_ylabel('Charging Site', fontsize=12)
    ax.legend(title='Payment Type')
    ax.grid(axis='x', linestyle='--', alpha=0.7)

    plt.tight_layout()

    # Save the figure
    save_path = os.path.join(save_dir, "economic_accessibility_by_site.png")
    plt.savefig(save_path, dpi=1200)
    plt.show()
    plt.close()

    print(f"\n Economic accessibility chart saved successfully to: {save_path}")


# Main execution

if __name__ == "__main__":
    if os.path.exists(LOCATIONS_FILE):
        # Run the full pipeline
        df_locations = prepare_economic_analysis_data(LOCATIONS_FILE)
        if df_locations is not None:
            create_economic_accessibility_chart(df_locations, SAVE_DIR)
    else:
        print(f" ERROR: The locations file was not found at '{LOCATIONS_FILE}'.")

"""Overall Interpretation
The most striking takeaway is that the public charging network in this dataset is overwhelmingly free to use. The vast majority of charging points, represented by the blue bars, do not require payment. This indicates a deliberate strategy by the council to encourage EV adoption by removing one of the primary cost barriers.

However, the distribution of these free chargers is not uniform, which leads to some interesting social and economic questions.

Breakdown by Key Sites:
Major Transport Hubs (Broxden & Kinross Park & Ride):

What it shows: These are two of the largest sites, and they are composed almost entirely of free chargers.

Interpretation for your dissertation: This is a strong policy signal. The council is actively incentivizing commuters to use Park & Ride facilities by offering free charging. This is a strategy to reduce city-center congestion and emissions. You can discuss this as a successful example of integrated transport and energy policy.

City Centre Hubs (Canal Street & South Inch Car Parks):

What it shows: These sites show a more complex picture. While they have many free chargers, they also have a significant number of "Paid" or "Unknown" chargers. Canal Street, the largest single site, has the most diverse mix.

Interpretation for your dissertation: This is where your socio-technical analysis becomes crucial. Why is there a mix of payment types in the city centre? It could be that the free chargers are lower power (Fast/Standard) while the paid ones are high-power (Rapid). Or it could be a strategy to manage demand in high-traffic areas. This "hybrid" model at key urban locations is a key finding.

Specialized Sites (Friarton Depot):

What it shows: This site is composed entirely of chargers with an "Unknown" payment type.

Interpretation for your dissertation: Given that this is a depot, these are likely fleet chargers not intended for the general public. The "Unknown" status might mean they are privately managed or have a different internal billing system. This highlights that not all "public" infrastructure serves the same purpose or user group.

Key Insights for Your Dissertation:
A "Freemium" Model: The network operates on a "freemium" model. Basic access is largely free, especially at strategic locations like Park & Rides, to encourage adoption. However, in the most high-demand urban locations, a paid or mixed model exists.

Data Quality as a Finding: The presence of the "Unknown" category is itself a finding. It points to inconsistencies in the public data, which is a real-world challenge for any analysis. You can mention this as a limitation and a recommendation for improvement in data collection practices.

The Question of Equity: This plot allows you to ask critical questions. Does providing free charging at Park & Ride facilities primarily benefit affluent commuters with EVs? Are there enough free chargers in residential areas for people who don't have private driveways? This visualization is the perfect starting point for that discussion.

## **Creating a Clean, Enriched Dataset of Charger Locations**

I read the council’s charger list from the CSV, drop entries that lack a name or coordinates, and rename the key columns so CP_Name becomes Site and x and y become longitude and latitude; I tidy the Site text and standardise several awkward labels so places that are really the same show up under one clear name, which should make later analysis less noisy; I add simple features that I will reuse throughout the project, including ChargerType, where any CH_SPEED text containing “RAPID” is marked Rapid and everything else becomes Fast or Standard, PaymentType, where CP_CHARGE is mapped to Free or Paid with unknowns left as Unknown, and LocationType, which classifies each Site as a transport hub, depot, public car park, or on-street or other based on the wording; I also create MaxStayHrs by turning the STAY field into a number and, if that is missing, assuming 24 hours so I have a sensible ceiling rather than blanks; once the table is prepared I print a short success message and return it, then in the main block I only run the pipeline if the file exists and show a quick preview of the cleaned data, which helps me confirm the transformations look plausible before I use the dataset in plots or models.
"""

import pandas as pd
import numpy as np
import os

# setup
# This script assumes the locations CSV file is available at the specified path.

LOCATIONS_FILE = "/content/drive/MyDrive/CIM/Dissertation/Perth and Kinross Council, UK (Public Charging)/Electric_Vehicle_Charging_Points_2765336306591006216.csv"

# Data preparation

def create_prepared_locations_dataframe(locations_csv_path):
    """
    Loads and prepares the current list of all charger locations, returning a
    single, clean DataFrame for all subsequent analyses.
    """
    print("\n--- Preparing Final Analysis DataFrame ---")
    try:
        # Load the official PKC charger locations
        df = pd.read_csv(locations_csv_path)

        # --- a. Handle Missing Data ---
        # Remove rows where the charger name or coordinates are missing
        df.dropna(subset=['CP_Name', 'x', 'y'], inplace=True)

        # --- b. Standardize Column Names ---
        df.rename(columns={'CP_Name': 'Site', 'x': 'longitude', 'y': 'latitude'}, inplace=True)

        # --- c. Clean and Standardize Site Names ---
        df['Site'] = df['Site'].astype(str).str.strip()
        location_name_map = {
            'ACCESS ROAD INTO PARK AND RIDE BROXDEN AVENUE': 'Broxden Park & Ride',
            'CAR PARK OFF JUNCTION ROAD PARK AND RIDE': 'Kinross Park and Ride',
            'CAR PARK OFF SHORE ROAD AT SOUTH INCH': 'South Inch Car Park',
            'CANAL STREETMULTY STOREY CAR PARK': 'Canal Street Car Park',
            'CAR PARK MULTI STOREY OFF CANAL STREET': 'Canal Street Car Park',
            'CAR PARK OFF KING STREET TOWARDS GALVELMORE STREET': 'King Street Car Park, Crieff',
            'CAR PARK OFF HIGH STREET': 'Crown Inn Wynd Car Park, Auchterarder',
            'CAR PARK OFF CROFT LANE AT LESLIE STREET': 'Leslie Street Car Park, Blairgowrie',
            'CAR PARK OFF RIE-ACHAN ROAD': 'Rie-Achan Road Car Park, Pitlochry',
            'CAR PARK OFF MILL STREET EAST': 'Mill Street Car Park',
            'CAR PARK OFF MONESS TERRACE': 'Moness Terrace Car Park, Aberfeldy',
            'FRIARTON WASTE TRANSFER STATION': 'Friarton Depot',
            'CAR PARK IN MARKET SQUARE': 'Market Square Alyth',
            'LEADENFLOWER CAR PARK': 'Atholl Street Car Park, Dunkeld'
        }
        df['Site'] = df['Site'].replace(location_name_map)

        # --- d. Create Feature: ChargerType ---
        if 'CH_SPEED' in df.columns:
            df['ChargerType'] = np.where(df['CH_SPEED'].str.contains("RAPID", case=False), 'Rapid', 'Fast/Standard')
        else:
            df['ChargerType'] = 'Unknown'

        # --- e. Create Feature: PaymentType ---
        if 'CP_CHARGE' in df.columns:
            df['PaymentType'] = df['CP_CHARGE'].str.strip().replace({'FREE': 'Free', 'CHARGE': 'Paid'}).fillna('Unknown')
        else:
            df['PaymentType'] = 'Unknown'

        # --- f. Create Feature: LocationType ---
        def get_location_type(site_name):
            if "Park & Ride" in site_name: return "Transport Hub (Park & Ride)"
            if "Depot" in site_name: return "Depot / Fleet"
            if "Car Park" in site_name: return "Public Car Park"
            return "On-Street / Other"
        df['LocationType'] = df['Site'].apply(get_location_type)

        # --- g. Create Feature: MaxStayHrs ---
        if 'STAY' in df.columns:
            df['MaxStayHrs'] = pd.to_numeric(df['STAY'], errors='coerce').fillna(24) # Assume 24h for no limit
        else:
            df['MaxStayHrs'] = 24 # Default to 24h if column is missing

        print(f" Successfully created the final prepared DataFrame with {len(df)} charging points.")
        return df

    except Exception as e:
        print(f" Could not prepare analysis data. Error: {e}")
        return None

# Main Execution

if __name__ == "__main__":
    if os.path.exists(LOCATIONS_FILE):
        # Create the final, clean DataFrame
        df_locations_prepared = create_prepared_locations_dataframe(LOCATIONS_FILE)
        if df_locations_prepared is not None:
            print("\n--- Preview of Prepared Data ---")
            print(df_locations_prepared.head())
    else:
        print(f" ERROR: The locations file was not found at '{LOCATIONS_FILE}'.")

"""## **Intended Use Plot: Maximum Stay by Location Type**

I assume the cleaned DataFrame is already in memory, set up an output folder called project_visualizations, and draw a single, readable chart that shows how maximum stay limits vary by place; I use a horizontal violin plot for each LocationType so the shape captures the full spread of stay times, add the inner quartile marks to hint at the middle range, and layer a light swarm of individual points on top to keep the real observations visible rather than hiding everything behind summaries; I stick with a simple white-grid style, label the axes, add a clear title, and cap the x axis at about a day, from just below zero to 25 hours, which keeps attention on the common short-stay policies while still catching anything near 24; once drawn, I save a high-resolution image named intended_use_by_stay_time.png inside project_visualizations and print a short success message, and if the DataFrame is missing or something else trips an error I report that plainly so I can fix it quickly.
"""

import matplotlib.pyplot as plt
import seaborn as sns
import os

# Setup
SAVE_DIR = "project_visualizations"
if not os.path.exists(SAVE_DIR):
    os.makedirs(SAVE_DIR)

# This script assumes the 'df_locations_prepared' DataFrame is already in memory.

# Plot generation
print("\n--- Generating Intended Use (Max Stay) plot ---")
try:
    plt.style.use('seaborn-v0_8-whitegrid')
    plt.figure(figsize=(12, 8))

    # --- IMPROVED VISUALIZATION: Use a Violin Plot with Jitter ---
    # A violin plot is better for showing the distribution of points.
    # The inner 'quartile' shows the box plot ranges, and the swarm shows individual points.
    sns.violinplot(
        data=df_locations_prepared,
        x='MaxStayHrs',
        y='LocationType',
        orient='h',
        palette='viridis',
        inner='quartile', # Shows the quartiles inside the violin
        cut=0 # Prevents the violin from extending beyond the data range
    )

    # Overlay a swarm plot to show individual data points (jitter)
    sns.swarmplot(
        data=df_locations_prepared,
        x='MaxStayHrs',
        y='LocationType',
        orient='h',
        color='white',
        edgecolor='gray',
        size=5
    )

    plt.title('Intended Use Analysis: Maximum Stay Times by Location Type', fontsize=16)
    plt.xlabel('Maximum Stay Allowed (Hours)', fontsize=12)
    plt.ylabel('Location Type', fontsize=12)
    plt.grid(axis='x', linestyle='--', alpha=0.7)

    # Set a more reasonable x-axis limit to focus on the main distribution
    # We can cap it slightly above the most common short-stay limits.
    plt.xlim(-1, 25)

    plt.tight_layout()

    # Save the figure
    save_path = os.path.join(SAVE_DIR, "intended_use_by_stay_time.png")
    plt.savefig(save_path, dpi=1200)
    plt.show()
    plt.close()

    print(f"\n Intended use chart saved successfully to: {save_path}")

except NameError:
    print(" ERROR: The 'df_locations_prepared' DataFrame was not found.")
    print("Please ensure you have successfully run the 'Final Data Preparation for Analysis' script first.")
except Exception as e:
    print(f" Could not create the analysis chart. Error: {e}")

"""Overall Interpretation
The plot reveals a clear and deliberate strategy by the council to design different types of charging infrastructure for different social purposes. There is a strong distinction between infrastructure designed for short-term, transient use (like public car parks) and infrastructure for long-duration, depot-based charging.

Breakdown by Location Type:
Public Car Park:

What it shows: This is the most interesting category. The "violin" is very wide and heavily concentrated between 1 and 4 hours. The white dots (representing individual chargers) are all clustered here.

Interpretation: This indicates that the vast majority of chargers in public car parks are explicitly designed for short-stay users, such as shoppers or visitors. The goal is to encourage quick turnover, not all-day parking. The single outlier point at 24 hours represents a long-stay car park, but it is the exception, not the rule.

Transport Hub (Park & Ride) and On-Street / Other:

What it shows: Both of these categories have very thin violins, with all data points clustered at very short stay times (around 1-2 hours).

Interpretation: This is a very significant finding. It suggests that even at Park & Ride facilities, which are typically used by all-day commuters, the chargers are intended for rapid top-ups, not for commuters to leave their cars charging all day. This highlights a potential mismatch between infrastructure design and user needs.

Depot / Fleet:

What it shows: The data points are all clustered at the 24-hour mark.

Interpretation: This makes perfect sense. These chargers are not for the general public but for council vehicles or other fleets that are parked overnight. The 24-hour limit essentially means there is "no limit," allowing for slow, full charging.

Key Insights for Your Dissertation:
Focus on Transient Charging: The public-facing network is overwhelmingly designed to support transient charging (short trips, shopping) rather than solving the problem for residents without private driveways or all-day commuters.

The Commuter Problem: The short stay limits at Park & Ride hubs are a key socio-technical finding. You can argue that this infrastructure design does not currently serve the needs of commuters who might want to leave their car for 8-9 hours. This could be a barrier to EV adoption for this group.

Clear Policy Segmentation: The plot provides strong evidence that the council has different policies for different locations. Your dissertation can discuss the pros and cons of this approach—it ensures high turnover in busy car parks but may leave certain user groups (like commuters) underserved.

## **Technology Equity: Connector Standards Pie Chart**

I assume the cleaned DataFrame is already in memory, create an output folder if needed, then scan the CON_TYPE text for three common plug standards, CCS, CHAdeMO and Type 2, counting how many chargers mention each one regardless of letter case; because a single charger can list more than one connector, these counts may overlap, which is fine for this purpose since I just want a sense of the mix on the ground; I turn the counts into a small table and draw a simple pie chart with percentage labels, a soft colour palette, a clear title and tidy white edges on each slice so the segments are easy to read; finally I save a high-resolution image called technology_equity_connector_types.png in the project_visualizations folder and print a short success message, and if the prepared DataFrame is missing or something else fails I show a plain error so I know what to fix.
"""

import matplotlib.pyplot as plt
import seaborn as sns
import os
import pandas as pd

# Setup
SAVE_DIR = "project_visualizations"
if not os.path.exists(SAVE_DIR):
    os.makedirs(SAVE_DIR)

# This script assumes the 'df_locations_prepared' DataFrame is already in memory.

# Plot generation
print("\n--- Generating Technology Equity (Connector Type) plot ---")
try:
    # --- Data Analysis ---
    # The CON_TYPE column often contains multiple values separated by semicolons.
    # We need to count the presence of each key standard.
    # Note: A single charger can have multiple connectors.

    connector_counts = {
        'CCS': df_locations_prepared['CON_TYPE'].str.contains('CCS', case=False).sum(),
        'CHAdeMO': df_locations_prepared['CON_TYPE'].str.contains('CHAdeMO', case=False).sum(),
        'Type 2': df_locations_prepared['CON_TYPE'].str.contains('TYPE 2', case=False).sum()
    }

    # Convert to a pandas Series for easy plotting
    connector_series = pd.Series(connector_counts)

    # --- Visualization ---
    plt.style.use('seaborn-v0_8-whitegrid')
    plt.figure(figsize=(10, 8))

    # Create the pie chart
    wedges, texts, autotexts = plt.pie(
        connector_series,
        labels=connector_series.index,
        autopct='%1.1f%%',
        startangle=140,
        colors=sns.color_palette('pastel'),
        wedgeprops={'edgecolor': 'white', 'linewidth': 1}
    )

    # Improve text formatting
    plt.setp(autotexts, size=12, weight="bold")
    plt.setp(texts, size=12)

    plt.title('Technology Equity: Distribution of Connector Standards', fontsize=18)
    plt.ylabel('') # Hide the y-label for a pie chart

    plt.tight_layout()

    # Save the figure
    save_path = os.path.join(SAVE_DIR, "technology_equity_connector_types.png")
    plt.savefig(save_path, dpi=1200)
    plt.show()
    plt.close()

    print(f"\n Connector type distribution chart saved successfully to: {save_path}")

except NameError:
    print(" ERROR: The 'df_locations_prepared' DataFrame was not found.")
    print("Please ensure you have successfully run the 'Final Data Preparation for Analysis' script first.")
except Exception as e:
    print(f" Could not create the analysis chart. Error: {e}")

"""Overall Interpretation
The chart reveals a network that was well-designed for the EV market of the past but now faces a significant challenge in serving the market of the future. While it provides excellent coverage for all types of connectors currently on the road, the heavy investment in the older CHAdeMO standard creates a potential "technology bottleneck" for the growing number of modern EV drivers.

Breakdown by Connector Type:
Type 2 (58.3%): The Workhorse Connector

What it is: This is the universal standard for AC "Fast/Standard" charging across the UK and Europe. It's the most common plug type on the network.

Interpretation: This is a positive finding. It means that almost all EV drivers can use the majority of the charging sockets for slower, longer-duration charging (e.g., while parked for a few hours). This ensures broad basic compatibility.

CCS and CHAdeMO (20.8% each): The Rapid Charging Dilemma

What they are: These are the two competing standards for DC Rapid charging, which is essential for long journeys and quick top-ups.

CHAdeMO: The older standard, primarily used by the Nissan Leaf (one of the most popular early EVs) and some other Japanese models.

CCS: The modern standard used by virtually all new European, Korean, and American EVs (e.g., Tesla, VW, Hyundai, Kia, Ford).

Interpretation: The almost perfect 50/50 split between CCS and CHAdeMO is the most critical finding. It indicates that the network was built out during a time when the Nissan Leaf was a dominant force in the EV market. The council made a commendable effort to provide equal access for both types of vehicles.

Key Insights for Your Dissertation:
Legacy Support vs. Future Demand: The network is very equitable for current and past EV owners. However, the UK car market has decisively moved to the CCS standard. This means that as older CHAdeMO vehicles are retired, half of the rapid charging infrastructure will serve a shrinking fraction of the EV fleet.

The Future Bottleneck: This creates a significant "technology equity" issue for the future. A driver of a new Volkswagen ID.4 (which uses CCS) effectively sees only half of the available rapid chargers as usable. This can lead to queues and frustration, even if the CHAdeMO plug right next to it is empty.

Policy Recommendation: This analysis provides a strong, data-driven basis for a policy recommendation. You can argue that any future investment should prioritize adding more CCS connectors. This could involve either building new sites with a CCS-first approach or retrofitting existing rapid chargers to have more CCS plugs, ensuring the public investment remains relevant and serves the majority of future EV drivers.

## **Infrastructure Density: Hubs vs Spokes**

I assume the cleaned DataFrame is already in memory, create the project_visualizations folder if it does not exist, then count how many charging points appear at each Site so I can see where capacity clusters; I sort those counts for readability and draw a horizontal bar chart with a simple white grid, add a clear title and axis labels, and place small number labels at the end of each bar so the totals are obvious without squinting; the picture helps me spot hubs with many sockets versus smaller spoke sites with only one or two, which is useful when thinking about resilience and queuing; once the figure looks tidy, I save a high resolution image called infrastructure_density_hubs_vs_spokes.png in the project_visualizations folder and print a short success message, and if the DataFrame is missing or anything else goes wrong I surface a plain error so I know what to fix.
"""

import matplotlib.pyplot as plt
import seaborn as sns
import os
import pandas as pd

# Setup
SAVE_DIR = "project_visualizations"
if not os.path.exists(SAVE_DIR):
    os.makedirs(SAVE_DIR)

# This script assumes the 'df_locations_prepared' DataFrame is already in memory.

# Plot generation
print("\n--- Generating Infrastructure Density (Hubs vs. Spokes) plot ---")
try:
    # --- Data Analysis ---
    # Count the number of occurrences of each unique site name.
    # This gives us the number of charging points per site.
    site_counts = df_locations_prepared['Site'].value_counts()

    # --- Visualization ---
    plt.style.use('seaborn-v0_8-whitegrid')
    plt.figure(figsize=(12, 8))

    # Create the horizontal bar chart, sorting the values for a clean look.
    site_counts.sort_values().plot(kind='barh', color='purple')

    plt.title('Infrastructure Density: Number of Charging Points per Site', fontsize=18)
    plt.xlabel('Total Number of Charging Points', fontsize=12)
    plt.ylabel('Charging Site', fontsize=12)
    plt.grid(axis='x', linestyle='--', alpha=0.7)

    # Add data labels to the end of each bar for clarity
    for index, value in enumerate(site_counts.sort_values()):
        plt.text(value, index, f' {value}', va='center')

    plt.tight_layout()

    # Save the figure
    save_path = os.path.join(SAVE_DIR, "infrastructure_density_hubs_vs_spokes.png")
    plt.savefig(save_path, dpi=1200)
    plt.show()
    plt.close()

    print(f"\n Infrastructure density chart saved successfully to: {save_path}")

except NameError:
    print(" ERROR: The 'df_locations_prepared' DataFrame was not found.")
    print("Please ensure you have successfully run the 'Final Data Preparation for Analysis' script first.")
except Exception as e:
    print(f" Could not create the analysis chart. Error: {e}")

"""Overall Interpretation
The chart clearly demonstrates a "Hub and Spoke" deployment strategy. The council has focused on creating a small number of large, multi-charger "Hubs" at key strategic locations, while simultaneously deploying a large number of single-charger "Spokes" to maximize geographic coverage.

Breakdown of the Strategy:
The Hubs (High-Density Sites):

What it shows: You can see a few sites at the top of the chart with a significantly higher number of charging points than the rest. Canal Street Car Park (7 points), Broxden Park & Ride (6 points), and South Inch/Kinross Park and Ride (4 points each) are the clear hubs.

Interpretation: These are high-capacity sites designed to serve a large number of users simultaneously. Placing them in major car parks and transport interchanges is a deliberate strategy to support commuters and visitors in high-traffic areas. The key benefit of a hub is resilience; if one charger is broken or in use, others are available, leading to a better user experience.

The Spokes (Low-Density Sites):

What it shows: The majority of the sites listed have only one or two charging points. These are the "spokes" of the network.

Interpretation: The purpose of these single-charger sites is to provide maximum geographic reach. The goal is to ensure that as many towns and villages as possible have at least some access to a public charger, even if it's just one. This is crucial for encouraging EV adoption in more rural areas.

Key Insights for Your Dissertation:
A Deliberate Strategy: This is not a random distribution. The chart provides strong evidence of a thought-out strategy that balances the need for high capacity in urban centers with the need for broad coverage across the region.

The Resilience vs. Coverage Trade-Off: This is the core socio-technical point. The "Hubs" provide a reliable and convenient service. The "Spokes," however, represent a potential point of failure. If the single charger in a small town like Dunkeld or Alyth is out of order, it effectively removes that entire town from the public charging map, which can be a major issue for local residents and tourists.

Policy Discussion: This plot allows you to discuss the pros and cons of this strategy. Is it better to have fewer, larger, more reliable hubs, or more, smaller, but less resilient spokes? Your dissertation can use this chart as a basis to make recommendations for the next phase of infrastructure deployment.
"""

!pip install pulp

!pip install geopandas

"""# **DISCLAIMER: SOME ASPECT OF THIS CODE HAS BEEN GENERATED AND DEBIGGED USING LARGE LANGUAGE MODELS (GOOGLE GEMINI & CHATGPT)**"""

